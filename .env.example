# Database
DB_PASSWORD=
DB_USER=
DB_HOST=
DB_PORT=
DB_NAME=
DATABASE_URL=
REDIS_DATA=
REDIS_MAX_CONNECTIONS=
REDIS_PASSWORD=
REDIS_URL=

# =============================================================================
# API Keys
# =============================================================================

# Anthropic Claude API
# Get your key from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=sk-ant-api03-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
CLAUDE_API_KEY=sk-ant-api03-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# Google Gemini API
# Get your key from: https://aistudio.google.com/apikey
GOOGLE_API_KEY=AIzaSyXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
GEMINI_API_KEY=AIzaSyXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# =============================================================================
# Z.AI (GLM-4.7) API - CRITICAL CONFIGURATION WARNING
# =============================================================================
# ⚠️ IMPORTANT: Model names and Rate Limits change frequently!
#
# Before updating models, ALWAYS check:
# 1. https://z.ai/manage-apikey/rate-limits (Current models & limits)
# 2. Verify model availability and Concurrency Limits (QPS/TPM limits)
# 3. Check pricing and rate limits for each model
# 4. Update BOTH .env.example AND .env simultaneously
#
# Current GLM Models (2026-01-17):
# Model                | Concurrency | Speed      | Best For
# ---------------------|-------------|------------|------------------------
# glm-4-plus           | 20          | Fast       | News Processing ✅
# glm-4.7              | 3           | Slower     | Deep Reasoning
# glm-4.6v-flashx      | 3           | Very Fast  | JSON Structuring
# glm-4-flash          | 20          | Fast       | General Use
# glm-4-flashx         | 20          | Very Fast  | High Throughput
#
# Rate Limiting Formula:
#   Concurrency Limit = Max simultaneous requests
#   NEWS_GLM_RATE_LIMIT = Delay between requests (seconds)
#   Safe Request Rate = Concurrency Limit / 2 (conservative)
#
# Two-Stage Architecture:
#   - Stage 1 (Reasoning): GLM-4.7 for deep reasoning (slower, more thoughtful)
#   - Stage 2 (Structuring): GLM-4.6V-FlashX for fast JSON extraction (fast, cost-effective)
#
# Get your key from: https://open.bigmodel.cn/usercenter/apikeys
# Official models list: https://z.ai/manage-apikey/rate-limits
# =============================================================================
ZAI_API_KEY="your API key here"
ZAI_API_URL=https://api.z.ai/api/coding/paas/v4/chat/completions
GLM_API_KEY="your API key here"


# KIS (Korea Investment & Securities)
KIS_ACCOUNT_NUMBER=12345678
KIS_APP_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
KIS_APP_SECRET=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
KIS_PAPER_ACCOUNT=
KIS_PAPER_APP_KEY=
KIS_PAPER_APP_SECRET=
KIS_ENV=production
KIS_IS_VIRTUAL=false

# OpenAI API
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# Other APIs
MASSIVE_API_KEY=
POLYGON_API_KEY=
ALPACA_API_KEY=
ALPACA_SECRET_KEY=
NEWS_API_KEY=
FRED_API_KEY=

# =============================================================================
# AI Model Configuration
# =============================================================================
# Last updated: 2026-01-17
# Check for updates:
# - Gemini: https://ai.google.dev/gemini-api/docs/models/gemini
# - OpenAI: https://platform.openai.com/docs/models
# - Claude: https://docs.anthropic.com/en/docs/about-claude/models

# Gemini Model Selection (General Purpose)
# Available: gemini-2.5-flash-lite, gemini-2.5-flash, gemini-2.5-pro
# Recommended: gemini-2.5-flash-lite (fastest, cheapest, not deprecated)
# ⚠️ DEPRECATED: gemini-2.0-flash (retiring 2026-03-03)
GEMINI_MODEL=gemini-2.5-flash-lite

# =============================================================================
# War Room MVP - Hybrid LLM Configuration
# =============================================================================
# ⚠️ IMPORTANT: Concurrency Optimization Strategy
#
# To solve GLM Concurrency Limit 3, we use hybrid approach:
#   - Risk Agent: GLM-4.7 + GLM-4.6V (2 requests, highest accuracy for risk)
#   - Trader/Analyst: Gemini 2.5 Flash Lite (4 requests, higher concurrency limit 60+)
#
# Total GLM requests: 2 (Risk Agent only)
# Total Gemini requests: 4 (Trader + Analyst Agents)
# =============================================================================
# Stage 1: Reasoning Agents
# Trader Agent (Gemini) - High concurrency, fast response
GEMINI_MODEL_REASONING=gemini-2.5-flash-lite

# Analyst Agent (Gemini) - High concurrency, fast response
# (Uses same GEMINI_MODEL_REASONING)

# Trader/Analyst (Gemini) - JSON mode for reliable extraction
GEMINI_MODEL_STRUCTURING=gemini-2.5-flash-lite


# =============================================================================
# Benefits of Hybrid Approach:
# - Solves GLM Concurrency Limit 3 (was 6 requests, now 2)
# - Maintains GLM accuracy for critical Risk Agent
# - Faster response with Gemini 2.5 Flash Lite for Trader/Analyst
# - Cost-effective (Gemini is cheaper than GLM)
# - Using non-deprecated Gemini models (2.5 series, not 2.0)
# =============================================================================

# OpenAI Model Selection
# Available: gpt-4o, gpt-4o-mini, gpt-4-turbo
# Recommended: gpt-4o-mini (cost-effective)
OPENAI_MODEL=gpt-4o-mini

# Claude Model Selection
# Available: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022, claude-3-opus-20240229
# Recommended: claude-3-5-haiku-20241022 (fast, cost-effective)
CLAUDE_MODEL=claude-3-5-haiku-20241022

# =============================================================================
# War Room MVP Configuration
# =============================================================================
# War Room MVP Execution Mode
# - false: Direct class mode (기본값, 기존 WarRoomMVP 클래스 직접 사용)
# - true: Skill handler mode (새로운 agent skills 인터페이스 사용)
# Phase: Skills Migration (2026-01-02)
WAR_ROOM_MVP_USE_SKILLS=false

# =============================================================================
# News Poller Configuration
# =============================================================================
# ⚠️ IMPORTANT: News Poller LLM Model Selection
#
# News Poller consists of multiple components, each using different LLMs:
#   1. DeepReasoningAgent: Analyzes geopolitical/chip war events
#   2. UnifiedNewsProcessor: Extracts tickers/sectors from news
#   3. NewsDeepAnalyzer: Sentiment/urgency analysis (uses Ollama local)
#
# ALWAYS VERIFY: https://z.ai/manage-apikey/rate-limits
#
# Model Recommendations (by Concurrency Limit):
# - GLM-4-Plus: Concurrency 20 ✅ RECOMMENDED for NewsPoller
# - GLM-4-Flash: Concurrency 20 ✅ Alternative (faster)
# - GLM-4-FlashX: Concurrency 20 ✅ Alternative (very fast)
# - GLM-4.7: Concurrency 3 ❌ Too low for parallel news processing
# - GLM-4.6V-FlashX: Concurrency 3 ❌ Too low for parallel processing
# - Gemini 2.5 Flash: Concurrency 60+ ✅ Good alternative option
# =============================================================================

# Deep Reasoning Agent Model (geopolitical/chip war event analysis)
# Used by: DeepReasoningAgent in news_poller.py
# Options: GLM-4-Plus (Concurrency 20), GLM-4.7 (Concurrency 3), Gemini (Concurrency 60+)
# Recommended: GLM-4-Plus (high concurrency: 20)
# ALWAYS CHECK: https://z.ai/manage-apikey/rate-limits
DEEP_REASONING_MODEL=glm-4-plus
DEEP_REASONING_PROVIDER=glm

# Unified News Processor Model (ticker/sector extraction)
# Used by: UnifiedNewsProcessor in unified_news_processor.py
# Options: GLM-4-Plus (Concurrency 20), GLM-4-Flash (Concurrency 20), Gemini (Concurrency 60+)
# Recommended: GLM-4-Plus (high concurrency: 20, fast processing)
# ALWAYS CHECK: https://z.ai/manage-apikey/rate-limits
NEWS_GLM_MODEL=glm-4-plus

# GLM API Enable/Disable (Cost Control)
# true: Use GLM API (costs money, but better accuracy)
# false: Use MockGLMClient (COST: $0, uses local Ollama instead)
# Recommended: Set to false if you see "잔액 부족" errors or want to save costs
NEWS_GLM_ENABLED=true

# Local LLM Selection (Ollama vs GLM)
# true: Use Ollama Local LLM (COST: $0, completely free)
# false: Use GLM API (costs money, requires API key)
# Recommended: Set to true to save costs and avoid rate limit issues
NEWS_USE_OLLAMA=true

# GLM API Rate Limiting (seconds between calls)
# CRITICAL: Prevents "잔액 부족" errors (actually Rate Limit issues)
#
# Calculation Formula:
#   Safe Rate = Concurrency Limit / 2 (conservative estimate)
#   For GLM-4-Plus (Concurrency 20): Safe Rate = 10 requests/second
#   Delay = 1 / Safe Rate = 0.1 seconds minimum
#
# Recommended Settings:
#   NEWS_GLM_RATE_LIMIT=10.0 → 0.1 req/sec (MAXIMUM STABILITY, safest)
#   NEWS_GLM_RATE_LIMIT=5.0  → 0.2 req/sec (very stable, recommended)
#   NEWS_GLM_RATE_LIMIT=3.0  → 0.33 req/sec (stable)
#   NEWS_GLM_RATE_LIMIT=2.0  → 0.5 req/sec (may hit limit with GLM-4.7)
#   NEWS_GLM_RATE_LIMIT=1.0  → 1 req/sec (risky)
#   NEWS_GLM_RATE_LIMIT=0.5  → 2 req/sec (very risky)
#   NEWS_GLM_RATE_LIMIT=0.3  → 3.3 req/sec (WILL HIT LIMIT)
#
# If you see "잔액 부족" error:
#   1. First: INCREASE NEWS_GLM_RATE_LIMIT (1.0 → 2.0 → 3.0 → 5.0 → 10.0)
#   2. Second: DECREASE NEWS_GLM_CONCURRENCY (3 → 2 → 1)
#   3. Check current limits: https://z.ai/manage-apikey/rate-limits
#   4. Last: Consider API recharge
NEWS_GLM_RATE_LIMIT=5.0

# GLM API Concurrency Limit (simultaneous requests)
# CRITICAL: Prevents bursting requests that exceed GLM's Concurrency Limit
# This uses asyncio.Semaphore to limit the number of concurrent API calls
#
# Recommended Settings:
#   NEWS_GLM_CONCURRENCY=1   (MAXIMUM STABILITY, safest for GLM-4.7 Limit 3)
#   NEWS_GLM_CONCURRENCY=2   (very stable)
#   NEWS_GLM_CONCURRENCY=3   (stable, but may hit limit with multiple agents)
#   NEWS_GLM_CONCURRENCY=5   (balanced, for GLM-4-Plus Limit 20)
#   NEWS_GLM_CONCURRENCY=10  (aggressive, may hit limit)
#
# GLM-4-Plus has Concurrency Limit 20, but GLM-4.7/4.6V have Limit 3
# For Risk Agent (uses GLM-4.7 + GLM-4.6V), use CONCURRENCY=1 to prevent burst
NEWS_GLM_CONCURRENCY=1

# News Analysis Priority (for UnifiedNewsProcessor)
# true = Analyze all news (more cost, more signals)
# false = Analyze only critical news (cost-effective)
NEWS_ANALYZE_ALL=false

# News Poller Interval (seconds)
# Default: 300 (5 minutes)
NEWS_POLLER_INTERVAL=300

# =============================================================================
# Benefits of GLM-4-Plus for News Processing:
# - High Concurrency Limit: 20 (vs GLM-4.7: 3, GLM-4.6V: 3)
# - Fast processing for news analysis
# - Cost-effective
# - Accurate ticker/sector extraction
# - JSON mode support for structured output
# =============================================================================

# System
TELEGRAM_BOT_TOKEN=
TELEGRAM_CHAT_ID=
TELEGRAM_COMMANDER_CHAT_ID=
TELEGRAM_ENABLED=true

# Other
ALLOWED_ORIGINS=
APP_DEBUG=false
APP_ENV=production
APP_HOST=
APP_LOG_LEVEL=INFO
APP_PORT=3000
COMPOSE_PROJECT_NAME=ai-trading-system
EMAIL_ENABLED=false
EMAIL_FROM=your-email@gmail.com
EMAIL_PASSWORD=your_app_password
EMAIL_SMTP_PORT=587
EMAIL_SMTP_SERVER=smtp.gmail.com
EMAIL_TO=alerts@yourdomain.com
FEATURE_AUTO_TRADING=false
FEATURE_CACHE_TTL_DAILY=86400
FEATURE_CACHE_TTL_INTRADAY=300
FEATURE_CACHE_WARM_UP_TICKERS=AAPL,MSFT,NVDA,GOOGL,TSLA
FEATURE_CEO_ANALYSIS=true
FEATURE_INCREMENTAL_UPDATE=true
FEATURE_NEWS_FILTER=true
FEATURE_PAPER_TRADING=true
FEATURE_SEC_SEARCH=true
GRAFANA_ADMIN_PASSWORD=
GRAFANA_DATA=./data/grafana
GRAFANA_PASSWORD=
GRAFANA_ROOT_URL=http://192.168.50.148:3001
GRAFANA_USER=admin
KILL_SWITCH_DAILY_LOSS_PCT=2.0
KILL_SWITCH_ENABLED=true
MAX_POSITIONS=10
MAX_POSITION_SIZE_PCT=5.0
NGINX_HTTPS_PORT=443
NGINX_HTTP_PORT=80
PROMETHEUS_DATA=./data/prometheus
PROMETHEUS_PORT=9090
SECRET_KEY=
SLACK_ENABLED=false
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
STOCK_PRICE_MAX_RETRIES=3
STOCK_PRICE_RETRY_DELAY=300
STOCK_PRICE_UPDATE_TIME=06:00
STOP_LOSS_FIXED_PCT=3.0
STORAGE_BASE_PATH=/volume1/ai_trading
TRADING_END_TIME=15:30
TRADING_START_TIME=09:00
TZ=Asia/Seoul
WAR_ROOM_MVP_USE_SKILLS=true
# Disable embedded news poller (run backend/run_news_crawler.py instead)
DISABLE_EMBEDDED_NEWS_POLLER=1
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
USE_LOCAL_LLM=true
# Embedding Configuration
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DIMENSION=384

# =============================================================================
# Frontend Configuration
# =============================================================================
# VITE_API_URL - Backend API URL (default: http://localhost:8001)
VITE_API_URL=http://localhost:8001

# VITE_USE_MOCK_DATA - Use mock data for testing (default: false)
# Set to 'true' ONLY for development/testing to see sample portfolio data
# Production should always be 'false' to use real account data
VITE_USE_MOCK_DATA=false