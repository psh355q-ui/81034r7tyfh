"""
backtest_router.py - ë°±í…ŒìŠ¤íŠ¸ API

ğŸ“Š Data Sources:
    - PostgreSQL: analysis_results, news_articles í…Œì´ë¸”
        - ê³¼ê±° ë‰´ìŠ¤ ë¶„ì„ ë°ì´í„° ì¡°íšŒ
        - ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ë³„ í•„í„°ë§
    - KIS API: ê³¼ê±° ì£¼ê°€ ë°ì´í„°
        - kis_client.inquire_daily_price: êµ­ë‚´ì£¼ì‹ ì¼ë´‰
        - overseas_stock.get_daily_price: í•´ì™¸ì£¼ì‹ ì¼ë´‰
    - SignalBacktestEngine: ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„
        - ì‹œê·¸ë„ ìƒì„± ì‹œë®¬ë ˆì´ì…˜
        - ê±°ë˜ ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜
        - ì„±ê³¼ ì§€í‘œ ê³„ì‚°
    - File System: ./backtest_results/*.json
        - ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì˜êµ¬ ì €ì¥
        - ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ê´€ë¦¬

ğŸ”— External Dependencies:
    - fastapi: API ë¼ìš°íŒ…, BackgroundTasks
    - pydantic: ì„¤ì • ëª¨ë¸ ê²€ì¦
    - backend.backtesting.signal_backtest_engine: ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„
    - backend.backtesting.consensus_backtest: Consensus ë°±í…ŒìŠ¤íŠ¸
    - uuid: Job ID ìƒì„±
    - asyncio: ë¹„ë™ê¸° ì‹¤í–‰

ğŸ“¤ API Endpoints:
    - POST /backtest/run: ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë¹„ë™ê¸°)
    - GET /backtest/results: ë°±í…ŒìŠ¤íŠ¸ ëª©ë¡
    - GET /backtest/results/{id}: ìƒì„¸ ê²°ê³¼ ì¡°íšŒ
    - GET /backtest/status/{id}: ì‘ì—… ìƒíƒœ í™•ì¸
    - DELETE /backtest/results/{id}: ê²°ê³¼ ì‚­ì œ
    - POST /backtest/optimize: íŒŒë¼ë¯¸í„° Grid Search ìµœì í™”
    - POST /backtest/compare: ì—¬ëŸ¬ ë°±í…ŒìŠ¤íŠ¸ ë¹„êµ
    - POST /backtest/consensus/run: Consensus ë°±í…ŒìŠ¤íŠ¸

ğŸ”„ Called By:
    - frontend/src/pages/Backtest.tsx
    - frontend/src/components/Backtest/BacktestRunner.tsx
    - frontend/src/components/Backtest/ComparisonChart.tsx

ğŸ“ Notes:
    - ë°±í…ŒìŠ¤íŠ¸ëŠ” BackgroundTasksë¡œ ë¹„ë™ê¸° ì‹¤í–‰
    - ê²°ê³¼ëŠ” JSON íŒŒì¼ë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ + ë””ìŠ¤í¬)
    - ì‹¤ì œ ë°ì´í„° vs ìƒ˜í”Œ ë°ì´í„° ì„ íƒ ê°€ëŠ¥
    - Grid Searchë¡œ íŒŒë¼ë¯¸í„° ìµœì í™” ì§€ì›
    - Consensus ì „ëµ ë³„ë„ ì—”ë“œí¬ì¸íŠ¸

Phase 10: Signal Backtest API Router
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import json
import uuid
import asyncio
from pathlib import Path

# Import backtest engine
from backend.backtesting.signal_backtest_engine import (
    SignalBacktestEngine,
    NewsAnalysis,
    BacktestResult
)

router = APIRouter(prefix="/backtest", tags=["backtest"])

# ê²°ê³¼ ì €ì¥ì†Œ (ì‹¤ì œë¡œëŠ” DB ì‚¬ìš©)
RESULTS_DIR = Path("./backtest_results")
try:
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)
except PermissionError:
    # Docker í™˜ê²½ì—ì„œ ê¶Œí•œ ë¬¸ì œ ë°œìƒ ì‹œ /tmp ì‚¬ìš©
    RESULTS_DIR = Path("/tmp/backtest_results")
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)


# =============================================================================
# REQUEST/RESPONSE MODELS
# =============================================================================

class BacktestConfig(BaseModel):
    """ë°±í…ŒìŠ¤íŠ¸ ì„¤ì •"""
    start_date: str = Field(..., description="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    end_date: str = Field(..., description="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
    initial_capital: float = Field(default=100000.0, description="ì´ˆê¸° ìë³¸ê¸ˆ")
    
    # ê±°ë˜ ë¹„ìš©
    commission_rate: float = Field(default=0.00015, description="ìˆ˜ìˆ˜ë£Œìœ¨ (0.00015 = 0.015%)")
    slippage_bps: float = Field(default=1.0, description="ìŠ¬ë¦¬í”¼ì§€ (basis points)")
    
    # í¬ì§€ì…˜ ê´€ë¦¬
    max_holding_days: int = Field(default=5, description="ìµœëŒ€ ë³´ìœ  ê¸°ê°„")
    stop_loss_pct: float = Field(default=2.0, description="ì†ì ˆ í¼ì„¼íŠ¸")
    take_profit_pct: float = Field(default=5.0, description="ìµì ˆ í¼ì„¼íŠ¸")
    
    # ì‹œê·¸ë„ ìƒì„± íŒŒë¼ë¯¸í„°
    base_position_size: float = Field(default=0.05, description="ê¸°ë³¸ í¬ì§€ì…˜ í¬ê¸° (í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ìœ¨)")
    max_position_size: float = Field(default=0.10, description="ìµœëŒ€ í¬ì§€ì…˜ í¬ê¸°")
    min_sentiment_threshold: float = Field(default=0.7, description="ìµœì†Œ ê°ì • ì„ê³„ê°’")
    min_relevance_score: int = Field(default=70, description="ìµœì†Œ ê´€ë ¨ì„± ì ìˆ˜")
    
    # ê²€ì¦ íŒŒë¼ë¯¸í„°
    min_confidence: float = Field(default=0.7, description="ìµœì†Œ ì‹ ë¢°ë„")
    max_daily_trades: int = Field(default=10, description="ì¼ì¼ ìµœëŒ€ ê±°ë˜ íšŸìˆ˜")
    daily_loss_limit_pct: float = Field(default=2.0, description="ì¼ì¼ ì†ì‹¤ ì œí•œ (%)")


class BacktestRunRequest(BaseModel):
    """ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìš”ì²­"""
    config: BacktestConfig
    name: str = Field(default="Backtest", description="ë°±í…ŒìŠ¤íŠ¸ ì´ë¦„")
    description: str = Field(default="", description="ì„¤ëª…")
    use_real_data: bool = Field(default=True, description="ì‹¤ì œ DB ë°ì´í„° ì‚¬ìš© ì—¬ë¶€")


class BacktestRunResponse(BaseModel):
    """ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‘ë‹µ"""
    id: str
    status: str  # PENDING, RUNNING, COMPLETED, FAILED
    message: str
    created_at: str


class BacktestResultSummary(BaseModel):
    """ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½"""
    id: str
    name: str
    status: str
    created_at: str
    
    # ì£¼ìš” ì§€í‘œ
    total_return_pct: Optional[float] = None
    sharpe_ratio: Optional[float] = None
    max_drawdown_pct: Optional[float] = None
    win_rate: Optional[float] = None
    total_trades: Optional[int] = None


class OptimizationRequest(BaseModel):
    """íŒŒë¼ë¯¸í„° ìµœì í™” ìš”ì²­"""
    base_config: BacktestConfig
    
    # ìµœì í™”í•  íŒŒë¼ë¯¸í„° ë²”ìœ„
    param_ranges: Dict[str, List[float]] = Field(
        default={
            "min_sentiment_threshold": [0.6, 0.7, 0.8],
            "stop_loss_pct": [1.5, 2.0, 2.5, 3.0],
            "take_profit_pct": [3.0, 5.0, 7.0, 10.0]
        },
        description="ìµœì í™”í•  íŒŒë¼ë¯¸í„°ì™€ í…ŒìŠ¤íŠ¸í•  ê°’ë“¤"
    )
    
    optimization_metric: str = Field(
        default="sharpe_ratio",
        description="ìµœì í™” ê¸°ì¤€ ì§€í‘œ"
    )


class OptimizationResult(BaseModel):
    """ìµœì í™” ê²°ê³¼"""
    best_params: Dict[str, float]
    best_score: float
    optimization_metric: str
    all_results: List[Dict[str, Any]]
    total_combinations: int
    completed_at: str


class ComparisonRequest(BaseModel):
    """ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¹„êµ ìš”ì²­"""
    backtest_ids: List[str] = Field(..., description="ë¹„êµí•  ë°±í…ŒìŠ¤íŠ¸ ID ëª©ë¡")


class ComparisonResult(BaseModel):
    """ë°±í…ŒìŠ¤íŠ¸ ë¹„êµ ê²°ê³¼"""
    backtests: List[Dict[str, Any]]
    best_by_metric: Dict[str, str]  # metric -> backtest_id
    analysis: Dict[str, Any]


# =============================================================================
# IN-MEMORY STATE (ì‹¤ì œë¡œëŠ” Redis ë˜ëŠ” DB ì‚¬ìš©)
# =============================================================================

backtest_jobs: Dict[str, Dict] = {}


# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def load_news_analyses_from_db() -> List[NewsAnalysis]:
    """DBì—ì„œ ë‰´ìŠ¤ ë¶„ì„ ë°ì´í„° ë¡œë“œ (ì‹¤ì œ êµ¬í˜„ í•„ìš”)"""
    # TODO: ì‹¤ì œ DB ì¿¼ë¦¬ êµ¬í˜„
    # í˜„ì¬ëŠ” ë¹ˆ ëª©ë¡ ë°˜í™˜
    return []


def load_price_data_from_db(
    tickers: List[str],
    start_date: datetime,
    end_date: datetime
) -> Dict[str, Dict[str, float]]:
    """DBì—ì„œ ê°€ê²© ë°ì´í„° ë¡œë“œ (ì‹¤ì œ êµ¬í˜„ í•„ìš”)"""
    try:
        from backend.trading import kis_client
        from backend.trading import overseas_stock
        import time
        
        result = {}
        for ticker in tickers:
            ticker_data = {}
            
            # Domestic Stock (6 digits)
            if ticker.isdigit() and len(ticker) == 6:
                daily_prices = kis_client.inquire_daily_price(ticker, period="D")
                for dp in daily_prices:
                    date_str = dp.get("stck_bsop_date")
                    if date_str:
                        try:
                            dt_str = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
                            dt_obj = datetime.strptime(dt_str, "%Y-%m-%d")
                            if start_date <= dt_obj <= end_date:
                                ticker_data[dt_str] = float(dp.get("stck_clpr", 0))
                        except Exception:
                            continue

            # Overseas Stock (Alphabet)
            else:
                # Try major exchanges
                for exc in ["NASD", "NYSE", "AMEX"]:
                    daily_prices = overseas_stock.get_daily_price(exc, ticker.upper(), period="D")
                    if daily_prices:
                        break
                
                for dp in daily_prices:
                    # Overseas fields: xymd (date), clos (close price)
                    date_str = dp.get("xymd")
                    if date_str:
                        try:
                            dt_str = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
                            dt_obj = datetime.strptime(dt_str, "%Y-%m-%d")
                            if start_date <= dt_obj <= end_date:
                                # ovrs_nmix_prpr or clos check
                                price = float(dp.get("clos", dp.get("ovrs_nmix_prpr", 0)))
                                ticker_data[dt_str] = price
                        except Exception:
                            continue
            
            if ticker_data:
                result[ticker] = ticker_data
                print(f"  -> {ticker}: {len(ticker_data)} days loaded")
            else:
                print(f"  -> {ticker}: No data found")
            
            time.sleep(0.1)
            
        print(f"âœ… Loaded real price data for {len(result)} tickers")
        return result
        
    except Exception as e:
        print(f"âš ï¸ Failed to load real price data: {e}")
        return {}


def generate_sample_data():
    """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    import random
    random.seed(42)
    
    # 30ì¼ ìƒ˜í”Œ ë‰´ìŠ¤ ë¶„ì„ ë°ì´í„°
    analyses = []
    tickers = ["AAPL", "TSLA", "MSFT", "NVDA", "GOOGL", "AMZN", "META"]
    
    start_date = datetime(2024, 1, 1)
    
    for i in range(20):  # 20ê°œì˜ ë‰´ìŠ¤ ë¶„ì„
        day_offset = random.randint(0, 29)
        news_date = start_date + timedelta(days=day_offset)
        
        ticker = random.choice(tickers)
        
        # ëœë¤ ê°ì •
        sentiment_value = random.gauss(0, 0.5)
        if sentiment_value > 0.3:
            sentiment = "POSITIVE"
        elif sentiment_value < -0.3:
            sentiment = "NEGATIVE"
        else:
            sentiment = "NEUTRAL"
        
        analyses.append(NewsAnalysis(
            id=f"analysis_{i:03d}",
            article_id=f"article_{i:03d}",
            crawled_at=news_date + timedelta(hours=random.randint(9, 16)),
            analyzed_at=news_date + timedelta(hours=random.randint(9, 16), minutes=10),
            sentiment_overall=sentiment,
            sentiment_score=abs(sentiment_value),
            sentiment_confidence=random.uniform(0.6, 0.95),
            urgency=random.choice(["IMMEDIATE", "SHORT_TERM", "LONG_TERM"]),
            impact_magnitude=random.uniform(0.3, 0.9),
            risk_category=random.choice(["LOW", "MEDIUM", "HIGH"]),
            key_facts=[f"Key fact {j}" for j in range(3)],
            related_tickers=[{
                "ticker_symbol": ticker,
                "relevance_score": random.randint(70, 95)
            }]
        ))
    
    # 30ì¼ ê°€ê²© ë°ì´í„°
    price_data = {}
    base_prices = {
        "AAPL": 180.0, "TSLA": 250.0, "MSFT": 350.0,
        "NVDA": 500.0, "GOOGL": 140.0, "AMZN": 150.0, "META": 350.0
    }
    
    for day in range(30):
        date = start_date + timedelta(days=day)
        date_str = date.strftime("%Y-%m-%d")
        
        price_data[date_str] = {}
        
        for ticker, base_price in base_prices.items():
            noise = random.gauss(0, 0.015)  # 1.5% í‘œì¤€í¸ì°¨
            trend = 0.001 * day  # ì¼ì¼ 0.1% ìƒìŠ¹
            price = base_price * (1 + trend + noise)
            price_data[date_str][ticker] = round(price, 2)
    
    return analyses, price_data, start_date, start_date + timedelta(days=29)


async def run_backtest_async(job_id: str, config: BacktestConfig, use_real_data: bool):
    """ë¹„ë™ê¸° ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    
    try:
        backtest_jobs[job_id]["status"] = "RUNNING"
        backtest_jobs[job_id]["started_at"] = datetime.now().isoformat()
        
        # ë°ì´í„° ë¡œë“œ
        if use_real_data:
            analyses = load_news_analyses_from_db()
            start_date = datetime.strptime(config.start_date, "%Y-%m-%d")
            end_date = datetime.strptime(config.end_date, "%Y-%m-%d")
            
            # í‹°ì»¤ ì¶”ì¶œ
            tickers = set()
            for analysis in analyses:
                for ticker_info in analysis.related_tickers:
                    tickers.add(ticker_info.get("ticker_symbol"))
            
            price_data = load_price_data_from_db(list(tickers), start_date, end_date)
        else:
            # ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©
            analyses, price_data, start_date, end_date = generate_sample_data()
        
        # ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ ìƒì„±
        engine = SignalBacktestEngine(
            initial_capital=config.initial_capital,
            commission_rate=config.commission_rate,
            slippage_bps=config.slippage_bps,
            max_holding_days=config.max_holding_days,
            stop_loss_pct=config.stop_loss_pct,
            take_profit_pct=config.take_profit_pct
        )
        
        # ì‹œê·¸ë„ ìƒì„±ê¸° ì„¤ì •
        engine.signal_generator.base_position_size = config.base_position_size
        engine.signal_generator.max_position_size = config.max_position_size
        engine.signal_generator.min_sentiment_threshold = config.min_sentiment_threshold
        engine.signal_generator.min_relevance_score = config.min_relevance_score
        
        # ê²€ì¦ê¸° ì„¤ì •
        engine.signal_validator.min_confidence = config.min_confidence
        engine.signal_validator.max_daily_trades = config.max_daily_trades
        engine.signal_validator.daily_loss_limit_pct = config.daily_loss_limit_pct
        
        # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = await engine.run(analyses, price_data, start_date, end_date)
        
        # ê²°ê³¼ ì €ì¥
        result_dict = {
            "id": job_id,
            "name": backtest_jobs[job_id]["name"],
            "description": backtest_jobs[job_id]["description"],
            "config": config.dict(),
            "result": {
                "start_date": result.start_date,
                "end_date": result.end_date,
                "initial_capital": result.initial_capital,
                "final_value": result.final_value,
                "total_return_pct": result.total_return_pct,
                "sharpe_ratio": result.sharpe_ratio,
                "max_drawdown_pct": result.max_drawdown_pct,
                "win_rate": result.win_rate,
                "total_trades": result.total_trades,
                "winning_trades": result.winning_trades,
                "losing_trades": result.losing_trades,
                "avg_win_pct": result.avg_win_pct,
                "avg_loss_pct": result.avg_loss_pct,
                "profit_factor": result.profit_factor,
                "total_signals": result.total_signals,
                "executed_signals": result.executed_signals,
                "rejected_signals": result.rejected_signals,
                "best_day_pct": result.best_day_pct,
                "worst_day_pct": result.worst_day_pct,
                "avg_daily_return_pct": result.avg_daily_return_pct,
                "parameters": result.parameters,
                "daily_values": result.daily_values,
                "trades": result.trades
            },
            "created_at": backtest_jobs[job_id]["created_at"],
            "completed_at": datetime.now().isoformat()
        }
        
        # íŒŒì¼ë¡œ ì €ì¥
        result_file = RESULTS_DIR / f"{job_id}.json"
        with open(result_file, "w") as f:
            json.dump(result_dict, f, indent=2, default=str)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        backtest_jobs[job_id]["status"] = "COMPLETED"
        backtest_jobs[job_id]["completed_at"] = datetime.now().isoformat()
        backtest_jobs[job_id]["result_file"] = str(result_file)
        
    except Exception as e:
        backtest_jobs[job_id]["status"] = "FAILED"
        backtest_jobs[job_id]["error"] = str(e)
        backtest_jobs[job_id]["completed_at"] = datetime.now().isoformat()


# =============================================================================
# API ENDPOINTS
# =============================================================================

@router.post("/run", response_model=BacktestRunResponse)
async def run_backtest(
    request: BacktestRunRequest,
    background_tasks: BackgroundTasks
):
    """
    ìƒˆ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    - ë¹„ë™ê¸°ë¡œ ì‹¤í–‰ë˜ë©°, job_idë¥¼ ë°˜í™˜
    - GET /results/{id}ë¡œ ê²°ê³¼ í™•ì¸
    """
    
    job_id = str(uuid.uuid4())
    
    # ì‘ì—… ë“±ë¡
    backtest_jobs[job_id] = {
        "id": job_id,
        "name": request.name,
        "description": request.description,
        "status": "PENDING",
        "config": request.config.dict(),
        "created_at": datetime.now().isoformat(),
        "started_at": None,
        "completed_at": None,
        "result_file": None,
        "error": None
    }
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
    background_tasks.add_task(
        run_backtest_async,
        job_id,
        request.config,
        request.use_real_data
    )
    
    return BacktestRunResponse(
        id=job_id,
        status="PENDING",
        message="Backtest job created. Check results with GET /results/{id}",
        created_at=backtest_jobs[job_id]["created_at"]
    )


@router.get("/results", response_model=List[BacktestResultSummary])
async def get_backtest_results():
    """
    ëª¨ë“  ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ
    """
    
    results = []
    
    for job_id, job in backtest_jobs.items():
        summary = BacktestResultSummary(
            id=job_id,
            name=job["name"],
            status=job["status"],
            created_at=job["created_at"]
        )
        
        # ì™„ë£Œëœ ê²½ìš° ì§€í‘œ ì¶”ê°€
        if job["status"] == "COMPLETED" and job["result_file"]:
            try:
                with open(job["result_file"], "r") as f:
                    result_data = json.load(f)
                    result = result_data["result"]
                    
                    summary.total_return_pct = result["total_return_pct"]
                    summary.sharpe_ratio = result["sharpe_ratio"]
                    summary.max_drawdown_pct = result["max_drawdown_pct"]
                    summary.win_rate = result["win_rate"]
                    summary.total_trades = result["total_trades"]
            except Exception:
                pass
        
        results.append(summary)
    
    # ìµœì‹ ìˆœ ì •ë ¬
    results.sort(key=lambda x: x.created_at, reverse=True)
    
    return results


@router.get("/results/{job_id}")
async def get_backtest_result(job_id: str):
    """
    íŠ¹ì • ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„¸ ì¡°íšŒ
    """
    
    if job_id not in backtest_jobs:
        raise HTTPException(status_code=404, detail=f"Backtest {job_id} not found")
    
    job = backtest_jobs[job_id]
    
    # ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì€ ê²½ìš°
    if job["status"] in ["PENDING", "RUNNING"]:
        return {
            "id": job_id,
            "status": job["status"],
            "message": "Backtest is still running. Please wait.",
            "created_at": job["created_at"],
            "started_at": job.get("started_at")
        }
    
    # ì‹¤íŒ¨í•œ ê²½ìš°
    if job["status"] == "FAILED":
        return {
            "id": job_id,
            "status": "FAILED",
            "error": job.get("error", "Unknown error"),
            "created_at": job["created_at"],
            "completed_at": job.get("completed_at")
        }
    
    # ì™„ë£Œëœ ê²½ìš° - íŒŒì¼ì—ì„œ ê²°ê³¼ ë¡œë“œ
    if job["result_file"]:
        try:
            with open(job["result_file"], "r") as f:
                return json.load(f)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to load result: {str(e)}")
    
    raise HTTPException(status_code=500, detail="Result file not found")


@router.post("/optimize", response_model=OptimizationResult)
async def optimize_parameters(request: OptimizationRequest):
    """
    íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰
    
    Grid Search ë°©ì‹ìœ¼ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì¡°í•© íƒìƒ‰
    """
    
    from itertools import product
    
    # íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„±
    param_names = list(request.param_ranges.keys())
    param_values = list(request.param_ranges.values())
    combinations = list(product(*param_values))
    
    results = []
    best_score = float('-inf') if request.optimization_metric != "max_drawdown_pct" else float('inf')
    best_params = {}
    
    # ê° ì¡°í•©ì— ëŒ€í•´ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    for combo in combinations:
        # ì„¤ì • ë³µì‚¬
        config = request.base_config.copy()
        
        # íŒŒë¼ë¯¸í„° ì ìš©
        params = dict(zip(param_names, combo))
        for key, value in params.items():
            setattr(config, key, value)
        
        # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìƒ˜í”Œ ë°ì´í„°)
        analyses, price_data, start_date, end_date = generate_sample_data()
        
        engine = SignalBacktestEngine(
            initial_capital=config.initial_capital,
            commission_rate=config.commission_rate,
            slippage_bps=config.slippage_bps,
            max_holding_days=config.max_holding_days,
            stop_loss_pct=config.stop_loss_pct,
            take_profit_pct=config.take_profit_pct
        )
        
        engine.signal_generator.base_position_size = config.base_position_size
        engine.signal_generator.max_position_size = config.max_position_size
        engine.signal_generator.min_sentiment_threshold = config.min_sentiment_threshold
        engine.signal_validator.min_confidence = config.min_confidence
        
        result = await engine.run(analyses, price_data, start_date, end_date)
        
        # ì ìˆ˜ ì¶”ì¶œ
        score = getattr(result, request.optimization_metric, 0)
        
        # ê²°ê³¼ ê¸°ë¡
        results.append({
            "params": params,
            "score": score,
            "total_return_pct": result.total_return_pct,
            "sharpe_ratio": result.sharpe_ratio,
            "max_drawdown_pct": result.max_drawdown_pct,
            "win_rate": result.win_rate,
            "total_trades": result.total_trades
        })
        
        # ìµœì ê°’ ì—…ë°ì´íŠ¸
        if request.optimization_metric == "max_drawdown_pct":
            # Drawdownì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (ëœ ìŒìˆ˜)
            if score > best_score:
                best_score = score
                best_params = params
        else:
            # ë‚˜ë¨¸ì§€ëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
            if score > best_score:
                best_score = score
                best_params = params
    
    return OptimizationResult(
        best_params=best_params,
        best_score=best_score,
        optimization_metric=request.optimization_metric,
        all_results=results,
        total_combinations=len(combinations),
        completed_at=datetime.now().isoformat()
    )


@router.post("/compare", response_model=ComparisonResult)
async def compare_backtests(request: ComparisonRequest):
    """
    ì—¬ëŸ¬ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¹„êµ
    """
    
    backtests = []
    
    for job_id in request.backtest_ids:
        if job_id not in backtest_jobs:
            continue
        
        job = backtest_jobs[job_id]
        
        if job["status"] != "COMPLETED" or not job["result_file"]:
            continue
        
        try:
            with open(job["result_file"], "r") as f:
                result_data = json.load(f)
                backtests.append({
                    "id": job_id,
                    "name": job["name"],
                    "result": result_data["result"],
                    "config": result_data["config"]
                })
        except Exception:
            continue
    
    if not backtests:
        raise HTTPException(status_code=404, detail="No valid backtests found")
    
    # ê° ì§€í‘œë³„ ìµœê³  ë°±í…ŒìŠ¤íŠ¸ ì°¾ê¸°
    metrics = ["total_return_pct", "sharpe_ratio", "win_rate", "profit_factor"]
    best_by_metric = {}
    
    for metric in metrics:
        best_id = max(backtests, key=lambda x: x["result"].get(metric, 0))["id"]
        best_by_metric[metric] = best_id
    
    # Max Drawdownì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
    best_drawdown = max(backtests, key=lambda x: x["result"].get("max_drawdown_pct", -100))
    best_by_metric["max_drawdown_pct"] = best_drawdown["id"]
    
    # ì¢…í•© ë¶„ì„
    analysis = {
        "average_return": sum(b["result"]["total_return_pct"] for b in backtests) / len(backtests),
        "average_sharpe": sum(b["result"]["sharpe_ratio"] for b in backtests) / len(backtests),
        "average_win_rate": sum(b["result"]["win_rate"] for b in backtests) / len(backtests),
        "total_backtests": len(backtests),
        "recommendation": ""
    }
    
    # ì¶”ì²œ
    best_overall = max(
        backtests,
        key=lambda x: (
            x["result"]["sharpe_ratio"] * 0.4 +
            x["result"]["total_return_pct"] * 0.3 +
            (1 + x["result"]["max_drawdown_pct"] / 100) * 0.3  # Drawdownì€ ìŒìˆ˜ì´ë¯€ë¡œ ì¡°ì •
        )
    )
    analysis["recommendation"] = f"Best overall: {best_overall['name']} (ID: {best_overall['id']})"
    
    return ComparisonResult(
        backtests=[{
            "id": b["id"],
            "name": b["name"],
            "total_return_pct": b["result"]["total_return_pct"],
            "sharpe_ratio": b["result"]["sharpe_ratio"],
            "max_drawdown_pct": b["result"]["max_drawdown_pct"],
            "win_rate": b["result"]["win_rate"],
            "total_trades": b["result"]["total_trades"],
            "profit_factor": b["result"]["profit_factor"]
        } for b in backtests],
        best_by_metric=best_by_metric,
        analysis=analysis
    )


@router.delete("/results/{job_id}")
async def delete_backtest_result(job_id: str):
    """
    ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì‚­ì œ
    """
    
    if job_id not in backtest_jobs:
        raise HTTPException(status_code=404, detail=f"Backtest {job_id} not found")
    
    job = backtest_jobs[job_id]
    
    # íŒŒì¼ ì‚­ì œ
    if job.get("result_file"):
        try:
            Path(job["result_file"]).unlink(missing_ok=True)
        except Exception:
            pass
    
    # ë©”ëª¨ë¦¬ì—ì„œ ì‚­ì œ
    del backtest_jobs[job_id]
    
    return {"message": f"Backtest {job_id} deleted successfully"}


@router.get("/status/{job_id}")
async def get_backtest_status(job_id: str):
    """
    ë°±í…ŒìŠ¤íŠ¸ ì‘ì—… ìƒíƒœ í™•ì¸
    """
    
    if job_id not in backtest_jobs:
        raise HTTPException(status_code=404, detail=f"Backtest {job_id} not found")
    
    job = backtest_jobs[job_id]
    
    return {
        "id": job_id,
        "status": job["status"],
        "created_at": job["created_at"],
        "started_at": job.get("started_at"),
        "completed_at": job.get("completed_at"),
        "error": job.get("error")
    }


# =============================================================================
# CONSENSUS BACKTEST ENDPOINTS
# =============================================================================

class ConsensusBacktestRequest(BaseModel):
    """Consensus ë°±í…ŒìŠ¤íŠ¸ ìš”ì²­"""
    tickers: List[str] = Field(default=["NVDA", "AMD", "AAPL"], description="í…ŒìŠ¤íŠ¸í•  ì¢…ëª©ë“¤")
    start_date: str = Field(default="2024-01-01", description="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    end_date: str = Field(default="2024-06-01", description="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
    initial_capital: float = Field(default=100000.0, description="ì´ˆê¸° ìë³¸ê¸ˆ")
    consensus_threshold: float = Field(default=0.6, description="Consensus ìŠ¹ì¸ ì„ê³„ê°’ (0.6 = 60%)")
    use_mock_consensus: bool = Field(default=False, description="ì‹¤ì œ Consensus ì‚¬ìš©")
    
    class Config:
        json_schema_extra = {
            "example": {
                "tickers": ["NVDA", "AMD", "AAPL"],
                "start_date": "2024-01-01",
                "end_date": "2024-06-01",
                "initial_capital": 100000.0,
                "consensus_threshold": 0.6,
                "use_mock_consensus": True
            }
        }


consensus_backtest_jobs: Dict[str, Dict] = {}


async def run_consensus_backtest_async(job_id: str, request: ConsensusBacktestRequest):
    """ë¹„ë™ê¸° Consensus ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    
    try:
        consensus_backtest_jobs[job_id]["status"] = "RUNNING"
        consensus_backtest_jobs[job_id]["started_at"] = datetime.now().isoformat()
        
        from backend.backtesting.consensus_backtest import ConsensusBacktest
        
        backtest = ConsensusBacktest(
            initial_capital=request.initial_capital,
            consensus_threshold=request.consensus_threshold,
            use_mock_consensus=request.use_mock_consensus
        )
        
        start_date = datetime.strptime(request.start_date, "%Y-%m-%d")
        end_date = datetime.strptime(request.end_date, "%Y-%m-%d")
        
        result = backtest.run(
            tickers=request.tickers,
            start_date=start_date,
            end_date=end_date
        )
        
        # ê²°ê³¼ ì €ì¥
        result_dict = {
            "id": job_id,
            "type": "consensus",
            "request": request.dict(),
            "result": result,
            "created_at": consensus_backtest_jobs[job_id]["created_at"],
            "completed_at": datetime.now().isoformat()
        }
        
        result_file = RESULTS_DIR / f"consensus_{job_id}.json"
        with open(result_file, "w") as f:
            json.dump(result_dict, f, indent=2, default=str)
        
        consensus_backtest_jobs[job_id]["status"] = "COMPLETED"
        consensus_backtest_jobs[job_id]["completed_at"] = datetime.now().isoformat()
        consensus_backtest_jobs[job_id]["result_file"] = str(result_file)
        consensus_backtest_jobs[job_id]["result"] = result
        
    except Exception as e:
        consensus_backtest_jobs[job_id]["status"] = "FAILED"
        consensus_backtest_jobs[job_id]["error"] = str(e)
        consensus_backtest_jobs[job_id]["completed_at"] = datetime.now().isoformat()


@router.post("/consensus")
async def run_consensus_backtest(
    request: ConsensusBacktestRequest,
    background_tasks: BackgroundTasks
):
    """
    Consensus ì „ëµ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    3-AI Consensus + DCA ì „ëµì„ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    """
    
    job_id = str(uuid.uuid4())
    
    consensus_backtest_jobs[job_id] = {
        "id": job_id,
        "type": "consensus",
        "status": "PENDING",
        "request": request.dict(),
        "created_at": datetime.now().isoformat(),
        "started_at": None,
        "completed_at": None,
        "result_file": None,
        "result": None,
        "error": None
    }
    
    background_tasks.add_task(
        run_consensus_backtest_async,
        job_id,
        request
    )
    
    return {
        "id": job_id,
        "type": "consensus",
        "status": "PENDING",
        "message": "Consensus backtest job created. Check results with GET /backtest/consensus/{id}",
        "created_at": consensus_backtest_jobs[job_id]["created_at"]
    }


@router.get("/consensus/list")
async def list_consensus_backtests():
    """ëª¨ë“  Consensus ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ëª©ë¡"""
    
    results = []
    for job_id, job in consensus_backtest_jobs.items():
        summary = {
            "id": job_id,
            "status": job["status"],
            "created_at": job["created_at"],
            "tickers": job["request"]["tickers"]
        }
        
        if job["status"] == "COMPLETED" and job.get("result"):
            result = job["result"]
            summary["total_return_pct"] = result.get("total_return_pct")
            summary["sharpe_ratio"] = result.get("sharpe_ratio")
            summary["win_rate"] = result.get("win_rate")
        
        results.append(summary)
    
    results.sort(key=lambda x: x["created_at"], reverse=True)
    return results


@router.get("/consensus/{job_id}")
async def get_consensus_backtest_result(job_id: str):
    """íŠ¹ì • Consensus ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„¸"""
    
    if job_id not in consensus_backtest_jobs:
        raise HTTPException(status_code=404, detail=f"Consensus backtest {job_id} not found")
    
    job = consensus_backtest_jobs[job_id]
    
    if job["status"] in ["PENDING", "RUNNING"]:
        return {
            "id": job_id,
            "status": job["status"],
            "message": "Backtest is still running. Please wait.",
            "created_at": job["created_at"],
            "started_at": job.get("started_at")
        }
    
    if job["status"] == "FAILED":
        return {
            "id": job_id,
            "status": "FAILED",
            "error": job.get("error", "Unknown error"),
            "created_at": job["created_at"],
            "completed_at": job.get("completed_at")
        }
    
    if job["result_file"]:
        try:
            with open(job["result_file"], "r") as f:
                return json.load(f)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to load result: {str(e)}")
    
    return job


@router.get("/consensus/quick-test")
async def quick_consensus_test():
    """
    ë¹ ë¥¸ Consensus ë°±í…ŒìŠ¤íŠ¸ (ë™ê¸° ì‹¤í–‰)
    
    ìƒ˜í”Œ ë°ì´í„°ë¡œ ì¦‰ì‹œ ê²°ê³¼ ë°˜í™˜
    """
    
    try:
        from backend.backtesting.consensus_backtest import ConsensusBacktest
        
        backtest = ConsensusBacktest(
            initial_capital=100000.0,
            consensus_threshold=0.6,
            use_mock_consensus=True
        )
        
        result = backtest.run(
            tickers=["NVDA", "AMD"],
            start_date=datetime(2024, 1, 1),
            end_date=datetime(2024, 2, 1)  # 1ê°œì›”
        )
        
        return {
            "status": "COMPLETED",
            "type": "quick_test",
            "result": result,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return {
            "status": "FAILED",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

