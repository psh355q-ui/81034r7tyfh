"""
News Poller Service

Features:
- Periodic RSS Crawling (5 min interval)
- Keyword Pre-filtering (save AI tokens)
- Triggering Deep Reasoning Agent for critical events
"""

import asyncio
import logging
from datetime import datetime
from typing import List

from sqlalchemy.orm import Session

from backend.data.rss_crawler import RSSCrawler
from backend.data.news_analyzer import NewsDeepAnalyzer
from backend.ai.reasoning.deep_reasoning_agent import DeepReasoningAgent
# Use PostgreSQL instead of SQLite
from backend.database.models import NewsArticle, NewsAnalysis, TradingSignal
from backend.database.repository import get_sync_session
from backend.data.processors.unified_news_processor import UnifiedNewsProcessor

logger = logging.getLogger(__name__)

# Trigger Keywords (Pre-filter)
# ì´ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë‰´ìŠ¤ë§Œ AI ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ ë¹„ìš© ì ˆê°
CRITICAL_KEYWORDS = [
    "war", "invasion", "military", "conflict", "attack",
    "sanction", "embargo", "ban",
    "crisis", "shortage", "collapse", "bankrupt",
    "rate hike", "inflation", "cpi", "fomc",
    "oil", "semiconductor", "chip", "taiwan", "china"
]

class NewsPoller:
    def __init__(self):
        self.is_running = False
        self.interval_seconds = 300  # 5 minutes
        self.deep_agent = DeepReasoningAgent()
        self._last_triggered = {}  # Cache for debouncing events
        
    async def start(self):
        """Start the polling loop"""
        if self.is_running:
            return
        
        self.is_running = True
        logger.info("ğŸ“° NewsPoller started (Interval: 5m)")
        
        while self.is_running:
            try:
                await self.poll_and_process()
            except Exception as e:
                logger.error(f"âŒ NewsPoller Loop Error: {e}", exc_info=True)
            
            await asyncio.sleep(self.interval_seconds)

    def stop(self):
        self.is_running = False
        logger.info("ğŸ“° NewsPoller stopped")

    async def poll_and_process(self):
        """Single polling cycle (with UnifiedNewsProcessor)"""
        db = get_sync_session()  # PostgreSQL session
        try:
            crawler = RSSCrawler(db)
            
            # 1. Fetch all enabled feeds (DB ì €ì¥ ì•ˆí•¨!)
            logger.info("ğŸ•·ï¸ Fetching RSS feeds...")
            raw_articles = await asyncio.to_thread(crawler.fetch_all_feeds)
            
            if not raw_articles:
                logger.info("No new articles found.")
                return

            logger.info(f"ğŸ“¥ Fetched {len(raw_articles)} raw articles.")

            # 2. UnifiedNewsProcessorë¡œ í†µí•© ì²˜ë¦¬
            logger.info("âš™ï¸ Processing through UnifiedNewsProcessor...")
            processor = UnifiedNewsProcessor(
                db=db,
                semantic_dedup=False,  # ë¹„í™œì„±í™” (í–¥í›„ í™œì„±í™” ê°€ëŠ¥)
                analyze_all=False  # ì¤‘ìš”í•œ ê²ƒë§Œ ë¶„ì„
            )
            
            result = await processor.process_batch(raw_articles)
            
            # 3. í†µê³„ ë¡œê¹…
            stats = processor.get_stats()
            logger.info(f"""ğŸ“Š Processing Complete:
  Total: {stats['total']}
  Saved: {stats['saved']}
  Analyzed: {stats['analyzed']}
  Skipped (URL): {stats['skipped_url']}
  Skipped (Hash): {stats['skipped_hash']}
  Errors: {stats['errors']}
""")

            # 4. ì¤‘ìš” ë‰´ìŠ¤ì— ëŒ€í•œ Deep Reasoning íŠ¸ë¦¬ê±°
            for processed in result.processed:
                if processed.analysis and processed.analysis.urgency in ["high", "critical"]:
                    logger.warning(f"ğŸ§  High Urgency Event: {processed.article.title[:60]}...")
                    
                    # í‚¤ì›Œë“œ ì¶”ì¶œ
                    matched_keywords = self._check_keywords(processed.article)
                    
                    if matched_keywords:
                        await self._trigger_deep_reasoning(
                            db,
                            processed.article,
                            matched_keywords,
                            processed.analysis
                        )
                    
        finally:
            db.close()

    def _check_keywords(self, article: NewsArticle) -> List[str]:
        """Check if article matches critical keywords"""
        text_to_check = (f"{article.title} {article.summary or ''}").lower()
        matches = [k for k in CRITICAL_KEYWORDS if k in text_to_check]
        return matches

    async def _trigger_deep_reasoning(self, db: Session, article: NewsArticle, keywords: List[str], analysis: NewsAnalysis):
        """Deep Reasoning Agent í˜¸ì¶œ ë° ì‹œê·¸ë„ ìƒì„± (with Debouncing)"""
        try:
            # ê²°ì •: Event Type (ë‹¨ìˆœ í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤í•‘ for MVP)
            event_type = "GEOPOLITICS"
            if any(k in keywords for k in ["semiconductor", "chip", "taiwan"]):
                event_type = "CHIP_WAR"

            # Debouncing Check: 1ì‹œê°„ ë‚´ ë™ì¼ ìœ í˜• ì´ë²¤íŠ¸ ë¬´ì‹œ
            last_time = self._last_triggered.get(event_type)
            if last_time and (datetime.utcnow() - last_time).total_seconds() < 3600:
                logger.info(f"â³ Debouncing: Skipping Deep Reasoning for {event_type} (Last run: {last_time})")
                return

            # ê¸°ë³¸ ì •ë³´ êµ¬ì„±
            base_info = {
                "title": article.title,
                "summary": article.summary,
                "published_at": str(article.published_date),
                "source": article.source,
                "initial_analysis": {
                    "urgency": analysis.urgency,
                    "sentiment": analysis.sentiment_overall,
                    "impact": analysis.market_impact_short
                }
            }

            # Deep Reasoning ì‹¤í–‰
            result = await self.deep_agent.analyze_event(event_type, keywords, base_info)
            
            # Update debounce timestamp check
            self._last_triggered[event_type] = datetime.utcnow()
            
            if result.get("status") == "SUCCESS":
                action_plan = result.get("action_plan", {})
                action = action_plan.get("action", "HOLD")
                
                if action != "HOLD":
                    # Signal ìƒì„± & ì €ì¥
                    signal = TradingSignal(
                        ticker="MARKET" if event_type=="GEOPOLITICS" else "NVDA", # MVP Simplification
                        action=action,
                        signal_type="DEEP_REASONING",
                        confidence=action_plan.get("confidence", 0.0),
                        reasoning=f"Event: {article.title[:50]}... | {action_plan.get('reasoning')}", # Fixed field name
                        source="news_poller", # Fixed source
                        generated_at=datetime.utcnow(),
                        # meta_data=result # Postgres model might not have meta_data column yet or use JSON
                    )
                    
                    # Safe DB Add with error handling
                    try:
                       db.add(signal)
                       db.commit()
                       logger.info(f"ğŸš¨ DeepReasoning Signal Created: {action} (Conf: {signal.confidence})")
                    except Exception as db_e:
                       logger.error(f"Failed to save signal: {db_e}")
                       db.rollback()
            
        except Exception as e:
            logger.error(f"âŒ DeepReasoning Trigger Failed: {e}", exc_info=True)
