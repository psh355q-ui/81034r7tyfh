"""
Trending News Detector - ìµœê·¼ ì´ìŠˆ ìë™ ê°ì§€

í•˜ë“œì½”ë”©ëœ í‚¤ì›Œë“œ ëŒ€ì‹  ë™ì ìœ¼ë¡œ ìµœê·¼ íŠ¸ë Œë“œë¥¼ ê°ì§€:
1. ë‰´ìŠ¤ ë¹ˆë„ ë¶„ì„ (24ì‹œê°„ ë‚´ ë“±ì¥ íšŸìˆ˜)
2. ì‹œì¥ ì˜í–¥ë ¥ ë¶„ì„ (íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ ìƒì„± ì—¬ë¶€)
3. LLMì„ í†µí•œ ì¤‘ìš”ë„ í‰ê°€
4. ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ìì—°ìŠ¤ëŸ¬ìš´ ìš°ì„ ìˆœìœ„ ì¡°ì •

ì‘ì„±ì¼: 2026-01-21
"""

import logging
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from sqlalchemy import select, desc, and_, func
from sqlalchemy.ext.asyncio import AsyncSession
from collections import Counter
import re

from backend.ai.gemini_client import call_gemini_api
from backend.core.database import DatabaseSession
from backend.database.models import (
    NewsInterpretation,
    NewsArticle,
    TradingSignal,
    DeepReasoningAnalysis
)

logger = logging.getLogger(__name__)


class TrendingNewsDetector:
    """
    ìµœê·¼ ì´ìŠˆë¥¼ ë™ì ìœ¼ë¡œ ê°ì§€í•˜ëŠ” ì‹œìŠ¤í…œ

    íŠ¹ì§•:
    - ë‰´ìŠ¤ ë¹ˆë„ ë¶„ì„ (ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œ/ì´ë²¤íŠ¸)
    - ì‹œì¥ ì˜í–¥ë ¥ ë¶„ì„ (íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ ìƒì„± ì—¬ë¶€)
    - LLMì„ í†µí•œ ì¤‘ìš”ë„ í‰ê°€
    - ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ìì—°ìŠ¤ëŸ¬ìš´ ê°ì‡  (decay)
    """

    def __init__(self):
        self.model_name = "gemini-2.0-flash-exp"

    async def detect_trending_topics(
        self,
        lookback_hours: int = 24,
        top_n: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ìµœê·¼ íŠ¸ë Œë”© í† í”½ ê°ì§€

        Args:
            lookback_hours: ë¶„ì„ ê¸°ê°„ (ì‹œê°„)
            top_n: ìƒìœ„ Nê°œ í† í”½

        Returns:
            [{
                'topic': str,  # í† í”½ëª… (ì˜ˆ: "Davos Forum", "Fed Rate Cut")
                'score': float,  # ì¤‘ìš”ë„ ì ìˆ˜ (0-100)
                'frequency': int,  # ë“±ì¥ íšŸìˆ˜
                'market_impact': str,  # HIGH/MEDIUM/LOW
                'sentiment': str,  # BULLISH/BEARISH/NEUTRAL
                'key_news': [...]  # ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤
            }]
        """
        async with DatabaseSession() as session:
            cutoff = datetime.now() - timedelta(hours=lookback_hours)

            # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
            news_articles = await self._fetch_recent_news(session, cutoff)

            if not news_articles:
                logger.warning("No recent news found")
                return []

            # 2. í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹ˆë„ ë¶„ì„
            keyword_freq = await self._analyze_keyword_frequency(news_articles)

            # 3. í† í”½ í´ëŸ¬ìŠ¤í„°ë§ (ìœ ì‚¬ í‚¤ì›Œë“œ ê·¸ë£¹í™”)
            topics = await self._cluster_keywords_to_topics(keyword_freq, news_articles)

            # 4. ì‹œì¥ ì˜í–¥ë ¥ ë¶„ì„
            topics_with_impact = await self._analyze_market_impact(session, topics, cutoff)

            # 5. LLMìœ¼ë¡œ ì¤‘ìš”ë„ í‰ê°€
            scored_topics = await self._score_topics_with_llm(topics_with_impact, news_articles)

            # 6. ì •ë ¬ ë° ìƒìœ„ Nê°œ ë°˜í™˜
            scored_topics.sort(key=lambda x: x['score'], reverse=True)

            return scored_topics[:top_n]

    async def _fetch_recent_news(
        self,
        session: AsyncSession,
        cutoff: datetime
    ) -> List[Tuple[NewsArticle, Optional[NewsInterpretation]]]:
        """ìµœê·¼ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            stmt = (
                select(NewsArticle, NewsInterpretation)
                .outerjoin(NewsInterpretation, NewsArticle.id == NewsInterpretation.news_article_id)
                .where(NewsArticle.published_date >= cutoff)
                .order_by(desc(NewsArticle.published_date))
                .limit(100)  # ìµœê·¼ 100ê°œ
            )
            result = await session.execute(stmt)
            return result.all()

        except Exception as e:
            logger.error(f"Failed to fetch recent news: {e}")
            return []

    async def _analyze_keyword_frequency(
        self,
        news_articles: List[Tuple[NewsArticle, Optional[NewsInterpretation]]]
    ) -> Counter:
        """
        í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„

        Returns:
            Counter({'Trump': 15, 'Fed': 12, 'China': 10, ...})
        """
        keywords = []

        for article, interp in news_articles:
            # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            title_words = self._extract_keywords_from_text(article.title)
            keywords.extend(title_words)

            # ìš”ì•½ì—ì„œë„ ì¶”ì¶œ
            if article.summary:
                summary_words = self._extract_keywords_from_text(article.summary)
                keywords.extend(summary_words)

        # ë¹ˆë„ ê³„ì‚°
        freq = Counter(keywords)

        # ë¶ˆìš©ì–´ ì œê±° (a, the, is ë“±)
        stopwords = {'a', 'an', 'the', 'is', 'are', 'was', 'were', 'in', 'on', 'at', 'to', 'for', 'of', 'and', 'or', 'but'}
        freq = Counter({k: v for k, v in freq.items() if k.lower() not in stopwords})

        return freq

    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ

        - ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ (ê³ ìœ ëª…ì‚¬)
        - 2ê¸€ì ì´ìƒ
        - ìˆ«ì ì œì™¸
        """
        if not text:
            return []

        # ë‹¨ì–´ ë¶„ë¦¬
        words = re.findall(r'\b[A-Z][a-zA-Z]+\b', text)

        # í•„í„°ë§
        keywords = [w for w in words if len(w) >= 2 and not w.isdigit()]

        return keywords

    async def _cluster_keywords_to_topics(
        self,
        keyword_freq: Counter,
        news_articles: List[Tuple[NewsArticle, Optional[NewsInterpretation]]]
    ) -> List[Dict[str, Any]]:
        """
        í‚¤ì›Œë“œë¥¼ í† í”½ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§

        ì˜ˆ:
        - 'Trump', 'Trump administration', 'President Trump' â†’ 'Trump Administration'
        - 'Fed', 'Federal Reserve', 'Powell' â†’ 'Federal Reserve'
        - 'Davos', 'WEF', 'World Economic Forum' â†’ 'Davos Forum'
        """
        # LLMìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§
        top_keywords = [k for k, v in keyword_freq.most_common(30)]

        prompt = f"""
ë‹¤ìŒì€ ìµœê·¼ 24ì‹œê°„ ë‰´ìŠ¤ì—ì„œ ìì£¼ ë“±ì¥í•œ í‚¤ì›Œë“œ ëª©ë¡ì…ë‹ˆë‹¤:

{', '.join(top_keywords)}

ì´ í‚¤ì›Œë“œë“¤ì„ ì˜ë¯¸ê°€ ìœ ì‚¬í•œ ê²ƒë¼ë¦¬ ê·¸ë£¹í™”í•˜ì—¬ "í† í”½"ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ (JSON):
[
    {{
        "topic": "í† í”½ëª… (ì˜ˆ: Trump Administration, Federal Reserve, China-Taiwan Tensions)",
        "keywords": ["ê´€ë ¨ í‚¤ì›Œë“œ1", "ê´€ë ¨ í‚¤ì›Œë“œ2", ...],
        "description": "í† í”½ ì„¤ëª… (1ì¤„)"
    }},
    ...
]

ê·œì¹™:
- ìµœëŒ€ 10ê°œ í† í”½
- ê° í† í”½ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ (ì˜ˆ: "Trump" â†’ "Trump Administration Policies")
- ì‹œì¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” í† í”½ ìš°ì„ 
- JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥, ì¶”ê°€ ì„¤ëª… ë¶ˆí•„ìš”
"""

        try:
            response = await call_gemini_api(prompt, self.model_name)

            # JSON íŒŒì‹±
            import json
            # Geminiê°€ ```json ... ``` ë¡œ ê°ìŒ€ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°
            response_clean = response.strip()
            if response_clean.startswith('```'):
                response_clean = response_clean.split('```')[1]
                if response_clean.startswith('json'):
                    response_clean = response_clean[4:]
            response_clean = response_clean.strip()

            topics = json.loads(response_clean)

            # ë¹ˆë„ ì¶”ê°€
            for topic in topics:
                topic['frequency'] = sum(keyword_freq.get(kw, 0) for kw in topic['keywords'])

            return topics

        except Exception as e:
            logger.error(f"Failed to cluster keywords: {e}")
            # í´ë°±: ìƒìœ„ í‚¤ì›Œë“œë¥¼ í† í”½ìœ¼ë¡œ ì‚¬ìš©
            return [
                {
                    'topic': keyword,
                    'keywords': [keyword],
                    'description': f'News about {keyword}',
                    'frequency': count
                }
                for keyword, count in keyword_freq.most_common(10)
            ]

    async def _analyze_market_impact(
        self,
        session: AsyncSession,
        topics: List[Dict[str, Any]],
        cutoff: datetime
    ) -> List[Dict[str, Any]]:
        """
        í† í”½ë³„ ì‹œì¥ ì˜í–¥ë ¥ ë¶„ì„

        - íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ ìƒì„± ì—¬ë¶€
        - Deep Reasoning ë¶„ì„ ì—¬ë¶€
        """
        try:
            # ìµœê·¼ íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ ì¡°íšŒ
            stmt_signals = (
                select(TradingSignal)
                .where(TradingSignal.created_at >= cutoff)
            )
            result_signals = await session.execute(stmt_signals)
            signals = result_signals.scalars().all()

            # Deep Reasoning ì¡°íšŒ
            stmt_dr = (
                select(DeepReasoningAnalysis)
                .where(DeepReasoningAnalysis.created_at >= cutoff)
            )
            result_dr = await session.execute(stmt_dr)
            analyses = result_dr.scalars().all()

            # í† í”½ë³„ ì˜í–¥ë ¥ ê³„ì‚°
            for topic in topics:
                keywords = topic['keywords']

                # ì‹œê·¸ë„ ë§¤ì¹­
                related_signals = [
                    s for s in signals
                    if any(kw.lower() in s.reasoning.lower() for kw in keywords)
                ]

                # Deep Reasoning ë§¤ì¹­
                related_analyses = [
                    a for a in analyses
                    if any(kw.lower() in a.theme.lower() for kw in keywords)
                ]

                # ì˜í–¥ë ¥ ê³„ì‚°
                signal_count = len(related_signals)
                analysis_count = len(related_analyses)

                if signal_count >= 3 or analysis_count >= 2:
                    topic['market_impact'] = 'HIGH'
                elif signal_count >= 1 or analysis_count >= 1:
                    topic['market_impact'] = 'MEDIUM'
                else:
                    topic['market_impact'] = 'LOW'

                # ê°ì„± ê³„ì‚° (ì‹œê·¸ë„ ê¸°ì¤€)
                if related_signals:
                    buy_count = sum(1 for s in related_signals if s.action == 'BUY')
                    sell_count = sum(1 for s in related_signals if s.action == 'SELL')

                    if buy_count > sell_count * 1.5:
                        topic['sentiment'] = 'BULLISH'
                    elif sell_count > buy_count * 1.5:
                        topic['sentiment'] = 'BEARISH'
                    else:
                        topic['sentiment'] = 'NEUTRAL'
                else:
                    topic['sentiment'] = 'NEUTRAL'

            return topics

        except Exception as e:
            logger.error(f"Failed to analyze market impact: {e}")
            # í´ë°±: ëª¨ë“  í† í”½ LOWë¡œ ì„¤ì •
            for topic in topics:
                topic['market_impact'] = 'LOW'
                topic['sentiment'] = 'NEUTRAL'
            return topics

    async def _score_topics_with_llm(
        self,
        topics: List[Dict[str, Any]],
        news_articles: List[Tuple[NewsArticle, Optional[NewsInterpretation]]]
    ) -> List[Dict[str, Any]]:
        """
        LLMìœ¼ë¡œ í† í”½ ì¤‘ìš”ë„ í‰ê°€ (0-100ì )

        ê³ ë ¤ ì‚¬í•­:
        - ë¹ˆë„ (ìì£¼ ë“±ì¥í• ìˆ˜ë¡ ë†’ìŒ)
        - ì‹œì¥ ì˜í–¥ë ¥ (HIGH > MEDIUM > LOW)
        - ì‹œì˜ì„± (ìµœê·¼ ë‰´ìŠ¤ì¼ìˆ˜ë¡ ë†’ìŒ)
        - ê¸€ë¡œë²Œ ì˜í–¥ë ¥ (ê¸€ë¡œë²Œ ì´ë²¤íŠ¸ > ì§€ì—­ ì´ë²¤íŠ¸)
        """
        try:
            # í† í”½ ìš”ì•½
            topics_summary = []
            for topic in topics:
                topics_summary.append({
                    'topic': topic['topic'],
                    'frequency': topic['frequency'],
                    'market_impact': topic['market_impact'],
                    'sentiment': topic['sentiment']
                })

            prompt = f"""
ë‹¤ìŒì€ ìµœê·¼ 24ì‹œê°„ ë‰´ìŠ¤ì—ì„œ ê°ì§€ëœ í† í”½ë“¤ì…ë‹ˆë‹¤:

{topics_summary}

ê° í† í”½ì˜ ì¤‘ìš”ë„ë¥¼ 0-100ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
1. ì‹œì¥ ì˜í–¥ë ¥ (40ì ): ì£¼ì‹/ì±„ê¶Œ/ì™¸í™˜ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
2. ê¸€ë¡œë²Œ ì˜í–¥ë ¥ (30ì ): ì „ ì„¸ê³„ì  ê´€ì‹¬ë„
3. ì‹œì˜ì„± (20ì ): í˜„ì¬ ì§„í–‰ ì¤‘ì´ê±°ë‚˜ ê³§ ë°œìƒí•  ì´ë²¤íŠ¸
4. ë¹ˆë„ (10ì ): ë‰´ìŠ¤ ë“±ì¥ íšŸìˆ˜

ì¶œë ¥ í˜•ì‹ (JSON):
[
    {{
        "topic": "í† í”½ëª…",
        "score": 85,
        "reasoning": "í‰ê°€ ì´ìœ  (1ì¤„)"
    }},
    ...
]

ê·œì¹™:
- ê°ê´€ì ìœ¼ë¡œ í‰ê°€ (ê°ì • ë°°ì œ)
- ì‹¤ì œ ì‹œì¥ì— ì˜í–¥ì„ ì¤„ í† í”½ ìš°ì„  (ì˜ˆ: Fed ê¸ˆë¦¬ > ì—°ì˜ˆì¸ ìŠ¤ìº”ë“¤)
- JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥
"""

            response = await call_gemini_api(prompt, self.model_name)

            # JSON íŒŒì‹±
            import json
            response_clean = response.strip()
            if response_clean.startswith('```'):
                response_clean = response_clean.split('```')[1]
                if response_clean.startswith('json'):
                    response_clean = response_clean[4:]
            response_clean = response_clean.strip()

            scores = json.loads(response_clean)

            # í† í”½ì— ì ìˆ˜ ì¶”ê°€
            score_dict = {s['topic']: s for s in scores}

            for topic in topics:
                if topic['topic'] in score_dict:
                    topic['score'] = score_dict[topic['topic']]['score']
                    topic['reasoning'] = score_dict[topic['topic']]['reasoning']
                else:
                    # í´ë°±: ë¹ˆë„ì™€ ì˜í–¥ë ¥ ê¸°ì¤€ ì ìˆ˜
                    freq_score = min(topic['frequency'] * 2, 30)
                    impact_score = {'HIGH': 40, 'MEDIUM': 20, 'LOW': 10}.get(topic['market_impact'], 10)
                    topic['score'] = freq_score + impact_score
                    topic['reasoning'] = 'Automatic scoring based on frequency and impact'

            return topics

        except Exception as e:
            logger.error(f"Failed to score topics with LLM: {e}")
            # í´ë°±: ë¹ˆë„ì™€ ì˜í–¥ë ¥ ê¸°ì¤€ ì ìˆ˜
            for topic in topics:
                freq_score = min(topic['frequency'] * 2, 30)
                impact_score = {'HIGH': 40, 'MEDIUM': 20, 'LOW': 10}.get(topic.get('market_impact', 'LOW'), 10)
                topic['score'] = freq_score + impact_score
                topic['reasoning'] = 'Automatic scoring based on frequency and impact'

            return topics

    async def get_key_news_for_topic(
        self,
        topic: Dict[str, Any],
        limit: int = 5
    ) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • í† í”½ì˜ ì£¼ìš” ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°

        Returns:
            [{
                'title': str,
                'source': str,
                'url': str,
                'published': datetime,
                'sentiment': str
            }]
        """
        async with DatabaseSession() as session:
            keywords = topic['keywords']
            cutoff = datetime.now() - timedelta(hours=24)

            try:
                # í‚¤ì›Œë“œ í¬í•¨ ë‰´ìŠ¤ ì¡°íšŒ
                stmt = (
                    select(NewsArticle, NewsInterpretation)
                    .outerjoin(NewsInterpretation, NewsArticle.id == NewsInterpretation.news_article_id)
                    .where(NewsArticle.published_date >= cutoff)
                    .order_by(desc(NewsArticle.published_date))
                )
                result = await session.execute(stmt)
                all_news = result.all()

                # í‚¤ì›Œë“œ ë§¤ì¹­
                related_news = []
                for article, interp in all_news:
                    text = f"{article.title} {article.summary or ''}".lower()
                    if any(kw.lower() in text for kw in keywords):
                        related_news.append({
                            'title': article.title,
                            'source': article.source,
                            'url': article.url,
                            'published': article.published_date,
                            'sentiment': interp.headline_bias if interp else 'NEUTRAL'
                        })

                        if len(related_news) >= limit:
                            break

                return related_news

            except Exception as e:
                logger.error(f"Failed to get key news: {e}")
                return []


# ============================================================================
# í…ŒìŠ¤íŠ¸
# ============================================================================

if __name__ == "__main__":
    async def test():
        detector = TrendingNewsDetector()

        print("=" * 70)
        print("Trending News Detector Test")
        print("=" * 70)

        # íŠ¸ë Œë”© í† í”½ ê°ì§€
        topics = await detector.detect_trending_topics(lookback_hours=24, top_n=10)

        print(f"\nğŸ“Š Detected {len(topics)} trending topics:\n")

        for i, topic in enumerate(topics, 1):
            print(f"{i}. {topic['topic']} (Score: {topic['score']}/100)")
            print(f"   Frequency: {topic['frequency']} mentions")
            print(f"   Market Impact: {topic['market_impact']}")
            print(f"   Sentiment: {topic['sentiment']}")
            print(f"   Reasoning: {topic['reasoning']}")
            print()

        # ìƒìœ„ í† í”½ì˜ ì£¼ìš” ë‰´ìŠ¤
        if topics:
            print("=" * 70)
            print(f"Key News for Top Topic: {topics[0]['topic']}")
            print("=" * 70)

            key_news = await detector.get_key_news_for_topic(topics[0], limit=5)

            for i, news in enumerate(key_news, 1):
                print(f"{i}. {news['title']}")
                print(f"   Source: {news['source']}")
                print(f"   Sentiment: {news['sentiment']}")
                print()

    asyncio.run(test())
