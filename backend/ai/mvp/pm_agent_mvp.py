"""
PM Agent MVP - Final Decision Maker

Phase: MVP Consolidation
Date: 2025-12-31

Purpose:
    í¬íŠ¸í´ë¦¬ì˜¤ ë§¤ë‹ˆì € - ìµœì¢… ì˜ì‚¬ê²°ì •ì
    - Hard Rules ê²€ì¦ (ì½”ë“œ ê¸°ë°˜, AI í•´ì„ ê¸ˆì§€)
    - Silence Policy ì‹¤í–‰ (íŒë‹¨ ê±°ë¶€ ê¶Œí•œ)
    - 3ê°œ Agent ì˜ê²¬ í†µí•© ë° ìµœì¢… ê²°ì •
    - í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ì¤€ ë¦¬ìŠ¤í¬ ê´€ë¦¬

Key Responsibilities:
    1. Hard Rules ê²€ì¦ (code-enforced, not AI)
    2. Silence Policy (confidence < threshold â†’ reject)
    3. 3ê°œ Agent ì˜ê²¬ ê°€ì¤‘ í‰ê·  ë° ìµœì¢… ê²°ì •
    4. í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ì¤€ ë¦¬ìŠ¤í¬ ì²´í¬ (ì§‘ì¤‘ë„, ìƒê´€ê´€ê³„)
    5. ìµœì¢… ê±°ë¶€ê¶Œ í–‰ì‚¬ (extreme risk, low confidence)

Hard Rules (Code-Enforced):
    1. Position Size > 30% â†’ REJECT
    2. Total Portfolio Risk > 5% â†’ REJECT
    3. Agent Disagreement > 60% â†’ REJECT or REDUCE
    4. Average Confidence < 50% â†’ REJECT (Silence Policy)
    5. Stop Loss not set â†’ REJECT
    6. Risk Level = "extreme" â†’ REJECT
"""

import os
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime
import google.generativeai as genai

# Configure logger
logger = logging.getLogger(__name__)


class PMAgentMVP:
    """MVP PM Agent - ìµœì¢… ì˜ì‚¬ê²°ì •ì + Hard Rules + Silence Policy"""

    def __init__(self):
        """Initialize PM Agent MVP"""
        # Gemini API ì„¤ì •
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found in environment variables")

        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.0-flash-exp')

        # Agent configuration
        self.role = "í¬íŠ¸í´ë¦¬ì˜¤ ë§¤ë‹ˆì €"

        # ===================================================================
        # HARD RULES (Code-Enforced, NOT AI-interpreted)
        # Updated: 2025-12-31 - Relaxed max_agent_disagreement for Phase 1
        # ===================================================================
        self.HARD_RULES = {
            'max_position_size': 0.30,  # 30% í¬ì§€ì…˜ ì ˆëŒ€ ìƒí•œ
            'max_portfolio_risk': 0.05,  # 5% í¬íŠ¸í´ë¦¬ì˜¤ ì „ì²´ ë¦¬ìŠ¤í¬ ìƒí•œ
            'min_avg_confidence': 0.50,  # 50% í‰ê·  confidence í•˜í•œ (Silence Policy)
            'max_agent_disagreement': 0.75,  # 75% ì˜ê²¬ ë¶ˆì¼ì¹˜ ìƒí•œ (Phase 1 ì™„í™”: 60% â†’ 75%)
            'stop_loss_required': True,  # Stop Loss í•„ìˆ˜
            'reject_extreme_risk': True,  # Risk Level "extreme" ì‹œ ê±°ë¶€
            'max_correlated_positions': 3,  # ë†’ì€ ìƒê´€ê´€ê³„ í¬ì§€ì…˜ ìµœëŒ€ 3ê°œ
            'max_sector_concentration': 0.40  # 40% ì„¹í„° ì§‘ì¤‘ë„ ìƒí•œ
        }
        # ğŸ” DEBUG: PM Agent ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œì  í™•ì¸
        logger.info(f"ğŸ” INIT DEBUG: PMAgentMVP created, max_agent_disagreement={self.HARD_RULES['max_agent_disagreement']}, instance_id={id(self)}, HARD_RULES_id={id(self.HARD_RULES)}")

        # Silence Policy threshold
        self.SILENCE_THRESHOLD = 0.50  # Confidence < 50% â†’ íŒë‹¨ ê±°ë¶€

        # System prompt
        self.system_prompt = """ë‹¹ì‹ ì€ í¬íŠ¸í´ë¦¬ì˜¤ ë§¤ë‹ˆì €ì…ë‹ˆë‹¤.

ì—­í• :
1. 3ê°œ Agent ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ê²°ì •
2. í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ì¤€ ë¦¬ìŠ¤í¬ í‰ê°€
3. ì˜ê²¬ ë¶ˆì¼ì¹˜ ì‹œ ì¡°ì • ë° ì¤‘ì¬
4. ìµœì¢… ê±°ë¶€ê¶Œ í–‰ì‚¬ (í•„ìš” ì‹œ)

ë¶„ì„ ì›ì¹™:
- Hard RulesëŠ” ì½”ë“œê°€ ê²€ì¦ (ë‹¹ì‹ ì€ íŒë‹¨ë§Œ)
- Confidenceê°€ ë‚®ìœ¼ë©´ ê±°ë¶€ ê¶Œì¥
- Agent ê°„ ì˜ê²¬ ì°¨ì´ê°€ í¬ë©´ ì‹ ì¤‘ ëª¨ë“œ
- í¬íŠ¸í´ë¦¬ì˜¤ ì „ì²´ ê´€ì ì—ì„œ í‰ê°€

ì¶œë ¥ í˜•ì‹:
{
    "final_decision": "approve" | "reject" | "reduce_size" | "silence",
    "confidence": 0.0 ~ 1.0,
    "reasoning": "ìµœì¢… ê²°ì • ê·¼ê±°",
    "recommended_action": "buy" | "sell" | "hold",
    "position_size_adjustment": 0.0 ~ 1.0 (1.0 = full size, 0.5 = half),
    "risk_assessment": {
        "portfolio_risk_score": 0.0 ~ 10.0,
        "concentration_risk": 0.0 ~ 10.0,
        "correlation_risk": 0.0 ~ 10.0,
        "overall_portfolio_health": 0.0 ~ 10.0
    },
    "agent_consensus": {
        "agreement_level": 0.0 ~ 1.0,
        "conflicting_opinions": ["agent1 vs agent2 on X"],
        "resolution": "how conflicts were resolved"
    },
    "warnings": ["warning1", "warning2", ...],
    "approval_conditions": ["condition1", "condition2", ...] or []
}

ì¤‘ìš”:
- final_decision = "silence"ëŠ” íŒë‹¨ ê±°ë¶€ (ì •ë³´ ë¶ˆì¶©ë¶„)
- Agent ì˜ê²¬ì´ ìƒì¶©í•˜ë©´ ë³´ìˆ˜ì ìœ¼ë¡œ ê²°ì •
- í¬íŠ¸í´ë¦¬ì˜¤ ì „ì²´ ê±´ê°•ë„ ìš°ì„  ê³ ë ¤
- **ë°˜ë“œì‹œ í•œê¸€ë¡œ ì‘ë‹µí•  ê²ƒ** (reasoning, warnings, approval_conditions ë“± ëª¨ë“  í…ìŠ¤íŠ¸ í•„ë“œëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±)
"""

    def make_final_decision(
        self,
        symbol: str,
        trader_opinion: Dict[str, Any],
        risk_opinion: Dict[str, Any],
        analyst_opinion: Dict[str, Any],
        portfolio_state: Dict[str, Any],
        correlation_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ìµœì¢… ì˜ì‚¬ê²°ì • ìˆ˜í–‰

        Args:
            symbol: ì¢…ëª© ì‹¬ë³¼
            trader_opinion: Trader Agent MVP ì˜ê²¬
                {
                    'action': str,
                    'confidence': float,
                    'opportunity_score': float,
                    'weight': 0.35
                }
            risk_opinion: Risk Agent MVP ì˜ê²¬
                {
                    'risk_level': str,
                    'confidence': float,
                    'recommendation': str,
                    'position_size_usd': float,
                    'stop_loss_pct': float,
                    'weight': 0.35
                }
            analyst_opinion: Analyst Agent MVP ì˜ê²¬
                {
                    'action': str,
                    'confidence': float,
                    'overall_information_score': float,
                    'red_flags': list,
                    'weight': 0.30
                }
            portfolio_state: í¬íŠ¸í´ë¦¬ì˜¤ í˜„ì¬ ìƒíƒœ
                {
                    'total_value': float,
                    'current_positions': [{'symbol': str, 'value': float, 'sector': str}],
                    'available_cash': float,
                    'total_risk': float
                }
            correlation_data: ìƒê´€ê´€ê³„ ë°ì´í„° (optional)
                {
                    'correlated_positions': [{'symbol': str, 'correlation': float}]
                }

        Returns:
            Dict containing:
                - final_decision: approve/reject/reduce_size/silence
                - confidence: ìµœì¢… confidence
                - reasoning: ê²°ì • ê·¼ê±°
                - recommended_action: buy/sell/hold
                - position_size_adjustment: í¬ì§€ì…˜ ì¡°ì • ë°°ìœ¨
                - risk_assessment: ë¦¬ìŠ¤í¬ í‰ê°€
                - agent_consensus: Agent í•©ì˜ ë¶„ì„
                - warnings: ê²½ê³  ì‚¬í•­
                - hard_rules_passed: Hard Rules í†µê³¼ ì—¬ë¶€
                - hard_rules_violations: Hard Rules ìœ„ë°˜ ë‚´ì—­
        """
        # ================================================================
        # STEP 1: HARD RULES VALIDATION (Code-Enforced)
        # ================================================================
        hard_rules_result = self._validate_hard_rules(
            symbol=symbol,
            trader_opinion=trader_opinion,
            risk_opinion=risk_opinion,
            analyst_opinion=analyst_opinion,
            portfolio_state=portfolio_state,
            correlation_data=correlation_data
        )

        # Hard Rules ìœ„ë°˜ ì‹œ ì¦‰ì‹œ ê±°ë¶€
        if not hard_rules_result['passed']:
            return {
                'agent': 'pm_mvp',
                'final_decision': 'reject',
                'confidence': 0.0,
                'reasoning': f"Hard Rules ìœ„ë°˜: {', '.join(hard_rules_result['violations'])}",
                'recommended_action': 'hold',
                'position_size_adjustment': 0.0,
                'risk_assessment': {
                    'portfolio_risk_score': 10.0,
                    'concentration_risk': 10.0,
                    'correlation_risk': 10.0,
                    'overall_portfolio_health': 0.0
                },
                'agent_consensus': {
                    'agreement_level': 0.0,
                    'conflicting_opinions': [],
                    'resolution': 'Rejected by Hard Rules'
                },
                'warnings': hard_rules_result['violations'],
                'hard_rules_passed': False,
                'hard_rules_violations': hard_rules_result['violations'],
                'timestamp': datetime.utcnow().isoformat(),
                'symbol': symbol
            }

        # ================================================================
        # STEP 2: SILENCE POLICY CHECK
        # ================================================================
        avg_confidence = (
            trader_opinion['confidence'] * trader_opinion['weight'] +
            risk_opinion['confidence'] * risk_opinion['weight'] +
            analyst_opinion['confidence'] * analyst_opinion['weight']
        )

        # Silence Policy: í‰ê·  confidence < threshold â†’ íŒë‹¨ ê±°ë¶€
        if avg_confidence < self.SILENCE_THRESHOLD:
            return {
                'agent': 'pm_mvp',
                'final_decision': 'silence',
                'confidence': avg_confidence,
                'reasoning': f"Silence Policy: Average confidence ({avg_confidence:.2f}) below threshold ({self.SILENCE_THRESHOLD})",
                'recommended_action': 'hold',
                'position_size_adjustment': 0.0,
                'risk_assessment': {
                    'portfolio_risk_score': 5.0,
                    'concentration_risk': 5.0,
                    'correlation_risk': 5.0,
                    'overall_portfolio_health': 5.0
                },
                'agent_consensus': {
                    'agreement_level': 0.0,
                    'conflicting_opinions': [],
                    'resolution': 'Silence - insufficient confidence'
                },
                'warnings': ['Insufficient confidence for decision'],
                'hard_rules_passed': True,
                'hard_rules_violations': [],
                'timestamp': datetime.utcnow().isoformat(),
                'symbol': symbol
            }

        # ================================================================
        # STEP 3: AI-BASED FINAL DECISION
        # ================================================================
        # Construct prompt for PM Agent
        prompt = self._build_prompt(
            symbol=symbol,
            trader_opinion=trader_opinion,
            risk_opinion=risk_opinion,
            analyst_opinion=analyst_opinion,
            portfolio_state=portfolio_state,
            correlation_data=correlation_data,
            avg_confidence=avg_confidence
        )

        # Call Gemini API
        try:
            response = self.model.generate_content([
                self.system_prompt,
                prompt
            ])

            # Parse response
            result = self._parse_response(response.text)

            # Add metadata and hard rules info
            result['agent'] = 'pm_mvp'
            result['hard_rules_passed'] = True
            result['hard_rules_violations'] = []
            result['timestamp'] = datetime.utcnow().isoformat()
            result['symbol'] = symbol
            result['avg_agent_confidence'] = avg_confidence

            return result

        except Exception as e:
            # Error handling - return safe default (reject)
            return {
                'agent': 'pm_mvp',
                'final_decision': 'reject',
                'confidence': 0.0,
                'reasoning': f'PM analysis failed: {str(e)}',
                'recommended_action': 'hold',
                'position_size_adjustment': 0.0,
                'risk_assessment': {
                    'portfolio_risk_score': 10.0,
                    'concentration_risk': 10.0,
                    'correlation_risk': 10.0,
                    'overall_portfolio_health': 0.0
                },
                'agent_consensus': {
                    'agreement_level': 0.0,
                    'conflicting_opinions': [],
                    'resolution': 'Error - rejected for safety'
                },
                'warnings': [f'PM Agent error: {str(e)}'],
                'hard_rules_passed': True,
                'hard_rules_violations': [],
                'timestamp': datetime.utcnow().isoformat(),
                'symbol': symbol,
                'error': str(e)
            }

    def _validate_hard_rules(
        self,
        symbol: str,
        trader_opinion: Dict[str, Any],
        risk_opinion: Dict[str, Any],
        analyst_opinion: Dict[str, Any],
        portfolio_state: Dict[str, Any],
        correlation_data: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Hard Rules ê²€ì¦ (Code-Enforced)

        Returns:
            {
                'passed': bool,
                'violations': List[str]
            }
        """
        violations = []

        # Rule 1: Position Size > 30%
        position_size_pct = risk_opinion.get('position_size_pct', 0.0)
        if position_size_pct > self.HARD_RULES['max_position_size']:
            violations.append(
                f"í¬ì§€ì…˜ í¬ê¸° {position_size_pct*100:.1f}%ê°€ ìµœëŒ€ í—ˆìš©ì¹˜ {self.HARD_RULES['max_position_size']*100}%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤"
            )

        # Rule 2: Total Portfolio Risk > 5%
        total_risk = portfolio_state.get('total_risk', 0.0)
        if total_risk > self.HARD_RULES['max_portfolio_risk']:
            violations.append(
                f"í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ìŠ¤í¬ {total_risk*100:.1f}%ê°€ ìµœëŒ€ í—ˆìš©ì¹˜ {self.HARD_RULES['max_portfolio_risk']*100:.1f}%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤"
            )

        # Rule 3: Agent Disagreement > 75% (Phase 1 ì™„í™”)
        actions = [
            trader_opinion.get('action', 'pass'),
            risk_opinion.get('recommendation', 'reject'),
            analyst_opinion.get('action', 'pass')
        ]
        # Count unique actions (excluding 'pass')
        non_pass_actions = [a for a in actions if a != 'pass']
        if len(non_pass_actions) > 0:
            disagreement = 1.0 - (non_pass_actions.count(non_pass_actions[0]) / len(non_pass_actions))
            # ğŸ” DEBUG: ì‹¤ì œ validation ì‹œì ì˜ HARD_RULES ê°’ í™•ì¸
            logger.warning(f"ğŸ” VALIDATION DEBUG: disagreement={disagreement:.2f}, max_allowed={self.HARD_RULES['max_agent_disagreement']}, HARD_RULES_id={id(self.HARD_RULES)}, actions={actions}, non_pass={non_pass_actions}")
            if disagreement > self.HARD_RULES['max_agent_disagreement']:
                violations.append(
                    f"Agent ì˜ê²¬ ë¶ˆì¼ì¹˜ {disagreement*100:.0f}%ê°€ ìµœëŒ€ í—ˆìš©ì¹˜ {self.HARD_RULES['max_agent_disagreement']*100:.0f}%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤"
                )

        # Rule 4: Average Confidence < 50% (handled by Silence Policy, but double-check)
        confidences = [
            trader_opinion.get('confidence', 0.0),
            risk_opinion.get('confidence', 0.0),
            analyst_opinion.get('confidence', 0.0)
        ]
        avg_conf = sum(confidences) / len(confidences)
        if avg_conf < self.HARD_RULES['min_avg_confidence']:
            violations.append(
                f"í‰ê·  ì‹ ë¢°ë„ {avg_conf*100:.0f}%ê°€ ìµœì†Œ ìš”êµ¬ì¹˜ {self.HARD_RULES['min_avg_confidence']*100:.0f}% ë¯¸ë§Œì…ë‹ˆë‹¤"
            )

        # Rule 5: Stop Loss Required
        if self.HARD_RULES['stop_loss_required']:
            stop_loss = risk_opinion.get('stop_loss_pct', 0.0)
            if stop_loss <= 0.0 or stop_loss > 0.10:  # Must be 0.1% ~ 10%
                violations.append(
                    f"ì†ì ˆë§¤ {stop_loss*100:.2f}%ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (0.1% ~ 10% ë²”ìœ„ì—¬ì•¼ í•¨)"
                )

        # Rule 6: Risk Level "extreme" â†’ Reject
        if self.HARD_RULES['reject_extreme_risk']:
            risk_level = risk_opinion.get('risk_level', 'medium')
            if risk_level == 'extreme':
                violations.append(
                    "ë¦¬ìŠ¤í¬ ìˆ˜ì¤€ì´ 'extreme'ìœ¼ë¡œ ìë™ ê±°ë¶€ë©ë‹ˆë‹¤"
                )

        # Rule 7: Correlated Positions > 3
        if correlation_data:
            correlated_positions = correlation_data.get('correlated_positions', [])
            high_corr_count = len([p for p in correlated_positions if p.get('correlation', 0) > 0.7])
            if high_corr_count >= self.HARD_RULES['max_correlated_positions']:
                violations.append(
                    f"ìƒê´€ê´€ê³„ê°€ ë†’ì€ í¬ì§€ì…˜ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤ ({high_corr_count}ê°œ) - ìµœëŒ€ {self.HARD_RULES['max_correlated_positions']}ê°œ"
                )

        # Rule 8: Sector Concentration > 40%
        current_positions = portfolio_state.get('current_positions', [])
        total_value = portfolio_state.get('total_value', 1)
        if current_positions:
            # Calculate sector concentration
            sector_values = {}
            for pos in current_positions:
                sector = pos.get('sector', 'Unknown')
                value = pos.get('value', 0)
                sector_values[sector] = sector_values.get(sector, 0) + value

            max_sector_pct = max(sector_values.values()) / total_value if total_value > 0 else 0
            if max_sector_pct > self.HARD_RULES['max_sector_concentration']:
                violations.append(
                    f"ì„¹í„° ì§‘ì¤‘ë„ {max_sector_pct*100:.1f}%ê°€ ìµœëŒ€ í—ˆìš©ì¹˜ {self.HARD_RULES['max_sector_concentration']*100:.1f}%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤"
                )

        return {
            'passed': len(violations) == 0,
            'violations': violations
        }

    def _build_prompt(
        self,
        symbol: str,
        trader_opinion: Dict[str, Any],
        risk_opinion: Dict[str, Any],
        analyst_opinion: Dict[str, Any],
        portfolio_state: Dict[str, Any],
        correlation_data: Optional[Dict[str, Any]],
        avg_confidence: float
    ) -> str:
        """Build PM decision prompt"""
        prompt_parts = [
            f"ì¢…ëª©: {symbol}",
            f"í‰ê·  Confidence: {avg_confidence:.2f}",
            "",
            "=== Trader Agent (35% weight) ===",
            f"Action: {trader_opinion.get('action', 'N/A')}",
            f"Confidence: {trader_opinion.get('confidence', 0):.2f}",
            f"Opportunity Score: {trader_opinion.get('opportunity_score', 0):.1f}",
            f"Reasoning: {trader_opinion.get('reasoning', 'N/A')}",
            "",
            "=== Risk Agent (35% weight) ===",
            f"Risk Level: {risk_opinion.get('risk_level', 'N/A')}",
            f"Confidence: {risk_opinion.get('confidence', 0):.2f}",
            f"Recommendation: {risk_opinion.get('recommendation', 'N/A')}",
            f"Position Size: ${risk_opinion.get('position_size_usd', 0):,.0f} ({(risk_opinion.get('position_size_pct', 0) * 100):.1f}%)",
            f"Stop Loss: {(risk_opinion.get('stop_loss_pct', 0) * 100):.1f}%",
            f"Reasoning: {risk_opinion.get('reasoning', 'N/A')}",
            "",
            "=== Analyst Agent (30% weight) ===",
            f"Action: {analyst_opinion.get('action', 'N/A')}",
            f"Confidence: {analyst_opinion.get('confidence', 0):.2f}",
            f"Info Score: {analyst_opinion.get('overall_information_score', 0):.1f}",
            f"Red Flags: {', '.join(analyst_opinion.get('red_flags', [])) or 'None'}",
            f"Reasoning: {analyst_opinion.get('reasoning', 'N/A')}",
            "",
            "=== Portfolio State ===",
            f"Total Value: ${portfolio_state.get('total_value', 0):,.0f}",
            f"Available Cash: ${portfolio_state.get('available_cash', 0):,.0f}",
            f"Current Positions: {len(portfolio_state.get('current_positions', []))}",
            f"Total Risk: {(portfolio_state.get('total_risk', 0) * 100):.1f}%",
        ]

        if correlation_data:
            prompt_parts.append("\n=== Correlation Data ===")
            corr_positions = correlation_data.get('correlated_positions', [])
            if corr_positions:
                prompt_parts.append("Highly Correlated Positions:")
                for pos in corr_positions[:3]:
                    prompt_parts.append(f"  - {pos.get('symbol', 'N/A')}: {pos.get('correlation', 0):.2f}")

        prompt_parts.append("\nìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ê²°ì •ì„ ë‚´ë¦¬ê³  JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.")

        return "\n".join(prompt_parts)

    def _parse_response(self, response_text: str) -> Dict[str, Any]:
        """Parse Gemini response"""
        import json
        import re

        # Extract JSON
        try:
            result = json.loads(response_text)
        except json.JSONDecodeError:
            json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group(0))
                else:
                    raise ValueError("No valid JSON found")

        # Provide defaults
        result.setdefault('final_decision', 'reject')
        result.setdefault('confidence', 0.5)
        result.setdefault('reasoning', 'No reasoning provided')
        result.setdefault('recommended_action', 'hold')
        result.setdefault('position_size_adjustment', 1.0)
        result.setdefault('warnings', [])
        result.setdefault('approval_conditions', [])

        if 'risk_assessment' not in result:
            result['risk_assessment'] = {}
        risk_assess = result['risk_assessment']
        risk_assess.setdefault('portfolio_risk_score', 5.0)
        risk_assess.setdefault('concentration_risk', 5.0)
        risk_assess.setdefault('correlation_risk', 5.0)
        risk_assess.setdefault('overall_portfolio_health', 5.0)

        if 'agent_consensus' not in result:
            result['agent_consensus'] = {}
        consensus = result['agent_consensus']
        consensus.setdefault('agreement_level', 0.5)
        consensus.setdefault('conflicting_opinions', [])
        consensus.setdefault('resolution', 'N/A')

        # Validate values
        valid_decisions = ['approve', 'reject', 'reduce_size', 'silence']
        if result['final_decision'] not in valid_decisions:
            result['final_decision'] = 'reject'

        valid_actions = ['buy', 'sell', 'hold']
        if result['recommended_action'] not in valid_actions:
            result['recommended_action'] = 'hold'

        result['confidence'] = max(0.0, min(1.0, float(result['confidence'])))
        result['position_size_adjustment'] = max(0.0, min(1.0, float(result['position_size_adjustment'])))

        return result

    def get_agent_info(self) -> Dict[str, Any]:
        """Get agent information"""
        return {
            'name': 'PMAgentMVP',
            'role': self.role,
            'focus': 'ìµœì¢… ì˜ì‚¬ê²°ì • + Hard Rules + Silence Policy',
            'responsibilities': [
                'Hard Rules ê²€ì¦ (code-enforced)',
                'Silence Policy ì‹¤í–‰',
                '3ê°œ Agent ì˜ê²¬ í†µí•©',
                'í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ì¤€ ë¦¬ìŠ¤í¬ ê´€ë¦¬',
                'ìµœì¢… ê±°ë¶€ê¶Œ í–‰ì‚¬'
            ],
            'hard_rules': self.HARD_RULES,
            'silence_threshold': self.SILENCE_THRESHOLD
        }


# Example usage
if __name__ == "__main__":
    pm = PMAgentMVP()

    # Test data
    trader_op = {
        'action': 'buy',
        'confidence': 0.75,
        'opportunity_score': 7.5,
        'reasoning': 'Strong momentum',
        'weight': 0.35
    }

    risk_op = {
        'risk_level': 'medium',
        'confidence': 0.65,
        'recommendation': 'approve',
        'position_size_usd': 5000,
        'position_size_pct': 0.05,
        'stop_loss_pct': 0.02,
        'reasoning': 'Acceptable risk',
        'weight': 0.35
    }

    analyst_op = {
        'action': 'buy',
        'confidence': 0.70,
        'overall_information_score': 6.0,
        'red_flags': [],
        'reasoning': 'Positive catalysts',
        'weight': 0.30
    }

    portfolio = {
        'total_value': 100000,
        'available_cash': 50000,
        'current_positions': [],
        'total_risk': 0.02
    }

    result = pm.make_final_decision(
        symbol='AAPL',
        trader_opinion=trader_op,
        risk_opinion=risk_op,
        analyst_opinion=analyst_op,
        portfolio_state=portfolio
    )

    print(f"Final Decision: {result['final_decision']}")
    print(f"Confidence: {result['confidence']:.2f}")
    print(f"Recommended Action: {result['recommended_action']}")
    print(f"Hard Rules Passed: {result['hard_rules_passed']}")
    print(f"Warnings: {result['warnings']}")
