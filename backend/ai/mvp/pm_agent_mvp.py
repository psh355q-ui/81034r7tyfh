"""
PM Agent MVP - Final Decision Maker

Phase: MVP Consolidation
Date: 2025-12-31

Purpose:
    í¬íŠ¸í´ë¦¬ì˜¤ ë§¤ë‹ˆì € - ìµœì¢… ì˜ì‚¬ê²°ì •ì
    - Hard Rules ê²€ì¦ (ì½”ë“œ ê¸°ë°˜, AI í•´ì„ ê¸ˆì§€)
    - Silence Policy ì‹¤í–‰ (íŒë‹¨ ê±°ë¶€ ê¶Œí•œ)
    - 3ê°œ Agent ì˜ê²¬ í†µí•© ë° ìµœì¢… ê²°ì •
    - í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ì¤€ ë¦¬ìŠ¤í¬ ê´€ë¦¬

Key Responsibilities:
    1. Hard Rules ê²€ì¦ (code-enforced, not AI)
    2. Silence Policy (confidence < threshold â†’ reject)
    3. 3ê°œ Agent ì˜ê²¬ ê°€ì¤‘ í‰ê·  ë° ìµœì¢… ê²°ì •
    4. í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ì¤€ ë¦¬ìŠ¤í¬ ì²´í¬ (ì§‘ì¤‘ë„, ìƒê´€ê´€ê³„)
    5. ìµœì¢… ê±°ë¶€ê¶Œ í–‰ì‚¬ (extreme risk, low confidence)

Hard Rules (Code-Enforced):
    1. Position Size > 30% â†’ REJECT
    2. Total Portfolio Risk > 5% â†’ REJECT
    3. Agent Disagreement > 60% â†’ REJECT or REDUCE
    4. Average Confidence < 50% â†’ REJECT (Silence Policy)
    5. Stop Loss not set â†’ REJECT
    6. Risk Level = "extreme" â†’ REJECT
"""

import os
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime
import google.generativeai as genai

from backend.ai.schemas.war_room_schemas import PMDecision
from backend.ai.safety.leverage_guardian import get_leverage_guardian
from backend.ai.router.persona_router import get_persona_router

# Configure logger
logger = logging.getLogger(__name__)


class PMAgentMVP:
    """MVP PM Agent - ìµœì¢… ì˜ì‚¬ê²°ì •ì + Hard Rules + Silence Policy"""

    def __init__(self):
        """Initialize PM Agent MVP"""
        # Gemini API ì„¤ì •
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found in environment variables")

        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.0-flash-exp')

        # Agent configuration
        self.role = "í¬íŠ¸í´ë¦¬ì˜¤ ë§¤ë‹ˆì €"

        # PersonaRouter ì¸ìŠ¤í„´ìŠ¤
        self.persona_router = get_persona_router()

        # Phase 2: Persona-specific Hard Rules
        self.PERSONA_HARD_RULES_DEF = {
            "trading": {
                "max_portfolio_risk_pct": 0.15,
                "max_agent_disagreement": 0.60,
                "max_position_pct": 0.10
            },
            "long_term": {
                "max_portfolio_risk_pct": 0.20,
                "max_agent_disagreement": 0.70,
                "max_position_pct": 0.15
            },
            "dividend": {
                "max_portfolio_risk_pct": 0.10,
                "max_agent_disagreement": 0.40,
                "max_position_pct": 0.08
            },
            "aggressive": {
                "max_portfolio_risk_pct": 0.25,
                "max_agent_disagreement": 0.80,
                "max_position_pct": 0.20
            }
        }
        
        # ===================================================================
        # HARD RULES (Dynamic from PersonaRouter)
        # Updated: 2026-01-08 - Persona-specific thresholds
        # ===================================================================
        # í˜ë¥´ì†Œë‚˜ë³„ ë™ì  ê·œì¹™ ê°€ì ¸ì˜¤ê¸°
        persona_hard_rules = self.persona_router.get_hard_rules()
        
        self.HARD_RULES = {
            'max_position_size': 0.30,  # 30% í¬ì§€ì…˜ ì ˆëŒ€ ìƒí•œ (ëª¨ë“  í˜ë¥´ì†Œë‚˜ ê³µí†µ)
            'max_portfolio_risk': 0.05,  # 5% í¬íŠ¸í´ë¦¬ì˜¤ ì „ì²´ ë¦¬ìŠ¤í¬ ìƒí•œ (ê³µí†µ)
            'min_avg_confidence': persona_hard_rules.get('min_avg_confidence', 0.50),  # í˜ë¥´ì†Œë‚˜ë³„
            'max_agent_disagreement': persona_hard_rules.get('max_agent_disagreement', 0.67),  # í˜ë¥´ì†Œë‚˜ë³„
            'stop_loss_required': True,  # Stop Loss í•„ìˆ˜ (ê³µí†µ)
            'reject_extreme_risk': True,  # Risk Level "extreme" ì‹œ ê±°ë¶€ (ê³µí†µ)
            'max_correlated_positions': 3,  # ë†’ì€ ìƒê´€ê´€ê³„ í¬ì§€ì…˜ ìµœëŒ€ 3ê°œ (ê³µí†µ)
            'max_sector_concentration': 0.40  # 40% ì„¹í„° ì§‘ì¤‘ë„ ìƒí•œ (ê³µí†µ)
        }
        
        # ğŸ” DEBUG: PM Agent ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œì  í™•ì¸
        current_mode = self.persona_router.get_current_mode()
        logger.info(
            f"ğŸ” INIT DEBUG: PMAgentMVP created with Persona={current_mode.value}, "
            f"max_agent_disagreement={self.HARD_RULES['max_agent_disagreement']}, "
            f"min_avg_confidence={self.HARD_RULES['min_avg_confidence']}, "
            f"instance_id={id(self)}"
        )

        # Silence Policy threshold
        self.SILENCE_THRESHOLD = 0.50  # Confidence < 50% â†’ íŒë‹¨ ê±°ë¶€

        # System prompt
        self.system_prompt = """ë‹¹ì‹ ì€ í¬íŠ¸í´ë¦¬ì˜¤ ë§¤ë‹ˆì €ì…ë‹ˆë‹¤.

ì—­í• :
1. 3ê°œ Agent ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ê²°ì •
2. í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ì¤€ ë¦¬ìŠ¤í¬ í‰ê°€
3. ì˜ê²¬ ë¶ˆì¼ì¹˜ ì‹œ ì¡°ì • ë° ì¤‘ì¬
4. ìµœì¢… ê±°ë¶€ê¶Œ í–‰ì‚¬ (í•„ìš” ì‹œ)

ë¶„ì„ ì›ì¹™:
- Hard RulesëŠ” ì½”ë“œê°€ ê²€ì¦ (ë‹¹ì‹ ì€ íŒë‹¨ë§Œ)
- Confidenceê°€ ë‚®ìœ¼ë©´ ê±°ë¶€ ê¶Œì¥
- Agent ê°„ ì˜ê²¬ ì°¨ì´ê°€ í¬ë©´ ì‹ ì¤‘ ëª¨ë“œ
- í¬íŠ¸í´ë¦¬ì˜¤ ì „ì²´ ê´€ì ì—ì„œ í‰ê°€

## Context-Aware Analysis (NEW)

`action_context` íŒŒë¼ë¯¸í„°ì— ë”°ë¼ ë¶„ì„ ê´€ì ì„ ì¡°ì •í•˜ì„¸ìš”:

### 1. existing_position (ë³´ìœ  ì¤‘ì¸ ì¢…ëª©)
- **ëª©ì **: HOLD vs SELL íŒë‹¨, ì¶”ê°€ë§¤ìˆ˜ ì—¬ë¶€ ê²°ì •
- **ë¶„ì„ ì´ˆì **:
  - í˜„ì¬ í¬ì§€ì…˜ ìœ ì§€ ê¶Œì¥ ì—¬ë¶€
  - ì¶”ê°€ ë§¤ìˆ˜ íƒ€ì´ë° ë° ê°€ê²©ëŒ€ (êµ¬ì²´ì )
  - ìµì ˆ/ì†ì ˆ ë ˆë²¨ (í‰ê· ê°€ ëŒ€ë¹„ %)
  - Stop-loss ì¡°ì • ê¶Œì¥
  - í¬ì§€ì…˜ ì¶•ì†Œ/í™•ëŒ€ ë¹„ìœ¨
  - íˆ¬ì ë…¼ë¦¬(Thesis) ìœ íš¨ì„± ì¬í™•ì¸
  - ë‹¤ìŒ ì¬í‰ê°€ ì‹œì  (ì‹¤ì  ë°œí‘œ, ì´ë²¤íŠ¸)

### 2. new_position (ì‹ ê·œ ì§„ì… ê²€í† )
- **ëª©ì **: BUY vs HOLD íŒë‹¨
- **ë¶„ì„ ì´ˆì **:
  - ì§„ì… íƒ€ì´ë° ë° ì§„ì…ê°€
  - ëª©í‘œê°€ ë° ì†ì ˆê°€
  - í¬ì§€ì…˜ ì‚¬ì´ì¦ˆ ê¶Œì¥

## Portfolio Action Guide (NEW)

ë³´ìœ  ì¢…ëª©ì— ëŒ€í•´ ë‹¤ìŒ 4ê°€ì§€ ì•¡ì…˜ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:

1. **SELL (ë§¤ë„ ì¶”ì²œ)**: ë¦¬ìŠ¤í¬ ê¸‰ì¦, ì†ì ˆê°€ ë„ë‹¬, ëª©í‘œê°€ ë„ë‹¬, ê¸°ìˆ ì  ì•½ì„¸
   - ì–¸ì œ: êµ¬ì²´ì  ê°€ê²© ë ˆë²¨ ë˜ëŠ” ì¡°ê±´ (ì˜ˆ: "$185 ì €í•­ ëŒíŒŒ ì‹¤íŒ¨ ì‹œ")
   - ì–¼ë§ˆë‚˜: ì¼ë¶€ ìµì ˆ(50%) vs ì „ëŸ‰ ì²­ì‚°

2. **BUY_MORE (ì¶”ê°€ ë§¤ìˆ˜)**: ê°•í•œ ëª¨ë©˜í…€, ê¸ì •ì  ì´‰ë§¤, ë‚®ì€ ë¦¬ìŠ¤í¬
   - ì–¸ì œ: êµ¬ì²´ì  ë§¤ìˆ˜ íƒ€ì´ë° (ì˜ˆ: "ì§€ì§€ì„  $176 ìœ ì§€ ì‹œ")
   - ì–¼ë§ˆë‚˜: ì¶”ê°€ ë§¤ìˆ˜ ë¹„ì¤‘ (ex: í˜„ì¬ ëŒ€ë¹„ +20%)

3. **HOLD (ë³´ìœ  ìœ ì§€)**: ì¤‘ë¦½ì  ì‹ í˜¸, ì´‰ë§¤ ëŒ€ê¸° ì¤‘
   - ì¶”ê°€ ë§¤ìˆ˜ ë¶ˆí•„ìš” ëª…ì‹œ
   - ë‹¤ìŒ ì¬í‰ê°€ ì‹œì  ì œì‹œ (ì˜ˆ: "ì‹¤ì  ë°œí‘œ 2026-02-15 í›„")
   - Stop-loss ì¡°ì • ì—¬ë¶€

4. **DO_NOT_BUY (ë¯¸ì§„ì…/ê´€ë§)**: ë†’ì€ ë¦¬ìŠ¤í¬, ë¶ˆí™•ì‹¤í•œ í…Œë§ˆ

ì¶œë ¥ í˜•ì‹:
{
    ... existing fields ...,
    "portfolio_action": "buy_more" | "sell" | "hold" | "do_not_buy",
    "action_reason": "ì•¡ì…˜ ì„ íƒ ì´ìœ  (í•œêµ­ì–´, êµ¬ì²´ì  ê°€ê²©/ì¡°ê±´ í¬í•¨)",
    "action_strength": "weak" | "moderate" | "strong",
    "position_adjustment_pct": -1.0 ~ 1.0  // -0.5 = 50% ë§¤ë„, +0.2 = 20% ì¶”ê°€ë§¤ìˆ˜
}

**ì¤‘ìš”**: action_reasonì—ëŠ” ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ê°€ê²© ë ˆë²¨ê³¼ ì¡°ê±´ì„ í¬í•¨í•˜ì„¸ìš”.
ì˜ˆ: "í‰ê· ê°€ $175 ëŒ€ë¹„ í˜„ì¬ê°€ $178 (+1.7%), ì €í•­ì„  $185 ëŒíŒŒ ì‹œ 50% ìµì ˆ ê¶Œì¥"

## Original Output Format
{
    "final_decision": "approve" | "reject" | "reduce_size" | "silence" | "conditional",
    "confidence": 0.0 ~ 1.0,
    "reasoning": "ìµœì¢… ê²°ì • ê·¼ê±°",
    "conditions": ["ì¡°ê±´1", "ì¡°ê±´2"] (final_decision="conditional"ì¼ ë•Œ í•„ìˆ˜),
    "human_question": "ì¸ê°„ í™•ì¸ ì§ˆë¬¸" (conditionalì¼ ë•Œ),
    "recommended_action": "buy" | "sell" | "hold",
    "position_size_adjustment": 0.0 ~ 1.0 (1.0 = full size, 0.5 = half),
    "risk_assessment": {
        "portfolio_risk_score": 0.0 ~ 10.0,
        "concentration_risk": 0.0 ~ 10.0,
        "correlation_risk": 0.0 ~ 10.0,
        "overall_portfolio_health": 0.0 ~ 10.0
    },
    "agent_consensus": {
        "agreement_level": 0.0 ~ 1.0,
        "conflicting_opinions": ["agent1 vs agent2 on X"],
        "resolution": "how conflicts were resolved"
    },
    "warnings": ["warning1", "warning2", ...],
    "approval_conditions": ["condition1", "condition2", ...] or [],
    "portfolio_action": "buy_more" | "sell" | "hold" | "do_not_buy",
    "action_reason": "ì•¡ì…˜ ì„ íƒ ì´ìœ  (í•œêµ­ì–´, êµ¬ì²´ì  ê°€ê²©/ì¡°ê±´ í¬í•¨)",
    "action_strength": "weak" | "moderate" | "strong",
    "position_adjustment_pct": -1.0 ~ 1.0
}

ì¤‘ìš”:
- final_decision = "silence"ëŠ” íŒë‹¨ ê±°ë¶€ (ì •ë³´ ë¶ˆì¶©ë¶„)
- Agent ì˜ê²¬ì´ ìƒì¶©í•˜ë©´ ë³´ìˆ˜ì ìœ¼ë¡œ ê²°ì •
- í¬íŠ¸í´ë¦¬ì˜¤ ì „ì²´ ê±´ê°•ë„ ìš°ì„  ê³ ë ¤
- **ë°˜ë“œì‹œ í•œê¸€ë¡œ ì‘ë‹µí•  ê²ƒ** (reasoning, warnings, approval_conditions, action_reason ë“± ëª¨ë“  í…ìŠ¤íŠ¸ í•„ë“œëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±)
"""

    def make_final_decision(
        self,
        symbol: str,
        trader_opinion: Dict[str, Any],
        risk_opinion: Dict[str, Any],
        analyst_opinion: Dict[str, Any],
        portfolio_state: Dict[str, Any],
        correlation_data: Optional[Dict[str, Any]] = None,
        action_context: str = "new_position"
    ) -> Dict[str, Any]:
        """
        ìµœì¢… ì˜ì‚¬ê²°ì • ìˆ˜í–‰
        
        Returns:
            Dict (compatible with PMDecision model)
        """
        # ================================================================
        # STEP 1: HARD RULES VALIDATION (Code-Enforced)
        # ================================================================
        hard_rules_result = self._validate_hard_rules(
            symbol=symbol,
            trader_opinion=trader_opinion,
            risk_opinion=risk_opinion,
            analyst_opinion=analyst_opinion,
            portfolio_state=portfolio_state,
            correlation_data=correlation_data
        )

        # Hard Rules ìœ„ë°˜ ì‹œ ì¦‰ì‹œ ê±°ë¶€
        if not hard_rules_result['passed']:
            return {
                'agent': 'pm_mvp',
                'final_decision': 'reject',
                'action': 'reject', # Schema compatibility
                'confidence': 0.0,
                'reasoning': f"Hard Rules ìœ„ë°˜: {', '.join(hard_rules_result['violations'])}",
                'recommended_action': 'hold',
                'position_size_adjustment': 0.0,
                'risk_assessment': {
                    'portfolio_risk_score': 10.0,
                    'concentration_risk': 10.0,
                    'correlation_risk': 10.0,
                    'overall_portfolio_health': 0.0
                },
                'agent_consensus': {
                    'agreement_level': 0.0,
                    'conflicting_opinions': [],
                    'resolution': 'Rejected by Hard Rules'
                },
                'warnings': hard_rules_result['violations'],
                'approval_conditions': [],
                'hard_rules_passed': False,
                'hard_rules_violations': hard_rules_result['violations'],
                'timestamp': datetime.utcnow().isoformat(),
                'symbol': symbol
            }

        # ================================================================
        # STEP 2: SILENCE POLICY CHECK (Dynamic from Persona)
        # ================================================================
        # í˜„ì¬ Personaì˜ min_avg_confidence ê°€ì ¸ì˜¤ê¸°
        current_persona_rules = self.persona_router.get_hard_rules()
        min_confidence_threshold = current_persona_rules.get('min_avg_confidence', 0.50)
        
        avg_confidence = (
            trader_opinion.get('confidence', 0) * trader_opinion.get('weight', 0.35) +
            risk_opinion.get('confidence', 0) * risk_opinion.get('weight', 0.35) +
            analyst_opinion.get('confidence', 0) * analyst_opinion.get('weight', 0.30)
        )

        # Silence Policy: í‰ê·  confidence < threshold â†’ íŒë‹¨ ê±°ë¶€ (Dynamic)
        if avg_confidence < min_confidence_threshold:
            current_mode = self.persona_router.get_current_mode()
            return {
                'agent': 'pm_mvp',
                'final_decision': 'silence',
                'action': 'silence', # Schema compatibility
                'confidence': avg_confidence,
                'reasoning': f"Silence Policy: Average confidence ({avg_confidence:.2f}) below threshold ({min_confidence_threshold}) for {current_mode.value} mode",
                'recommended_action': 'hold',
                'position_size_adjustment': 0.0,
                'risk_assessment': {
                    'portfolio_risk_score': 5.0,
                    'concentration_risk': 5.0,
                    'correlation_risk': 5.0,
                    'overall_portfolio_health': 5.0
                },
                'agent_consensus': {
                    'agreement_level': 0.0,
                    'conflicting_opinions': [],
                    'resolution': 'Silence - insufficient confidence'
                },
                'warnings': ['Insufficient confidence for decision'],
                'approval_conditions': [],
                'hard_rules_passed': True,
                'hard_rules_violations': [],
                'timestamp': datetime.utcnow().isoformat(),
                'symbol': symbol
            }

        # ================================================================
        # STEP 3: AI-BASED FINAL DECISION
        # ================================================================
        # Construct prompt for PM Agent
        prompt = self._build_prompt(
            symbol=symbol,
            trader_opinion=trader_opinion,
            risk_opinion=risk_opinion,
            analyst_opinion=analyst_opinion,
            portfolio_state=portfolio_state,
            correlation_data=correlation_data,
            avg_confidence=avg_confidence,
            action_context=action_context
        )

        # Call Gemini API
        try:
            response = self.model.generate_content([
                self.system_prompt,
                prompt
            ])

            logger.info(f"ğŸ” DEBUG: PM Agent Raw Response:\n{response.text}")

            # Parse and Validate with Pydantic
            decision = self._parse_response(response.text)
            
            # Convert to dict
            result = decision.model_dump()
            
            # Map action -> final_decision (for backward compatibility if needed)
            if 'action' in result and 'final_decision' not in result:
                result['final_decision'] = result['action']
            
            # Map risk_warnings -> warnings (for compatibility if needed)
            if 'risk_warnings' in result and 'warnings' not in result:
                result['warnings'] = result['risk_warnings']

            # Add metadata and hard rules info
            result['agent'] = 'pm_mvp'
            result['hard_rules_passed'] = True
            result['hard_rules_violations'] = []
            result['timestamp'] = datetime.utcnow().isoformat()
            result['symbol'] = symbol
            result['avg_agent_confidence'] = avg_confidence

            # NEW: Add portfolio action guide
            # If AI didn't provide portfolio_action, determine it from the decision
            if 'portfolio_action' not in result or not result['portfolio_action']:
                action_guide = self._determine_portfolio_action(
                    final_decision=result.get('final_decision', 'hold'),
                    recommended_action=result.get('recommended_action', 'hold'),
                    confidence=result.get('confidence', 0.5),
                    risk_level=risk_opinion.get('risk_level', 'medium'),
                    action_context=action_context
                )
                result.update(action_guide)

            return result

        except Exception as e:
            logger.error(f"âŒ PM Agent Analysis Failed: {str(e)}", exc_info=True)
            # Error handling - return safe default (reject)
            return {
                'agent': 'pm_mvp',
                'final_decision': 'reject',
                'action': 'reject',
                'confidence': 0.0,
                'reasoning': f'PM analysis failed: {str(e)}',
                'recommended_action': 'hold',
                'position_size_adjustment': 0.0,
                'risk_assessment': {
                    'portfolio_risk_score': 10.0,
                    'concentration_risk': 10.0,
                    'correlation_risk': 10.0,
                    'overall_portfolio_health': 0.0
                },
                'agent_consensus': {
                    'agreement_level': 0.0,
                    'conflicting_opinions': [],
                    'resolution': 'Error - rejected for safety'
                },
                'warnings': [f'PM Agent error: {str(e)}'],
                'approval_conditions': [],
                'hard_rules_passed': True,
                'hard_rules_violations': [],
                'timestamp': datetime.utcnow().isoformat(),
                'symbol': symbol,
                'error': str(e)
            }

    def _validate_hard_rules(
        self,
        symbol: str,
        trader_opinion: Dict[str, Any],
        risk_opinion: Dict[str, Any],
        analyst_opinion: Dict[str, Any],
        portfolio_state: Dict[str, Any],
        correlation_data: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Hard Rules ê²€ì¦ (Code-Enforced)
        Uses Persona-specific rules for limits.
        """
        violations = []
        
        # Get Current Persona Rules
        current_mode = self.persona_router.get_current_mode()
        persona_key = current_mode.value if hasattr(current_mode, 'value') else str(current_mode)
        
        # Fallback to default if persona not in map
        rules = self.PERSONA_HARD_RULES_DEF.get(persona_key, self.PERSONA_HARD_RULES_DEF['trading'])
        
        # Rule 1: Position Size Limit (Persona Specific)
        # Check absolute hard cap (30%) first, then persona limit
        position_size_pct = risk_opinion.get('position_size_pct', 0.0)
        
        if position_size_pct > self.HARD_RULES['max_position_size']: # 30% Absolute Cap
             violations.append(
                f"í¬ì§€ì…˜ í¬ê¸° {position_size_pct*100:.1f}%ê°€ ì‹œìŠ¤í…œ ì ˆëŒ€ í•œë„ {self.HARD_RULES['max_position_size']*100}%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤"
            )
        elif position_size_pct > rules['max_position_pct']: # Persona limit
            violations.append(
                f"í¬ì§€ì…˜ í¬ê¸° {position_size_pct*100:.1f}%ê°€ í˜„ì¬ í˜ë¥´ì†Œë‚˜({persona_key}) í•œë„ {rules['max_position_pct']*100:.1f}%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤"
            )

        # Rule 2: Total Portfolio Risk (Persona Specific)
        total_risk = portfolio_state.get('total_risk', 0.0)
        
        # Check absolute cap (5%) first? Or just use persona rule?
        # Plan implies specific persona rules: 15%, 20%, 10%, 25%.
        # Legacy hard rule was 5%. The new rules are much looser.
        # I will use the persona rule, but warn if it exceeds the 'safe' 5% legacy baseline if desired?
        # No, the plan replaces the 5% fixed limit.
        
        if total_risk > rules['max_portfolio_risk_pct']:
            violations.append(
                f"í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ìŠ¤í¬ {total_risk*100:.1f}%ê°€ í˜„ì¬ í˜ë¥´ì†Œë‚˜({persona_key}) í•œë„ {rules['max_portfolio_risk_pct']*100:.1f}%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤"
            )

        # Rule 3: Agent Disagreement (Directional) -- Updated to use pre-fetched rules
        max_disagreement = rules['max_agent_disagreement']

        # Debug info
        logger.info(
            f"ğŸ” VALIDATION DEBUG: Persona={persona_key}, Rules={{Risk: {rules['max_portfolio_risk_pct']:.2%}, "
            f"Pos: {rules['max_position_pct']:.2%}, Disagree: {max_disagreement:.2%}}}"
        )

        disagreement = self._calculate_directional_disagreement(
            votes=[
                {'action': trader_opinion.get('action', 'pass'), 'weight': 0.35},
                {'action': risk_opinion.get('recommendation', 'reject'), 'weight': 0.35},
                {'action': analyst_opinion.get('action', 'pass'), 'weight': 0.30}
            ]
        )
        
        # logger.info(...) already in code, keeping logic concise
        if disagreement > max_disagreement:
             violations.append(
                 f"Agent ë°©í–¥ì„± ë¶ˆì¼ì¹˜ {disagreement*100:.0f}%ê°€ ìµœëŒ€ í—ˆìš©ì¹˜ {max_disagreement*100:.0f}%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤ (Persona: {persona_key})"
            )
            
        # Update validation for Position Size and Portfolio Risk using Persona Rules
        # (Overwriting logic for Rule 1 & 2 implies we should have replaced them, 
        # but since this tool replaces chunks, I will modify them in subsequent chunks or assumes default usage is okay for now 
        # BUT wait, I need to apply persona_hard_rules to Rule 1 & 2 too.
        # I will inject a helper method for getting current rules and update Rule 1 & 2 in a separate tool call or larger chunk if possible.
        # For now, let's stick to replacing Rule 3 logic.)

        
        actions = [
            trader_opinion.get('action', 'pass'),
            risk_opinion.get('recommendation', 'reject'),
            analyst_opinion.get('action', 'pass')
        ]
        # Count unique actions (excluding 'pass')
        non_pass_actions = [a for a in actions if a != 'pass']
        if len(non_pass_actions) > 0:
            disagreement = 1.0 - (non_pass_actions.count(non_pass_actions[0]) / len(non_pass_actions))
            # ğŸ” DEBUG: í˜„ì¬ Persona ë° ë™ì  ê¸°ì¤€ í‘œì‹œ
            logger.warning(
                f"ğŸ” VALIDATION DEBUG: Persona={current_mode.value}, "
                f"disagreement={disagreement:.2f}, max_allowed={max_disagreement}, "
                f"actions={actions}, non_pass={non_pass_actions}"
            )
            if disagreement > max_disagreement:
                violations.append(
                    f"Agent ì˜ê²¬ ë¶ˆì¼ì¹˜ {disagreement*100:.0f}%ê°€ ìµœëŒ€ í—ˆìš©ì¹˜ {max_disagreement*100:.0f}%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤ (Persona: {current_mode.value})"
                )

        # Rule 4: Average Confidence (Dynamic from Persona)
        # í˜„ì¬ Personaì˜ min_avg_confidence ê°€ì ¸ì˜¤ê¸°
        current_persona_rules = self.persona_router.get_hard_rules()
        min_confidence = current_persona_rules.get('min_avg_confidence', 0.50)
        
        confidences = [
            trader_opinion.get('confidence', 0.0),
            risk_opinion.get('confidence', 0.0),
            analyst_opinion.get('confidence', 0.0)
        ]
        avg_conf = sum(confidences) / len(confidences)
        if avg_conf < min_confidence:
            violations.append(
                f"í‰ê·  ì‹ ë¢°ë„ {avg_conf*100:.0f}%ê°€ ìµœì†Œ ìš”êµ¬ì¹˜ {min_confidence*100:.0f}% ë¯¸ë§Œì…ë‹ˆë‹¤"
            )

        # Rule 5: Stop Loss Required
        if self.HARD_RULES['stop_loss_required']:
            stop_loss = float(risk_opinion.get('stop_loss_pct', 0.0))
            
            # AI Hallucination Guard: if > 1.0, assume it means percentage (e.g. 10.5 -> 0.105)
            if abs(stop_loss) > 1.0:
                stop_loss = stop_loss / 100.0
                
            # Handle negative values (e.g. -0.05 for 5% loss)
            abs_stop_loss = abs(stop_loss)
            
            if abs_stop_loss <= 0.0 or abs_stop_loss > 0.20:  # Must be 0.1% ~ 20%
                violations.append(
                    f"ì†ì ˆë§¤ {stop_loss*100:.2f}%ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (0.1% ~ 20% ë²”ìœ„ì—¬ì•¼ í•¨)"
                )

        # Rule 6: Risk Level "extreme" â†’ Reject
        if self.HARD_RULES['reject_extreme_risk']:
            risk_level = risk_opinion.get('risk_level', 'medium')
            if risk_level == 'extreme':
                violations.append(
                    "ë¦¬ìŠ¤í¬ ìˆ˜ì¤€ì´ 'extreme'ìœ¼ë¡œ ìë™ ê±°ë¶€ë©ë‹ˆë‹¤"
                )

        # Rule 7: Correlated Positions > 3
        if correlation_data:
            correlated_positions = correlation_data.get('correlated_positions', [])
            high_corr_count = len([p for p in correlated_positions if p.get('correlation', 0) > 0.7])
            if high_corr_count >= self.HARD_RULES['max_correlated_positions']:
                violations.append(
                    f"ìƒê´€ê´€ê³„ê°€ ë†’ì€ í¬ì§€ì…˜ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤ ({high_corr_count}ê°œ) - ìµœëŒ€ {self.HARD_RULES['max_correlated_positions']}ê°œ"
                )

        # Rule 8: Sector Concentration > 40%
        current_positions = portfolio_state.get('current_positions', [])
        total_value = portfolio_state.get('total_value', 1)
        if current_positions:
            # Calculate sector concentration
            sector_values = {}
            for pos in current_positions:
                sector = pos.get('sector', 'Unknown')
                value = pos.get('value', 0)
                sector_values[sector] = sector_values.get(sector, 0) + value

            max_sector_pct = max(sector_values.values()) / total_value if total_value > 0 else 0
            if max_sector_pct > self.HARD_RULES['max_sector_concentration']:
                violations.append(
                    f"ì„¹í„° ì§‘ì¤‘ë„ {max_sector_pct*100:.1f}%ê°€ ìµœëŒ€ í—ˆìš©ì¹˜ {self.HARD_RULES['max_sector_concentration']*100:.1f}%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤"
                )

        # Rule 9: Leverage Guardian (10% cap on leveraged ETFs)
        leverage_guardian = get_leverage_guardian()
        if leverage_guardian.is_leveraged(symbol):
            portfolio_value = portfolio_state.get('total_value', 100000)
            current_leverage_value = sum(
                pos.get('value', 0) for pos in current_positions 
                if leverage_guardian.is_leveraged(pos.get('symbol', ''))
            )
            position_value = risk_opinion.get('position_size_usd', 0)
            
            # Check if this order would exceed leverage cap
            max_leverage_value = portfolio_value * 0.10  # 10% cap
            if current_leverage_value + position_value > max_leverage_value:
                violations.append(
                    f"ë ˆë²„ë¦¬ì§€ ìƒí’ˆ í•œë„ ì´ˆê³¼: í˜„ì¬ {current_leverage_value:,.0f}ì› + ì‹ ê·œ {position_value:,.0f}ì› > ìµœëŒ€ {max_leverage_value:,.0f}ì› (10%)"
                )
            else:
                # Add warning (not violation) for leverage products
                logger.warning(
                    f"âš ï¸ ë ˆë²„ë¦¬ì§€ ìƒí’ˆ {symbol} ê±°ë˜: í˜„ì¬ ë ˆë²„ë¦¬ì§€ ë¹„ì¤‘ "
                    f"{(current_leverage_value + position_value) / portfolio_value * 100:.1f}%"
                )

        return {
            'passed': len(violations) == 0,
            'violations': violations
        }

    def _calculate_directional_disagreement(self, votes: List[Dict[str, Any]]) -> float:
        """
        Calculate disagreement based on direction (Attack vs Defense).
        Neutral votes are excluded from disagreement calculation.
        """
        directions = {
            "attack": ["buy", "ë§¤ìˆ˜", "approve", "recommend"],
            "defense": ["sell", "reduce_size", "ì¶•ì†Œ", "reject"],
            "neutral": ["hold", "ë³´ë¥˜", "pass", "silence"]
        }
        
        attack_weight = 0.0
        defense_weight = 0.0
        
        for v in votes:
            action = v.get('action', '').lower()
            weight = v.get('weight', 0.0)
            
            if action in directions['attack']:
                attack_weight += weight
            elif action in directions['defense']:
                defense_weight += weight
            # Neutral is ignored
            
        total = attack_weight + defense_weight
        if total == 0:
            return 0.0
            
        minority = min(attack_weight, defense_weight)
        # Disagreement is the ratio of the minority opinion to the total non-neutral opinion
        # e.g. 0.7 vs 0.3 -> disagreement is 0.3 / 1.0 = 0.3
        # e.g. 0.35 vs 0.35 -> disagreement is 0.35 / 0.7 = 0.5 (maximum disagreement)
        # Formula from plan: return minority / total
        return minority / total

    def _build_prompt(
        self,
        symbol: str,
        trader_opinion: Dict[str, Any],
        risk_opinion: Dict[str, Any],
        analyst_opinion: Dict[str, Any],
        portfolio_state: Dict[str, Any],
        correlation_data: Optional[Dict[str, Any]],
        avg_confidence: float,
        action_context: str = "new_position"
    ) -> str:
        """Build PM decision prompt"""
        prompt_parts = [
            f"ì¢…ëª©: {symbol}",
            f"Context: {action_context.upper()}",
            f"í‰ê·  Confidence: {avg_confidence:.2f}",
            "",
            "=== Trader Agent (35% weight) ===",
            f"Action: {trader_opinion.get('action', 'N/A')}",
            f"Confidence: {trader_opinion.get('confidence', 0):.2f}",
            f"Opportunity Score: {trader_opinion.get('opportunity_score', 0):.1f}",
            f"Reasoning: {trader_opinion.get('reasoning', 'N/A')}",
            "",
            "=== Risk Agent (35% weight) ===",
            f"Risk Level: {risk_opinion.get('risk_level', 'N/A')}",
            f"Confidence: {risk_opinion.get('confidence', 0):.2f}",
            f"Recommendation: {risk_opinion.get('recommendation', 'N/A')}",
            f"Position Size: ${risk_opinion.get('position_size_usd', 0):,.0f} ({(risk_opinion.get('position_size_pct', 0) * 100):.1f}%)",
            f"Stop Loss: {(risk_opinion.get('stop_loss_pct', 0) * 100):.1f}%",
            f"Reasoning: {risk_opinion.get('reasoning', 'N/A')}",
            "",
            "=== Analyst Agent (30% weight) ===",
            f"Action: {analyst_opinion.get('action', 'N/A')}",
            f"Confidence: {analyst_opinion.get('confidence', 0):.2f}",
            f"Info Score: {analyst_opinion.get('overall_information_score', 0):.1f}",
            f"Red Flags: {', '.join(analyst_opinion.get('red_flags', [])) or 'None'}",
            f"Reasoning: {analyst_opinion.get('reasoning', 'N/A')}",
            "",
            "=== Portfolio State ===",
            f"Total Value: ${portfolio_state.get('total_value', 0):,.0f}",
            f"Available Cash: ${portfolio_state.get('available_cash', 0):,.0f}",
            f"Current Positions: {len(portfolio_state.get('current_positions', []))}",
            f"Total Risk: {(portfolio_state.get('total_risk', 0) * 100):.1f}%",
        ]

        if correlation_data:
            prompt_parts.append("\n=== Correlation Data ===")
            corr_positions = correlation_data.get('correlated_positions', [])
            if corr_positions:
                prompt_parts.append("Highly Correlated Positions:")
                for pos in corr_positions[:3]:
                    prompt_parts.append(f"  - {pos.get('symbol', 'N/A')}: {pos.get('correlation', 0):.2f}")

        prompt_parts.append("\nìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ê²°ì •ì„ ë‚´ë¦¬ê³  JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.")

        return "\n".join(prompt_parts)

    def _determine_portfolio_action(
        self,
        final_decision: str,
        recommended_action: str,
        confidence: float,
        risk_level: str,
        action_context: str = "new_position"
    ) -> Dict[str, Any]:
        """
        Determine portfolio-level action from agent inputs.

        Mapping Logic:
        - approve + sell â†’ SELL
        - approve + buy + confidence > 0.7 â†’ BUY_MORE
        - approve + buy + confidence 0.5-0.7 â†’ HOLD
        - reject + extreme risk â†’ SELL
        - reject + medium/high risk â†’ HOLD
        - silence â†’ HOLD
        - reduce_size â†’ SELL (partial)

        Args:
            final_decision: PM's final decision (approve/reject/silence/reduce_size)
            recommended_action: Recommended action (buy/sell/hold)
            confidence: Confidence level (0.0 ~ 1.0)
            risk_level: Risk level (low/medium/high/extreme)

        Returns:
            Dict with portfolio_action, action_strength, position_adjustment_pct
        """
        # Action mapping based on final_decision, recommended_action, confidence, risk_level
        action_map = {
            ("approve", "sell"): ("sell", "strong"),
            ("approve", "buy"): ("buy_more" if confidence > 0.7 else "hold", "moderate"),
            ("reject", "extreme"): ("sell", "strong"),
            ("reject", "high"): ("hold", "moderate"),
            ("reject", "medium"): ("hold", "moderate"),
            ("silence", ""): ("hold", "weak"),
            ("reduce_size", ""): ("sell", "moderate"),
        }

        # Determine key for action_map
        if final_decision == "reject" and risk_level == "extreme":
            key = ("reject", "extreme")
        elif final_decision == "reject" and risk_level in ("high", "medium"):
            key = ("reject", risk_level)
        elif final_decision == "approve":
            key = ("approve", recommended_action)
        elif final_decision == "silence":
            key = ("silence", "")
        elif final_decision == "reduce_size":
            key = ("reduce_size", "")
        else:
            key = ("approve", "hold")  # Default fallback

        portfolio_action, strength = action_map.get(key, ("hold", "moderate"))

        # Context-Aware Refinement
        if action_context == "existing_position":
            # Avoid 'do_not_buy' for existing positions
            if portfolio_action == "do_not_buy":
                portfolio_action = "hold"
        elif action_context == "new_position":
            # For new positions, 'hold' often means 'do_not_buy' (don't enter yet)
            if portfolio_action == "hold":
                portfolio_action = "do_not_buy"
            # 'sell' is invalid for new position, map to 'do_not_buy'
            if portfolio_action == "sell":
                portfolio_action = "do_not_buy"

        return {
            "portfolio_action": portfolio_action,
            "action_strength": strength,
            "position_adjustment_pct": self._calculate_position_adjustment(
                portfolio_action, confidence
            )
        }

    def _calculate_position_adjustment(self, action: str, confidence: float) -> float:
        """
        Calculate position adjustment percentage.

        Args:
            action: Portfolio action (sell/buy_more/hold/do_not_buy)
            confidence: Confidence level (0.0 ~ 1.0)

        Returns:
            Position adjustment percentage (-1.0 ~ 1.0)
            -0.5 = sell 50%, +0.2 = buy 20% more
        """
        adjustments = {
            "sell": -0.5,      # Sell 50%
            "buy_more": 0.2,    # Add 20%
            "hold": 0.0,
            "do_not_buy": 0.0
        }
        base = adjustments.get(action, 0.0)
        return base * confidence  # Scale by confidence

    def _parse_response(self, response_text: str) -> PMDecision:
        """Parse Gemini response using Pydantic"""
        import json
        import re

        # Extract JSON
        try:
            result_dict = json.loads(response_text)
        except json.JSONDecodeError:
            json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                result_dict = json.loads(json_match.group(1))
            else:
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    result_dict = json.loads(json_match.group(0))
                else:
                    raise ValueError("No valid JSON found")

        # Map warnings -> risk_warnings
        if 'warnings' in result_dict and 'risk_warnings' not in result_dict:
            result_dict['risk_warnings'] = result_dict.pop('warnings')
            
        # Inject hard_rules_passed (logic handled outside AI, so if we are here, it passed)
        if 'hard_rules_passed' not in result_dict:
            result_dict['hard_rules_passed'] = True

        # Ensure compatibility
        valid_actions = ['approve', 'reject', 'reduce_size', 'silence', 'conditional']
        if result_dict.get('action') not in valid_actions:
            result_dict['action'] = 'reject'
            
        # Map conditional -> approve for legacy compatibility (optional, or keep as conditional)
        # If final_decision is conditional, ensures action is at least 'hold' or 'conditional'
        if result_dict.get('final_decision') == 'conditional':
             result_dict['action'] = 'conditional' # Ensure action matches if used elsewhere
            
        # Instantiate and Validate with Pydantic
        return PMDecision(**result_dict)

    def get_agent_info(self) -> Dict[str, Any]:
        """Get agent information"""
        return {
            'name': 'PMAgentMVP',
            'role': self.role,
            'focus': 'ìµœì¢… ì˜ì‚¬ê²°ì • + Hard Rules + Silence Policy',
            'responsibilities': [
                'Hard Rules ê²€ì¦ (code-enforced)',
                'Silence Policy ì‹¤í–‰',
                '3ê°œ Agent ì˜ê²¬ í†µí•©',
                'í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ì¤€ ë¦¬ìŠ¤í¬ ê´€ë¦¬',
                'ìµœì¢… ê±°ë¶€ê¶Œ í–‰ì‚¬'
            ],
            'hard_rules': self.HARD_RULES,
            'silence_threshold': self.SILENCE_THRESHOLD
        }


# Example usage
if __name__ == "__main__":
    pm = PMAgentMVP()

    # Test data
    trader_op = {
        'action': 'buy',
        'confidence': 0.75,
        'opportunity_score': 7.5,
        'reasoning': 'Strong momentum',
        'weight': 0.35
    }

    risk_op = {
        'risk_level': 'medium',
        'confidence': 0.65,
        'recommendation': 'approve',
        'position_size_usd': 5000,
        'position_size_pct': 0.05,
        'stop_loss_pct': 0.02,
        'reasoning': 'Acceptable risk',
        'weight': 0.35
    }

    analyst_op = {
        'action': 'buy',
        'confidence': 0.70,
        'overall_information_score': 6.0,
        'red_flags': [],
        'reasoning': 'Positive catalysts',
        'weight': 0.30
    }

    portfolio = {
        'total_value': 100000,
        'available_cash': 50000,
        'current_positions': [],
        'total_risk': 0.02
    }

    result = pm.make_final_decision(
        symbol='AAPL',
        trader_opinion=trader_op,
        risk_opinion=risk_op,
        analyst_opinion=analyst_op,
        portfolio_state=portfolio
    )

    print(f"Final Decision: {result['final_decision']}")
    print(f"Confidence: {result['confidence']:.2f}")
    print(f"Recommended Action: {result['recommended_action']}")
    print(f"Hard Rules Passed: {result['hard_rules_passed']}")
    print(f"Warnings: {result.get('warnings', [])}")
