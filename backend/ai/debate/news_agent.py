"""
News Agent for War Room Debate

7ë²ˆì§¸ War Room ë©¤ë²„ë¡œ ê¸´ê¸‰ ë‰´ìŠ¤(Grounding API)ì™€ ì¼ë°˜ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬
ë§¤ë§¤ ì˜ê²¬ì„ ì œì‹œí•©ë‹ˆë‹¤.

Vote Weight: 10%
Data Sources:
- GroundingSearchLog (ê¸´ê¸‰ ë‰´ìŠ¤)
- NewsArticle (ì¼ë°˜ ë‰´ìŠ¤)
- Gemini 2.0 Flash (ê°ì„± ë¶„ì„)

Author: AI Trading System
Date: 2025-12-21
Updated: 2025-12-27 - Added regulatory and litigation news detection
"""

from datetime import datetime, timedelta, date
from typing import Dict, Any, List, Optional
import logging
import json
import anthropic
import os

from backend.database.models import NewsArticle, GroundingSearchLog
from backend.database.repository import (
    get_sync_session,
    MacroContextRepository,
    NewsInterpretationRepository
)
from backend.ai.gemini_client import call_gemini_api

# Use GLM instead of Claude for cost efficiency
try:
    from backend.ai.glm_client import GLMClient
    GLM_AVAILABLE = True
except ImportError:
    GLM_AVAILABLE = False
    logger.warning("GLM client not available, will use fallback")

logger = logging.getLogger(__name__)


class NewsAgent:
    """ë‰´ìŠ¤ ê¸°ë°˜ íˆ¬í‘œ Agent (War Room 7th member)"""

    def __init__(self):
        self.agent_name = "news"
        self.vote_weight = 0.10  # 10% íˆ¬í‘œê¶Œ
        self.model_name = "gemini-2.0-flash-exp"
        # Use GLM client instead of Claude for cost efficiency
        if GLM_AVAILABLE and os.getenv("GLM_API_KEY"):
            try:
                self.glm_client = GLMClient()
                self.use_glm = True
                logger.info("âœ… NewsAgent using GLM-4.7 for news interpretation")
            except Exception as e:
                logger.warning(f"GLM client init failed: {e}, falling back to Claude")
                self.use_glm = False
        else:
            self.use_glm = False
        self.enable_interpretation = os.getenv("ENABLE_NEWS_INTERPRETATION", "true").lower() == "true"
    
    async def interpret_articles(self, ticker: str, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ì œê³µëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ í•´ì„ (War Room ì—°ë™ìš©)
        
        Args:
            ticker: ì¢…ëª© ì½”ë“œ
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (dict)
            
        Returns:
            List[Dict]: í•´ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.enable_interpretation or not articles:
            return []

        interpretations = []
        try:
            # Sync session for reading macro context
            db = get_sync_session()
            macro_context = self._get_macro_context(db)
            db.close()
            
            for article in articles:
                # Use summary if available, else content
                content = article.get('summary') or article.get('content') or ''
                
                # Call internal interpretation logic
                result = await self._interpret_news(
                    ticker=ticker,
                    headline=article['title'],
                    content=content,
                    macro_context=macro_context
                )
                
                if result:
                    interpretations.append(result)
                    
        except Exception as e:
            logger.error(f"âŒ NewsAgent.interpret_articles failed: {e}")
            
        return interpretations

    async def analyze(self, ticker: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ë‰´ìŠ¤ ë¶„ì„ í›„ íˆ¬í‘œ ê²°ì •
        
        Args:
            ticker: ë¶„ì„í•  í‹°ì»¤ (ì˜ˆ: AAPL)
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒ)
        
        Returns:
            {
                "agent": "news",
                "action": "BUY/SELL/HOLD",
                "confidence": 0.0-1.0,
                "reasoning": "...",
                "news_count": int,
                "emergency_count": int,
                "sentiment_score": float
            }
        """
        db = get_sync_session()
        
        try:
            # 1. Emergency News ì¡°íšŒ (ìµœê·¼ 15ì¼)
            cutoff = datetime.now() - timedelta(days=15)
            
            # GroundingSearchLog fields: query (not search_query), no ticker column, search_date (not created_at)
            emergency_news = db.query(GroundingSearchLog)\
                .filter(
                    GroundingSearchLog.query.ilike(f"%{ticker}%"),  # Search in query text
                    GroundingSearchLog.search_date >= cutoff         # Use search_date
                )\
                .order_by(GroundingSearchLog.search_date.desc())\
                .limit(5)\
                .all()
            
            # 2. ì¼ë°˜ ë‰´ìŠ¤ ì¡°íšŒ (ìµœê·¼ 15ì¼) - Phase 20 real-time news
            # Priority: tickers field > title/content search
            recent_news = db.query(NewsArticle)\
                .filter(
                    NewsArticle.published_date >= cutoff
                )\
                .order_by(NewsArticle.published_date.desc())\
                .limit(200)\
                .all()

            # í‹°ì»¤ í•„í„°ë§ (ìš°ì„ ìˆœìœ„: tickers ë°°ì—´ > ì œëª©/ë‚´ìš©)
            ticker_news = []
            for n in recent_news:
                # Check tickers array first (from Phase 20)
                if n.tickers and ticker.upper() in [t.upper() for t in n.tickers]:
                    ticker_news.append(n)
                # Fallback: title/content search
                elif ticker.upper() in n.title.upper() or ticker.upper() in (n.content or '').upper():
                    ticker_news.append(n)

                if len(ticker_news) >= 30:
                    break

            recent_news = ticker_news
            
            # 3. ë‰´ìŠ¤ ìš”ì•½ ìƒì„±
            news_summaries = []
            
            for news in emergency_news:
                # GroundingSearchLog has: query, result_count, estimated_cost
                # No 'urgency' field by default
                news_summaries.append({
                    "type": "EMERGENCY",
                    "urgency": "HIGH",  # Default urgency
                    "content": news.query[:200] if news.query else f"Emergency search for {ticker}"
                })
            
            for article in recent_news:
                # Use Phase 20 sentiment_score if available
                sentiment = article.sentiment_score if hasattr(article, 'sentiment_score') and article.sentiment_score else 0.0

                # Add tags for context (from Phase 20 auto-tagging)
                tags_str = ', '.join(article.tags[:3]) if hasattr(article, 'tags') and article.tags else ''

                news_summaries.append({
                    "type": "REGULAR",
                    "title": article.title,
                    "sentiment": sentiment,  # Phase 20 sentiment
                    "tags": tags_str,        # Phase 20 tags
                    "source": article.source if hasattr(article, 'source') else 'Unknown'
                })
            
            # ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ì¤‘ë¦½ íˆ¬í‘œ
            if not news_summaries:
                logger.info(f"ğŸ“° News Agent: No news found for {ticker}")
                return {
                    "agent": "news",
                    "action": "HOLD",
                    "confidence": 0.5,
                    "reasoning": f"{ticker}ì— ëŒ€í•œ ìµœê·¼ 15ì¼ ë‰´ìŠ¤ ì—†ìŒ (ì¤‘ë¦½ ìœ ì§€)",
                    "news_count": 0,
                    "emergency_count": 0,
                    "sentiment_score": 0.0
                }
            
            # 4. [NEW] ë‰´ìŠ¤ í•´ì„ (Phase 2)
            if self.enable_interpretation and (emergency_news or recent_news):
                logger.info(f"ğŸ” News Agent: Interpreting important news for {ticker}")
                await self._interpret_and_save_news(ticker, emergency_news, recent_news, db)

            # 5. ê·œì œ/ì†Œì†¡ ë‰´ìŠ¤ ê°ì§€
            regulatory_analysis = self._detect_regulatory_litigation(news_summaries)
            
            # 6. [NEW] ì§€ì •í•™/ì¹©ì›Œ ìœ„ê¸° ê°ì§€
            critical_event = self._detect_critical_events(news_summaries)

            # 7. ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„
            trend_analysis = self._analyze_temporal_trend(news_summaries)

            # 8. Geminië¡œ ê°ì„± ë¶„ì„
            logger.info(f"ğŸ“° News Agent: Analyzing {len(news_summaries)} news for {ticker}")
            sentiment_result = await self._analyze_sentiment(
                ticker, 
                news_summaries, 
                trend_analysis, 
                regulatory_analysis
            )

            # 9. íˆ¬í‘œ ê²°ì •
            action, confidence = self._decide_action(
                sentiment_result,
                len(emergency_news),
                len(recent_news),
                trend_analysis,
                regulatory_analysis,
                critical_event  # [NEW] Pass critical event
            )
            
            # íŠ¸ë Œë“œ ì •ë³´ ì¶”ê°€
            trend_info = ""
            if trend_analysis:
                trend_emoji = "ğŸ“ˆ" if trend_analysis['trend'] == 'IMPROVING' else "ğŸ“‰" if trend_analysis['trend'] == 'DETERIORATING' else "â¡ï¸"
                risk_emoji = "âœ…" if trend_analysis['risk_trajectory'] == 'DECREASING' else "âš ï¸" if trend_analysis['risk_trajectory'] == 'INCREASING' else "â–"
                trend_info = f"""
- ë‰´ìŠ¤ íŠ¸ë Œë“œ: {trend_emoji} {trend_analysis['trend']} (ìµœê·¼ {trend_analysis['sentiment_change']:+.2f})
- ìœ„í—˜ë„ ë°©í–¥: {risk_emoji} {trend_analysis['risk_trajectory']}"""

            # ê·œì œ/ì†Œì†¡ ì •ë³´ ì¶”ê°€
            regulatory_info = ""
            if regulatory_analysis['has_risk']:
                reg_emoji = "âš–ï¸" if regulatory_analysis['litigation_count'] > 0 else "ğŸ“œ"
                regulatory_info = f"""
- {reg_emoji} ê·œì œ/ì†Œì†¡: {regulatory_analysis['severity']} ({regulatory_analysis['litigation_count']}ê±´ ì†Œì†¡, {regulatory_analysis['regulatory_count']}ê±´ ê·œì œ)"""

            reasoning = f"""
ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ ({len(emergency_news)}ê°œ ê¸´ê¸‰ + {len(recent_news)}ê°œ ì¼ë°˜):
- ê°ì„± ì ìˆ˜: {sentiment_result['score']:.2f}
- ê¸ì • ë‰´ìŠ¤: {sentiment_result['positive_count']}ê°œ
- ë¶€ì • ë‰´ìŠ¤: {sentiment_result['negative_count']}ê°œ{trend_info}{regulatory_info}
- ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(sentiment_result['keywords'][:5])}
"""
            
            logger.info(f"ğŸ“° News Agent: {action} (confidence: {confidence:.2f})")
            
            return {
                "agent": "news",
                "action": action,
                "confidence": confidence,
                "reasoning": reasoning.strip(),
                "news_count": len(recent_news),
                "emergency_count": len(emergency_news),
                "sentiment_score": sentiment_result['score']
            }
        
        except Exception as e:
            logger.error(f"âŒ News Agent error: {e}", exc_info=True)
            # ì—ëŸ¬ ë°œìƒ ì‹œ ì¤‘ë¦½ íˆ¬í‘œ
            return {
                "agent": "news",
                "action": "HOLD",
                "confidence": 0.5,
                "reasoning": f"ë‰´ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
                "news_count": 0,
                "emergency_count": 0,
                "sentiment_score": 0.0
            }
        
        finally:
            db.close()
    
    def _analyze_temporal_trend(self, news_summaries: List[Dict]) -> Dict[str, Any]:
        """
        ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„: ë‰´ìŠ¤ ê°ì„±ì´ ì‹œê°„ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ë¶„ì„

        Returns:
            {
                "trend": "IMPROVING|DETERIORATING|STABLE",
                "recent_sentiment": float,  # ìµœê·¼ 3ì¼ í‰ê· 
                "older_sentiment": float,   # 4-15ì¼ í‰ê· 
                "sentiment_change": float,  # ë³€í™”ëŸ‰
                "risk_trajectory": "INCREASING|DECREASING|NEUTRAL"
            }
        """
        from datetime import datetime, timedelta

        now = datetime.now()
        recent_cutoff = now - timedelta(days=3)

        recent_news = []
        older_news = []

        for news in news_summaries:
            if news['type'] == 'EMERGENCY':
                # ê¸´ê¸‰ ë‰´ìŠ¤ëŠ” ìµœê·¼ìœ¼ë¡œ ê°„ì£¼
                recent_news.append(news)
                continue

            # ì¼ë°˜ ë‰´ìŠ¤ëŠ” ë°œí–‰ì¼ í™•ì¸ í•„ìš” (news_summariesì— published_at ì¶”ê°€ í•„ìš”)
            # í˜„ì¬ëŠ” ìˆœì„œ ê¸°ë°˜ìœ¼ë¡œ ì ˆë°˜ ë‚˜ëˆ”
            if len(recent_news) < len(news_summaries) / 2:
                recent_news.append(news)
            else:
                older_news.append(news)

        # ê° ê¸°ê°„ë³„ í‰ê·  ê°ì„± ê³„ì‚°
        recent_sentiment = sum(n.get('sentiment', 0) for n in recent_news) / len(recent_news) if recent_news else 0
        older_sentiment = sum(n.get('sentiment', 0) for n in older_news) / len(older_news) if older_news else 0

        sentiment_change = recent_sentiment - older_sentiment

        # íŠ¸ë Œë“œ íŒì •
        if sentiment_change > 0.2:
            trend = "IMPROVING"
            risk_trajectory = "DECREASING"
        elif sentiment_change < -0.2:
            trend = "DETERIORATING"
            risk_trajectory = "INCREASING"
        else:
            trend = "STABLE"
            risk_trajectory = "NEUTRAL"

        return {
            "trend": trend,
            "recent_sentiment": recent_sentiment,
            "older_sentiment": older_sentiment,
            "sentiment_change": sentiment_change,
            "risk_trajectory": risk_trajectory,
            "recent_count": len(recent_news),
            "older_count": len(older_news)
        }

    def _detect_regulatory_litigation(self, news_summaries: List[Dict]) -> Dict[str, Any]:
        """
        ê·œì œ/ì†Œì†¡ ë‰´ìŠ¤ ê°ì§€
        
        í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì§€:
        - ì†Œì†¡: lawsuit, litigation, sued, settlement, class action
        - ê·œì œ: regulation, SEC, FTC, antitrust, investigation, probe
        
        Returns:
            {
                "has_risk": bool,
                "litigation_count": int,  # ì†Œì†¡ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜
                "regulatory_count": int,  # ê·œì œ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜
                "severity": "CRITICAL|HIGH|MODERATE|LOW",
                "keywords_found": List[str]
            }
        """
        # ì†Œì†¡ ê´€ë ¨ í‚¤ì›Œë“œ
        litigation_keywords = [
            'lawsuit', 'litigation', 'sued', 'settlement', 'class action',
            'ì†Œì†¡', 'ì§‘ë‹¨ì†Œì†¡', 'í•©ì˜ê¸ˆ', 'ë²•ì  ë¶„ìŸ', 'ì†Œì†¡ íŒ¨ì†Œ'
        ]
        
        # ê·œì œ ê´€ë ¨ í‚¤ì›Œë“œ
        regulatory_keywords = [
            'sec', 'ftc', 'doj', 'antitrust', 'investigation', 'probe',
            'fine', 'penalty', 'violation', 'compliance',
            'ê·œì œ', 'ì¡°ì‚¬', 'ì œì¬', 'ìœ„ë°˜', 'ë²Œê¸ˆ', 'ë‹¹êµ­', 'ê°ì‚¬'
        ]
        
        litigation_count = 0
        regulatory_count = 0
        keywords_found = []
        
        for news in news_summaries:
            content = ""
            if news['type'] == 'EMERGENCY':
                content = news.get('content', '').lower()
            else:
                content = news.get('title', '').lower()
                
            # ì†Œì†¡ í‚¤ì›Œë“œ ê²€ì‚¬
            for keyword in litigation_keywords:
                if keyword.lower() in content:
                    litigation_count += 1
                    if keyword not in keywords_found:
                        keywords_found.append(keyword)
                    break  # í•œ ë‰´ìŠ¤ë‹¹ í•œ ë²ˆë§Œ ì¹´ìš´íŠ¸
            
            # ê·œì œ í‚¤ì›Œë“œ ê²€ì‚¬
            for keyword in regulatory_keywords:
                if keyword.lower() in content:
                    regulatory_count += 1
                    if keyword not in keywords_found:
                        keywords_found.append(keyword)
                    break
                    
        # ì‹¬ê°ë„ íŒì •
        total_issues = litigation_count + regulatory_count
        
        if total_issues == 0:
            severity = "NONE"
            has_risk = False
        elif total_issues >= 5 or litigation_count >= 3:
            severity = "CRITICAL"
            has_risk = True
        elif total_issues >= 3 or litigation_count >= 2:
            severity = "HIGH"
            has_risk = True
        elif total_issues >= 2:
            severity = "MODERATE"
            has_risk = True
        else:
            severity = "LOW"
            has_risk = True
            
        return {
            "has_risk": has_risk,
            "litigation_count": litigation_count,
            "regulatory_count": regulatory_count,
            "severity": severity,
            "keywords_found": keywords_found[:5]  # ìµœëŒ€ 5ê°œë§Œ
        }

    def detect_critical_events(self, news_items: List[Dict]) -> Dict[str, Any]:
        """
        [New] ì§€ì •í•™ì  ìœ„ê¸° / ì¹© ì›Œ / ë§¤í¬ë¡œ ì¶©ê²© ê°ì§€ (The Watchtower Trigger)
        Public method for external agents (e.g. AnalystAgentMVP)
        
        Uses centralized keywords from `watchtower_triggers.py` to optimize AI costs.
        Only triggers Deep Reasoning for truly critical events.
        
        Args:
            news_items: List of dicts. Keys can be 'title', 'content', 'summary'.
        
        Returns:
            {
                "event_type": "GEOPOLITICS|CHIP_WAR|MACRO|REGULATORY|NONE",
                "detected": bool,
                "urgency": "CRITICAL|HIGH|MEDIUM|NONE",
                "keywords": List[str]
            }
        """
        try:
            from backend.ai.monitoring.watchtower_triggers import (
                GEOPOLITICAL_TRIGGERS,
                CHIP_WAR_TRIGGERS,
                MACRO_SHOCK_TRIGGERS,
                REGULATORY_TRIGGERS
            )
        except ImportError:
            logger.error("âŒ Failed to import Watchtower triggers, using fallback")
            return {"event_type": "NONE", "detected": False, "urgency": "NONE", "keywords": []}

        detected_events = []
        
        for news in news_items:
            # Construct consolidated text for searching
            text_monitor = []
            if 'title' in news: text_monitor.append(news['title'])
            if 'headline' in news: text_monitor.append(news['headline'])
            if 'content' in news: text_monitor.append(str(news['content']))
            if 'summary' in news: text_monitor.append(str(news['summary']))
            if 'query' in news: text_monitor.append(str(news['query'])) # For GroundingSearchLog
            
            full_text = " ".join(text_monitor).lower()
                
            # 1. Geopolitics (Highest Priority)
            geo_matches = [kw for kw in GEOPOLITICAL_TRIGGERS if kw in full_text]
            if geo_matches:
                detected_events.append({
                    "type": "GEOPOLITICS",
                    "urgency": "CRITICAL",
                    "keywords": geo_matches
                })
            
            # 2. Chip War
            chip_matches = [kw for kw in CHIP_WAR_TRIGGERS if kw in full_text]
            if chip_matches:
                detected_events.append({
                    "type": "CHIP_WAR",
                    "urgency": "HIGH",
                    "keywords": chip_matches
                })

            # 3. Macro Shock
            macro_matches = [kw for kw in MACRO_SHOCK_TRIGGERS if kw in full_text]
            if macro_matches:
                detected_events.append({
                    "type": "MACRO_SHOCK",
                    "urgency": "HIGH", 
                    "keywords": macro_matches
                })
                
        # Sort by urgency (CRITICAL > HIGH > MEDIUM)
        urgency_map = {"CRITICAL": 3, "HIGH": 2, "MEDIUM": 1, "NONE": 0}
        
        if detected_events:
            # Select the most urgent event
            top_event = max(detected_events, key=lambda x: urgency_map.get(x['urgency'], 0))
            
            return {
                "event_type": top_event['type'],
                "detected": True,
                "urgency": top_event['urgency'],
                "keywords": list(set(top_event['keywords']))[:5]
            }
            
        return {
            "event_type": "NONE",
            "detected": False,
            "urgency": "NONE",
            "keywords": []
        }

    async def _analyze_sentiment(self, ticker: str, news_summaries: List[Dict], trend_analysis: Dict = None, regulatory_analysis: Dict = None) -> Dict[str, Any]:
        """Geminië¡œ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ (ì‹œê³„ì—´ íŠ¸ë Œë“œ í¬í•¨)"""

        if not news_summaries:
            return {
                'score': 0.0,
                'positive_count': 0,
                'negative_count': 0,
                'keywords': [],
                'trend': None
            }

        trend_context = ""
        if trend_analysis:
            trend_context = f"""

ì‹œê³„ì—´ íŠ¸ë Œë“œ:
- ìµœê·¼ 3ì¼ ê°ì„±: {trend_analysis['recent_sentiment']:.2f}
- 4-15ì¼ ê°ì„±: {trend_analysis['older_sentiment']:.2f}
- ë³€í™” ì¶”ì„¸: {trend_analysis['trend']} ({trend_analysis['sentiment_change']:+.2f})
- ìœ„í—˜ë„ ë°©í–¥: {trend_analysis['risk_trajectory']}
"""

        regulatory_context = ""
        if regulatory_analysis and regulatory_analysis['has_risk']:
            regulatory_context = f"""

ê·œì œ/ì†Œì†¡ ì´ìŠˆ:
- ì‹¬ê°ë„: {regulatory_analysis['severity']}
- ì†Œì†¡ ê±´ìˆ˜: {regulatory_analysis['litigation_count']}
- ê·œì œ ê±´ìˆ˜: {regulatory_analysis['regulatory_count']}
- ë°œê²¬ í‚¤ì›Œë“œ: {', '.join(regulatory_analysis['keywords_found'])}

**ê²½ê³ **: ê·œì œ/ì†Œì†¡ ì´ìŠˆëŠ” ì£¼ê°€ì— ë¶€ì •ì  ì˜í–¥ì„ ì¤„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ê°ì„± ì ìˆ˜ì— ë°˜ì˜í•˜ì„¸ìš”.
"""

        prompt = f"""
ë‹¹ì‹ ì€ {ticker} ì£¼ì‹ì— ëŒ€í•œ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ë‰´ìŠ¤ë“¤ì„ ë¶„ì„í•˜ì—¬ ì¢…í•© ì ìˆ˜ë¥¼ ì‚°ì¶œí•˜ì„¸ìš”:

{self._format_news_for_prompt(news_summaries)}
{trend_context}{regulatory_context}

**ì¤‘ìš”**:
1. ì‹œê³„ì—´ íŠ¸ë Œë“œë¥¼ ê³ ë ¤í•˜ì—¬, ìµœê·¼ ë‰´ìŠ¤ê°€ ê³¼ê±° ëŒ€ë¹„ ê°œì„ ë˜ëŠ”ì§€ ì•…í™”ë˜ëŠ”ì§€ ë°˜ì˜í•˜ì„¸ìš”.
2. ê·œì œ/ì†Œì†¡ ì´ìŠˆëŠ” ì‹¬ê°ë„ì— ë”°ë¼ ê°ì„± ì ìˆ˜ë¥¼ -0.2 ~ -0.5 í•˜í–¥ ì¡°ì •í•˜ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš” (ì¶”ê°€ ì„¤ëª… ì—†ì´):
{{
  "score": -1.0 ~ 1.0 (ë¶€ì • ~ ê¸ì •),
  "positive_count": ê¸ì • ë‰´ìŠ¤ ê°œìˆ˜,
  "negative_count": ë¶€ì • ë‰´ìŠ¤ ê°œìˆ˜,
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
}}
"""
        
        try:
            # Gemini API í˜¸ì¶œ
            response_text = await call_gemini_api(
                prompt=prompt,
                model_name=self.model_name,
                temperature=0.3
            )
            
            # JSON íŒŒì‹±
            # response_textê°€ "```json\n...\n```" í˜•ì‹ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì •ë¦¬
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            result = json.loads(response_text)
            
            return {
                'score': float(result.get('score', 0.0)),
                'positive_count': int(result.get('positive_count', 0)),
                'negative_count': int(result.get('negative_count', 0)),
                'keywords': result.get('keywords', [])
            }
        
        except Exception as e:
            logger.error(f"âŒ Sentiment analysis failed: {e}")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½ ë°˜í™˜
            return {
                'score': 0.0,
                'positive_count': 0,
                'negative_count': 0,
                'keywords': []
            }
    
    def _format_news_for_prompt(self, news_summaries: List[Dict]) -> str:
        """ë‰´ìŠ¤ë¥¼ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (Phase 20 enhanced)"""
        lines = []
        for i, news in enumerate(news_summaries, 1):
            if news['type'] == 'EMERGENCY':
                lines.append(f"{i}. [ê¸´ê¸‰ {news['urgency']}] {news['content']}")
            else:
                # Include sentiment and tags from Phase 20
                sentiment_emoji = "ğŸ“ˆ" if news.get('sentiment', 0) > 0.3 else "ğŸ“‰" if news.get('sentiment', 0) < -0.3 else "â–"
                tags_info = f" [{news.get('tags', '')}]" if news.get('tags') else ""
                source_info = f" ({news.get('source', 'Unknown')})"

                lines.append(f"{i}. {sentiment_emoji} {news['title']}{tags_info}{source_info}")

        return "\n".join(lines)
    
    def _decide_action(
        self,
        sentiment_result: Dict[str, Any],
        emergency_count: int,
        news_count: int,
        trend_analysis: Dict = None,
        regulatory_analysis: Dict = None,
        critical_event: Dict = None
    ) -> tuple[str, float]:
        """ê°ì„± ì ìˆ˜ â†’ ë§¤ë§¤ ê²°ì • (ì‹œê³„ì—´ íŠ¸ë Œë“œ, ê·œì œ, ì§€ì •í•™ ìœ„ê¸° ë°˜ì˜)"""

        score = sentiment_result['score']

        # ê¸´ê¸‰ ë‰´ìŠ¤ê°€ ìˆìœ¼ë©´ confidence ë†’ì„
        urgency_boost = 0.2 if emergency_count > 0 else 0

        # ì‹œê³„ì—´ íŠ¸ë Œë“œ ë°˜ì˜
        trend_boost = 0
        if trend_analysis:
            if trend_analysis['trend'] == 'IMPROVING':
                trend_boost = 0.1
            elif trend_analysis['trend'] == 'DETERIORATING':
                trend_boost = -0.1

        # ê·œì œ/ì†Œì†¡ ë¦¬ìŠ¤í¬ ë°˜ì˜
        regulatory_penalty = 0
        force_sell = False
        if regulatory_analysis and regulatory_analysis['has_risk']:
            if regulatory_analysis['severity'] == 'CRITICAL':
                regulatory_penalty = -0.5
                force_sell = True
            elif regulatory_analysis['severity'] == 'HIGH':
                regulatory_penalty = -0.3
            elif regulatory_analysis['severity'] == 'MODERATE':
                regulatory_penalty = -0.2
            else:
                regulatory_penalty = -0.1
                
        # [NEW] ì§€ì •í•™/ì¹©ì›Œ ìœ„ê¸° ë°˜ì˜ (SUPER PRIORITY)
        geo_penalty = 0
        if critical_event and critical_event['detected']:
            if critical_event['urgency'] == 'CRITICAL':
                # ì „ìŸ/ì¹¨ê³µ ë“±ì€ ë¬´ì¡°ê±´ ë§¤ë„ ë° ìµœëŒ€ ì‹ ë¢°ë„
                force_sell = True
                geo_penalty = -1.0 # ê°•ë ¥í•œ í•˜ë°© ì••ë ¥
                logger.warning(f"ğŸš¨ CRITICAL GEOPOLITICAL EVENT: {critical_event['keywords']}")
            elif critical_event['urgency'] == 'HIGH':
                geo_penalty = -0.5
                
        adjusted_score = score + trend_boost + regulatory_penalty + geo_penalty

        # ê°•ì œ ë§¤ë„ ì¡°ê±´ (ê·œì œ Critical or ì§€ì •í•™ Critical)
        if force_sell:
            action = "SELL"
            confidence = 0.95  # ë§¤ìš° ë†’ì€ í™•ì‹ 
        elif adjusted_score > 0.6:
            action = "BUY"
            confidence = min(0.95, abs(adjusted_score) + urgency_boost)
        elif adjusted_score < -0.6:
            action = "SELL"
            confidence = min(0.95, abs(adjusted_score) + urgency_boost)
        else:
            action = "HOLD"
            confidence = 0.5 + abs(adjusted_score) * 0.3

        return action, confidence

    # ====================================
    # Phase 2: News Interpretation Methods
    # ====================================

    async def _interpret_and_save_news(
        self,
        ticker: str,
        emergency_news: List,
        recent_news: List[NewsArticle],
        db_session
    ):
        """
        ì¤‘ìš” ë‰´ìŠ¤ë¥¼ ì„ íƒí•˜ì—¬ Claude APIë¡œ í•´ì„í•˜ê³  DBì— ì €ì¥

        Args:
            ticker: ì¢…ëª© ì½”ë“œ
            emergency_news: ê¸´ê¸‰ ë‰´ìŠ¤ ëª©ë¡
            recent_news: ì¼ë°˜ ë‰´ìŠ¤ ëª©ë¡
            db_session: DB ì„¸ì…˜
        """
        # 1. Macro context ì¡°íšŒ
        macro_context = self._get_macro_context(db_session)

        # 2. ì¤‘ìš” ë‰´ìŠ¤ ì„ íƒ (ìµœëŒ€ 5ê°œ)
        important_news = self._select_important_news(emergency_news, recent_news, limit=5)

        if not important_news:
            logger.info(f"ğŸ” News Agent: No important news to interpret for {ticker}")
            return

        # 3. ê° ë‰´ìŠ¤ í•´ì„ + DB ì €ì¥
        interpretation_repo = NewsInterpretationRepository(db_session)

        for news_item in important_news:
            try:
                # NewsArticleì¸ ê²½ìš°
                if isinstance(news_item, NewsArticle):
                    news_id = news_item.id
                    headline = news_item.title
                    content = news_item.content or ""
                else:
                    # GroundingSearchLogì¸ ê²½ìš° (ê¸´ê¸‰ ë‰´ìŠ¤)
                    news_id = None  # GroundingSearchLogëŠ” news_articles í…Œì´ë¸”ì— ì—†ìŒ
                    headline = news_item.query
                    content = ""

                # ì´ë¯¸ í•´ì„ëœ ë‰´ìŠ¤ëŠ” skip
                if news_id:
                    existing = interpretation_repo.get_by_news_article(news_id)
                    if existing:
                        logger.info(f"ğŸ” News Agent: Already interpreted news_id={news_id}, skipping")
                        continue

                # Claude APIë¡œ í•´ì„
                interpretation = await self._interpret_news(
                    ticker=ticker,
                    headline=headline,
                    content=content,
                    macro_context=macro_context
                )

                # DB ì €ì¥ (news_idê°€ ìˆëŠ” ê²½ìš°ë§Œ)
                if news_id and interpretation:
                    interpretation_data = {
                        "news_article_id": news_id,
                        "ticker": ticker,
                        "headline_bias": interpretation["headline_bias"],
                        "expected_impact": interpretation["expected_impact"],
                        "time_horizon": interpretation["time_horizon"],
                        "confidence": interpretation["confidence"],
                        "reasoning": interpretation["reasoning"],
                        "macro_context_id": macro_context["id"] if macro_context else None,
                        "interpreted_at": datetime.now()
                    }

                    saved = interpretation_repo.create(interpretation_data)
                    logger.info(f"âœ… News Agent: Saved interpretation id={saved.id} for news_id={news_id}")

            except Exception as e:
                logger.error(f"âŒ News Agent: Failed to interpret news: {e}", exc_info=True)
                continue

    def _get_macro_context(self, db_session) -> Optional[Dict]:
        """
        ì˜¤ëŠ˜ì˜ macro context ì¡°íšŒ

        Returns:
            Dict: macro context ë˜ëŠ” None
        """
        try:
            macro_repo = MacroContextRepository(db_session)
            snapshot = macro_repo.get_by_date(date.today())

            if snapshot:
                return {
                    "id": snapshot.id,
                    "regime": snapshot.regime,
                    "fed_stance": snapshot.fed_stance,
                    "vix_category": snapshot.vix_category,
                    "market_sentiment": snapshot.market_sentiment,
                    "sp500_trend": snapshot.sp500_trend,
                    "dominant_narrative": snapshot.dominant_narrative
                }
            else:
                logger.warning(f"âš ï¸ No macro context for {date.today()}, using fallback")
                return None

        except Exception as e:
            logger.error(f"âŒ Failed to get macro context: {e}")
            return None

    def _select_important_news(
        self,
        emergency_news: List,
        recent_news: List[NewsArticle],
        limit: int = 5
    ) -> List:
        """
        ì¤‘ìš” ë‰´ìŠ¤ ì„ íƒ

        ìš°ì„ ìˆœìœ„:
        1. ê¸´ê¸‰ ë‰´ìŠ¤ (ëª¨ë‘)
        2. sentiment_scoreê°€ ë†’ê±°ë‚˜ ë‚®ì€ ë‰´ìŠ¤
        3. ìµœì‹  ë‰´ìŠ¤

        Args:
            emergency_news: ê¸´ê¸‰ ë‰´ìŠ¤ ëª©ë¡
            recent_news: ì¼ë°˜ ë‰´ìŠ¤ ëª©ë¡
            limit: ìµœëŒ€ ê°œìˆ˜

        Returns:
            List: ì„ íƒëœ ë‰´ìŠ¤ ëª©ë¡
        """
        selected = []

        # 1. ê¸´ê¸‰ ë‰´ìŠ¤ ìš°ì„ 
        for news in emergency_news:
            if len(selected) >= limit:
                break
            selected.append(news)

        # 2. ì¼ë°˜ ë‰´ìŠ¤ ì¤‘ sentiment ê·¹ë‹¨ê°’ ìš°ì„ 
        if len(selected) < limit:
            # sentiment_score ê¸°ì¤€ ì •ë ¬
            sorted_news = sorted(
                recent_news,
                key=lambda x: abs(x.sentiment_score) if hasattr(x, 'sentiment_score') and x.sentiment_score else 0,
                reverse=True
            )

            for news in sorted_news:
                if len(selected) >= limit:
                    break
                selected.append(news)

        return selected[:limit]

    async def _interpret_news(
        self,
        ticker: str,
        headline: str,
        content: str,
        macro_context: Optional[Dict]
    ) -> Optional[Dict]:
        """
        Claude APIë¡œ ë‰´ìŠ¤ í•´ì„

        Args:
            ticker: ì¢…ëª© ì½”ë“œ
            headline: ë‰´ìŠ¤ í—¤ë“œë¼ì¸
            content: ë‰´ìŠ¤ ë³¸ë¬¸
            macro_context: ê±°ì‹œ ê²½ì œ ì»¨í…ìŠ¤íŠ¸

        Returns:
            Dict: {
                "headline_bias": "BULLISH|BEARISH|NEUTRAL",
                "expected_impact": "HIGH|MEDIUM|LOW",
                "time_horizon": "IMMEDIATE|INTRADAY|MULTI_DAY",
                "confidence": 0.0-1.0,
                "reasoning": "í•´ì„ ê·¼ê±°"
            }
        """
        macro_info = ""
        if macro_context:
            macro_info = f"""
í˜„ì¬ ê±°ì‹œ ê²½ì œ ìƒí™©:
- ì‹œì¥ ì²´ì œ: {macro_context['regime']}
- Fed ìŠ¤íƒ ìŠ¤: {macro_context['fed_stance']}
- VIX: {macro_context['vix_category']}
- ì‹œì¥ ì„¼í‹°ë¨¼íŠ¸: {macro_context['market_sentiment']}
- S&P 500 íŠ¸ë Œë“œ: {macro_context['sp500_trend']}
- ì§€ë°°ì  ì„œì‚¬: {macro_context['dominant_narrative']}
"""

        prompt = f"""
ë‹¹ì‹ ì€ {ticker} ì£¼ì‹ì— ëŒ€í•œ ë‰´ìŠ¤ í•´ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ íˆ¬ì ê´€ì ì—ì„œ í•´ì„í•˜ì„¸ìš”:

**ë‰´ìŠ¤ í—¤ë“œë¼ì¸**: {headline}

**ë‰´ìŠ¤ ë‚´ìš©**: {content[:500] if content else "ë‚´ìš© ì—†ìŒ"}
{macro_info}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš” (ì¶”ê°€ ì„¤ëª… ì—†ì´):
{{
  "headline_bias": "BULLISH|BEARISH|NEUTRAL",
  "expected_impact": "HIGH|MEDIUM|LOW",
  "time_horizon": "IMMEDIATE|INTRADAY|MULTI_DAY",
  "confidence": 0.0-1.0,
  "reasoning": "ì´ ë‰´ìŠ¤ê°€ {ticker} ì£¼ê°€ì— ë¯¸ì¹  ì˜í–¥ì— ëŒ€í•œ ê°„ê²°í•œ í•´ì„ (100ì ì´ë‚´)"
}}

**ì£¼ì˜ì‚¬í•­**:
- headline_bias: ë‰´ìŠ¤ê°€ ì£¼ê°€ì— ê¸ì •ì (BULLISH), ë¶€ì •ì (BEARISH), ì¤‘ë¦½ì (NEUTRAL)ì¸ì§€
- expected_impact: ì£¼ê°€ ë³€ë™ ì˜ˆìƒ í¬ê¸° (HIGH: 5%+, MEDIUM: 2-5%, LOW: <2%)
- time_horizon: ì˜í–¥ ì‹œì  (IMMEDIATE: 10ë¶„ë‚´, INTRADAY: ë‹¹ì¼, MULTI_DAY: ë©°ì¹ )
- confidence: ì´ í•´ì„ì— ëŒ€í•œ í™•ì‹ ë„ (0.0-1.0)
- ê±°ì‹œ ê²½ì œ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ í•´ì„í•˜ì„¸ìš”
"""

        try:
            # Use GLM for cost efficiency
            response = await self.glm_client.chat(
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=300,
                temperature=0.3
            )
            response_text = response["choices"][0]["message"]["content"].strip()

            # JSON íŒŒì‹±
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()

            interpretation = json.loads(response_text)

            # ê²€ì¦
            required_fields = ["headline_bias", "expected_impact", "time_horizon", "confidence", "reasoning"]
            for field in required_fields:
                if field not in interpretation:
                    raise ValueError(f"Missing field: {field}")

            logger.info(f"âœ… News Agent: Interpreted news - {interpretation['headline_bias']} / {interpretation['expected_impact']}")
            return interpretation

        except Exception as e:
            logger.error(f"âŒ News Agent: Claude interpretation failed: {e}", exc_info=True)
            return None
