"""
Strategy Refiner - ì „ëµ ìê¸° ê°œì„  ì‹œìŠ¤í…œ

Phase F4: ììœ¨ ì§„í™” ì‹œìŠ¤í…œ

AIê°€ ìŠ¤ìŠ¤ë¡œ ì„±ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì „ëµ ê°œì„ ì•ˆì„ ìƒì„±

í•µì‹¬ ê¸°ëŠ¥:
- ì¼ì¼/ì£¼ê°„ ì„±ê³¼ ë¶„ì„
- "ë°˜ì„±ë¬¸ ë° ê°œì„ ì•ˆ" ìƒì„±
- Config/Prompt ìˆ˜ì • ì œì•ˆ
- ìë™ í•™ìŠµ ë° ìµœì í™”

ì‘ì„±ì¼: 2025-12-08
ì°¸ì¡°: 10_Ideas_Integration_Plan_v3.md
"""

from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime, date, timedelta
from enum import Enum
import json
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìŠ¤í‚¤ë§ˆ ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ReviewPeriod(str, Enum):
    """ë¦¬ë·° ê¸°ê°„"""
    DAILY = "daily"
    WEEKLY = "weekly"
    MONTHLY = "monthly"


class ImprovementType(str, Enum):
    """ê°œì„  ìœ í˜•"""
    WEIGHT_ADJUSTMENT = "weight_adjustment"
    PARAMETER_TUNING = "parameter_tuning"
    PROMPT_MODIFICATION = "prompt_modification"
    STRATEGY_CHANGE = "strategy_change"
    RISK_RULE = "risk_rule"


class Priority(str, Enum):
    """ìš°ì„ ìˆœìœ„"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class TradeRecord:
    """ê±°ë˜ ê¸°ë¡"""
    trade_id: str
    ticker: str
    action: str  # BUY, SELL, HOLD
    entry_price: float
    exit_price: Optional[float]
    pnl: float  # ì†ìµ (%)
    ai_votes: Dict[str, str]  # {"claude": "BUY", "chatgpt": "HOLD", "gemini": "BUY"}
    ai_confidences: Dict[str, float]
    timestamp: datetime
    holding_period_days: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "trade_id": self.trade_id,
            "ticker": self.ticker,
            "action": self.action,
            "entry_price": self.entry_price,
            "exit_price": self.exit_price,
            "pnl": self.pnl,
            "ai_votes": self.ai_votes,
            "ai_confidences": self.ai_confidences,
            "timestamp": self.timestamp.isoformat(),
            "holding_period_days": self.holding_period_days
        }


@dataclass
class PerformanceSnapshot:
    """ì„±ê³¼ ìŠ¤ëƒ…ìƒ·"""
    period: ReviewPeriod
    start_date: date
    end_date: date
    
    # ì „ì²´ ì„±ê³¼
    total_trades: int
    win_count: int
    loss_count: int
    win_rate: float
    total_return: float  # %
    avg_return: float
    max_drawdown: float
    sharpe_ratio: Optional[float] = None
    
    # AIë³„ ì„±ê³¼
    agent_performance: Dict[str, Dict[str, float]] = field(default_factory=dict)
    
    # íŒ¨í„´ ë¶„ì„
    best_performing_tickers: List[str] = field(default_factory=list)
    worst_performing_tickers: List[str] = field(default_factory=list)
    common_mistakes: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "period": self.period.value,
            "date_range": f"{self.start_date} ~ {self.end_date}",
            "overall": {
                "total_trades": self.total_trades,
                "win_rate": self.win_rate,
                "total_return": self.total_return,
                "avg_return": self.avg_return,
                "max_drawdown": self.max_drawdown,
                "sharpe_ratio": self.sharpe_ratio
            },
            "agents": self.agent_performance,
            "analysis": {
                "best_tickers": self.best_performing_tickers,
                "worst_tickers": self.worst_performing_tickers,
                "mistakes": self.common_mistakes
            }
        }


@dataclass
class ImprovementSuggestion:
    """ê°œì„  ì œì•ˆ"""
    id: str
    improvement_type: ImprovementType
    priority: Priority
    title: str
    description: str
    rationale: str
    expected_impact: str
    implementation: Dict[str, Any]
    created_at: datetime = field(default_factory=datetime.now)
    applied: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "type": self.improvement_type.value,
            "priority": self.priority.value,
            "title": self.title,
            "description": self.description,
            "rationale": self.rationale,
            "expected_impact": self.expected_impact,
            "implementation": self.implementation,
            "applied": self.applied
        }


@dataclass
class ReflectionReport:
    """ë°˜ì„±ë¬¸ ë° ê°œì„ ì•ˆ ë³´ê³ ì„œ"""
    period: ReviewPeriod
    generated_at: datetime
    performance: PerformanceSnapshot
    
    # ë¶„ì„
    key_findings: List[str]
    lessons_learned: List[str]
    
    # ê°œì„ ì•ˆ
    suggestions: List[ImprovementSuggestion]
    
    # ìš”ì•½
    summary: str
    confidence: float
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "period": self.period.value,
            "generated_at": self.generated_at.isoformat(),
            "performance": self.performance.to_dict(),
            "findings": self.key_findings,
            "lessons": self.lessons_learned,
            "suggestions": [s.to_dict() for s in self.suggestions],
            "summary": self.summary,
            "confidence": self.confidence
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Strategy Refiner í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class StrategyRefiner:
    """
    ì „ëµ ìê¸° ê°œì„  ì‹œìŠ¤í…œ
    
    Usage:
        refiner = StrategyRefiner()
        
        # ê±°ë˜ ê¸°ë¡ ì¶”ê°€
        refiner.add_trade(trade_record)
        
        # ì¼ì¼ ë¦¬ë·°
        daily_report = refiner.generate_daily_review()
        
        # ì£¼ê°„ ë¦¬ë·°
        weekly_report = refiner.generate_weekly_review()
        
        # ê°œì„ ì•ˆ ì ìš©
        for suggestion in weekly_report.suggestions:
            if suggestion.priority == Priority.CRITICAL:
                refiner.apply_suggestion(suggestion)
    """
    
    # ì„±ê³¼ ê¸°ì¤€ê°’
    TARGET_WIN_RATE = 0.55
    TARGET_AVG_RETURN = 0.02  # 2%
    MAX_ACCEPTABLE_DD = 0.15  # 15%
    
    def __init__(self, data_dir: Optional[Path] = None):
        """ì´ˆê¸°í™”"""
        self.data_dir = data_dir or Path("data/evolution")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self._trades: List[TradeRecord] = []
        self._reports: List[ReflectionReport] = []
        self._applied_suggestions: List[ImprovementSuggestion] = []
        
        logger.info("StrategyRefiner initialized")
    
    def add_trade(self, trade: TradeRecord):
        """ê±°ë˜ ê¸°ë¡ ì¶”ê°€"""
        self._trades.append(trade)
        logger.debug(f"Added trade: {trade.ticker} {trade.action} PnL: {trade.pnl:.2%}")
    
    def add_trades(self, trades: List[TradeRecord]):
        """ì—¬ëŸ¬ ê±°ë˜ ê¸°ë¡ ì¶”ê°€"""
        self._trades.extend(trades)
    
    def get_trades(
        self,
        start_date: Optional[date] = None,
        end_date: Optional[date] = None
    ) -> List[TradeRecord]:
        """ê¸°ê°„ë³„ ê±°ë˜ ì¡°íšŒ"""
        trades = self._trades
        
        if start_date:
            trades = [t for t in trades if t.timestamp.date() >= start_date]
        if end_date:
            trades = [t for t in trades if t.timestamp.date() <= end_date]
        
        return trades
    
    def calculate_performance(
        self,
        trades: List[TradeRecord],
        period: ReviewPeriod
    ) -> PerformanceSnapshot:
        """ì„±ê³¼ ê³„ì‚°"""
        if not trades:
            return PerformanceSnapshot(
                period=period,
                start_date=date.today(),
                end_date=date.today(),
                total_trades=0,
                win_count=0,
                loss_count=0,
                win_rate=0.0,
                total_return=0.0,
                avg_return=0.0,
                max_drawdown=0.0
            )
        
        # ê¸°ë³¸ í†µê³„
        win_trades = [t for t in trades if t.pnl > 0]
        loss_trades = [t for t in trades if t.pnl <= 0]
        
        total_trades = len(trades)
        win_count = len(win_trades)
        loss_count = len(loss_trades)
        win_rate = win_count / total_trades if total_trades > 0 else 0
        
        returns = [t.pnl for t in trades]
        total_return = sum(returns)
        avg_return = total_return / total_trades if total_trades > 0 else 0
        
        # Max Drawdown ê³„ì‚° (ê°„ì†Œí™”)
        cumulative = 0
        peak = 0
        max_dd = 0
        for r in returns:
            cumulative += r
            if cumulative > peak:
                peak = cumulative
            dd = (peak - cumulative) if peak > 0 else 0
            if dd > max_dd:
                max_dd = dd
        
        # AIë³„ ì„±ê³¼
        agent_perf = self._calculate_agent_performance(trades)
        
        # í‹°ì»¤ë³„ ë¶„ì„
        ticker_returns = {}
        for t in trades:
            if t.ticker not in ticker_returns:
                ticker_returns[t.ticker] = []
            ticker_returns[t.ticker].append(t.pnl)
        
        ticker_avg = {
            ticker: sum(rets) / len(rets)
            for ticker, rets in ticker_returns.items()
        }
        sorted_tickers = sorted(ticker_avg.items(), key=lambda x: x[1], reverse=True)
        
        # ê³µí†µ ì‹¤ìˆ˜ ë¶„ì„
        mistakes = self._analyze_common_mistakes(trades)
        
        return PerformanceSnapshot(
            period=period,
            start_date=min(t.timestamp.date() for t in trades),
            end_date=max(t.timestamp.date() for t in trades),
            total_trades=total_trades,
            win_count=win_count,
            loss_count=loss_count,
            win_rate=win_rate,
            total_return=total_return,
            avg_return=avg_return,
            max_drawdown=max_dd,
            agent_performance=agent_perf,
            best_performing_tickers=[t[0] for t in sorted_tickers[:3]],
            worst_performing_tickers=[t[0] for t in sorted_tickers[-3:]],
            common_mistakes=mistakes
        )
    
    def _calculate_agent_performance(
        self,
        trades: List[TradeRecord]
    ) -> Dict[str, Dict[str, float]]:
        """AI ì—ì´ì „íŠ¸ë³„ ì„±ê³¼ ê³„ì‚°"""
        agents = ["claude", "chatgpt", "gemini"]
        result = {}
        
        for agent in agents:
            correct_calls = 0
            total_calls = 0
            confidence_sum = 0
            
            for trade in trades:
                if agent not in trade.ai_votes:
                    continue
                
                total_calls += 1
                vote = trade.ai_votes[agent]
                confidence = trade.ai_confidences.get(agent, 0.5)
                confidence_sum += confidence
                
                # ì˜¬ë°”ë¥¸ ì½œì¸ì§€ í™•ì¸
                if trade.pnl > 0:
                    if vote in ["BUY", "INCREASE"] and trade.action == "BUY":
                        correct_calls += 1
                    elif vote in ["SELL", "REDUCE"] and trade.action == "SELL":
                        correct_calls += 1
                elif trade.pnl < 0:
                    if vote in ["SELL", "REDUCE", "HOLD"] and trade.action == "BUY":
                        correct_calls += 1  # ë§¤ìˆ˜ ë°˜ëŒ€ê°€ ë§ì•˜ìŒ
            
            accuracy = correct_calls / total_calls if total_calls > 0 else 0
            avg_confidence = confidence_sum / total_calls if total_calls > 0 else 0
            
            result[agent] = {
                "accuracy": accuracy,
                "total_calls": total_calls,
                "avg_confidence": avg_confidence
            }
        
        return result
    
    def _analyze_common_mistakes(self, trades: List[TradeRecord]) -> List[str]:
        """ê³µí†µ ì‹¤ìˆ˜ ë¶„ì„"""
        mistakes = []
        
        loss_trades = [t for t in trades if t.pnl < 0]
        if not loss_trades:
            return ["ì†ì‹¤ ê±°ë˜ ì—†ìŒ"]
        
        # ë†’ì€ ì‹ ë¢°ë„ ì†ì‹¤
        high_conf_losses = [
            t for t in loss_trades
            if max(t.ai_confidences.values(), default=0) > 0.8
        ]
        if len(high_conf_losses) >= 3:
            mistakes.append(f"ë†’ì€ ì‹ ë¢°ë„ ì†ì‹¤ {len(high_conf_losses)}ê±´: ê³¼ì‹  ê²½í–¥")
        
        # ì§§ì€ ë³´ìœ ê¸°ê°„ ì†ì‹¤
        short_hold_losses = [
            t for t in loss_trades
            if t.holding_period_days < 3
        ]
        if len(short_hold_losses) >= 3:
            mistakes.append(f"ë‹¨ê¸° ì†ì ˆ {len(short_hold_losses)}ê±´: ì¡°ê¸‰í•œ ì²­ì‚°")
        
        # ë§Œì¥ì¼ì¹˜ ì†ì‹¤
        unanimous_losses = [
            t for t in loss_trades
            if len(set(t.ai_votes.values())) == 1
        ]
        if len(unanimous_losses) >= 2:
            mistakes.append(f"ë§Œì¥ì¼ì¹˜ ì†ì‹¤ {len(unanimous_losses)}ê±´: ê·¸ë£¹ì”½í¬ ìœ„í—˜")
        
        return mistakes if mistakes else ["íŠ¹ë³„í•œ íŒ¨í„´ ì—†ìŒ"]
    
    def generate_review(
        self,
        period: ReviewPeriod = ReviewPeriod.DAILY
    ) -> ReflectionReport:
        """ë¦¬ë·° ë³´ê³ ì„œ ìƒì„±"""
        # ê¸°ê°„ ì„¤ì •
        end_date = date.today()
        if period == ReviewPeriod.DAILY:
            start_date = end_date - timedelta(days=1)
        elif period == ReviewPeriod.WEEKLY:
            start_date = end_date - timedelta(days=7)
        else:
            start_date = end_date - timedelta(days=30)
        
        # ê±°ë˜ ì¡°íšŒ
        trades = self.get_trades(start_date, end_date)
        
        # ì„±ê³¼ ê³„ì‚°
        performance = self.calculate_performance(trades, period)
        
        # í•µì‹¬ ë°œê²¬ì‚¬í•­
        findings = self._generate_findings(performance)
        
        # êµí›ˆ
        lessons = self._generate_lessons(performance)
        
        # ê°œì„  ì œì•ˆ
        suggestions = self._generate_suggestions(performance)
        
        # ìš”ì•½
        summary = self._generate_summary(performance, suggestions)
        
        report = ReflectionReport(
            period=period,
            generated_at=datetime.now(),
            performance=performance,
            key_findings=findings,
            lessons_learned=lessons,
            suggestions=suggestions,
            summary=summary,
            confidence=0.8
        )
        
        self._reports.append(report)
        
        # ì €ì¥
        self._save_report(report)
        
        return report
    
    def generate_daily_review(self) -> ReflectionReport:
        """ì¼ì¼ ë¦¬ë·°"""
        return self.generate_review(ReviewPeriod.DAILY)
    
    def generate_weekly_review(self) -> ReflectionReport:
        """ì£¼ê°„ ë¦¬ë·°"""
        return self.generate_review(ReviewPeriod.WEEKLY)
    
    def _generate_findings(self, perf: PerformanceSnapshot) -> List[str]:
        """í•µì‹¬ ë°œê²¬ì‚¬í•­ ìƒì„±"""
        findings = []
        
        if perf.total_trades == 0:
            return ["ê±°ë˜ ì—†ìŒ"]
        
        # ìŠ¹ë¥  ë¶„ì„
        if perf.win_rate >= 0.6:
            findings.append(f"ë†’ì€ ìŠ¹ë¥  {perf.win_rate:.1%}: ì „ëµ íš¨ê³¼ì ")
        elif perf.win_rate < 0.4:
            findings.append(f"ë‚®ì€ ìŠ¹ë¥  {perf.win_rate:.1%}: ì „ëµ ì¬ê²€í†  í•„ìš”")
        
        # ìˆ˜ìµë¥  ë¶„ì„
        if perf.avg_return > 0.03:
            findings.append(f"ìš°ìˆ˜í•œ í‰ê·  ìˆ˜ìµë¥  {perf.avg_return:.2%}")
        elif perf.avg_return < -0.02:
            findings.append(f"ì†ì‹¤ {perf.avg_return:.2%}: ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê°•í™” í•„ìš”")
        
        # Drawdown ë¶„ì„
        if perf.max_drawdown > self.MAX_ACCEPTABLE_DD:
            findings.append(f"ê³¼ë„í•œ ë“œë¡œë‹¤ìš´ {perf.max_drawdown:.2%}")
        
        # AI ì„±ê³¼ ë¶„ì„
        for agent, data in perf.agent_performance.items():
            if data.get("accuracy", 0) < 0.4:
                findings.append(f"{agent} ì •í™•ë„ ë‚®ìŒ: {data['accuracy']:.1%}")
        
        return findings if findings else ["íŠ¹ë³„ ì‚¬í•­ ì—†ìŒ"]
    
    def _generate_lessons(self, perf: PerformanceSnapshot) -> List[str]:
        """êµí›ˆ ìƒì„±"""
        lessons = []
        
        if perf.common_mistakes:
            for mistake in perf.common_mistakes[:3]:
                lessons.append(f"êµí›ˆ: {mistake} â†’ ì£¼ì˜ í•„ìš”")
        
        if perf.worst_performing_tickers:
            lessons.append(f"í”¼í•´ì•¼ í•  ì¢…ëª© íŒ¨í„´: {', '.join(perf.worst_performing_tickers[:2])}")
        
        if perf.best_performing_tickers:
            lessons.append(f"ê°•ì : {', '.join(perf.best_performing_tickers[:2])} ë¶„ì„ ìš°ìˆ˜")
        
        return lessons if lessons else ["íŠ¹ë³„í•œ êµí›ˆ ì—†ìŒ"]
    
    def _generate_suggestions(
        self,
        perf: PerformanceSnapshot
    ) -> List[ImprovementSuggestion]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        suggestions = []
        suggestion_id = 0
        
        # 1. ê°€ì¤‘ì¹˜ ì¡°ì • ì œì•ˆ
        for agent, data in perf.agent_performance.items():
            accuracy = data.get("accuracy", 0.5)
            
            if accuracy < 0.4:
                suggestion_id += 1
                suggestions.append(ImprovementSuggestion(
                    id=f"sug_{suggestion_id}",
                    improvement_type=ImprovementType.WEIGHT_ADJUSTMENT,
                    priority=Priority.HIGH,
                    title=f"{agent} ê°€ì¤‘ì¹˜ í•˜í–¥",
                    description=f"{agent}ì˜ ì •í™•ë„ê°€ {accuracy:.1%}ë¡œ ë‚®ìŒ. ê°€ì¤‘ì¹˜ ê°ì†Œ ê¶Œì¥.",
                    rationale=f"ì§€ë‚œ ê¸°ê°„ {agent}ê°€ ì—¬ëŸ¬ ì†ì‹¤ ê±°ë˜ì—ì„œ ì˜ëª»ëœ íŒë‹¨",
                    expected_impact="ì „ì²´ ìŠ¹ë¥  2-5% ê°œì„  ì˜ˆìƒ",
                    implementation={
                        "agent": agent,
                        "current_weight": 1.0,
                        "suggested_weight": 0.7,
                        "method": "agent_weight_trainer.adjust_weight()"
                    }
                ))
            elif accuracy > 0.7:
                suggestion_id += 1
                suggestions.append(ImprovementSuggestion(
                    id=f"sug_{suggestion_id}",
                    improvement_type=ImprovementType.WEIGHT_ADJUSTMENT,
                    priority=Priority.MEDIUM,
                    title=f"{agent} ê°€ì¤‘ì¹˜ ìƒí–¥",
                    description=f"{agent}ì˜ ì •í™•ë„ê°€ {accuracy:.1%}ë¡œ ìš°ìˆ˜. ê°€ì¤‘ì¹˜ ì¦ê°€ ê¶Œì¥.",
                    rationale=f"ì§€ë‚œ ê¸°ê°„ {agent}ê°€ ì¼ê´€ëœ ì •í™•í•œ íŒë‹¨",
                    expected_impact="ì „ì²´ ìˆ˜ìµë¥  1-3% ê°œì„  ì˜ˆìƒ",
                    implementation={
                        "agent": agent,
                        "current_weight": 1.0,
                        "suggested_weight": 1.3,
                        "method": "agent_weight_trainer.adjust_weight()"
                    }
                ))
        
        # 2. ë“œë¡œë‹¤ìš´ ë†’ì„ ë•Œ ë¦¬ìŠ¤í¬ ê·œì¹™
        if perf.max_drawdown > self.MAX_ACCEPTABLE_DD:
            suggestion_id += 1
            suggestions.append(ImprovementSuggestion(
                id=f"sug_{suggestion_id}",
                improvement_type=ImprovementType.RISK_RULE,
                priority=Priority.CRITICAL,
                title="ì†ì ˆë§¤ ê¸°ì¤€ ê°•í™”",
                description=f"ë“œë¡œë‹¤ìš´ {perf.max_drawdown:.1%}ê°€ ê¸°ì¤€ {self.MAX_ACCEPTABLE_DD:.0%} ì´ˆê³¼",
                rationale="í° ì†ì‹¤ ë°©ì§€ë¥¼ ìœ„í•´ ì¡°ê¸° ì†ì ˆ í•„ìš”",
                expected_impact="ìµœëŒ€ ì†ì‹¤ 30-50% ê°ì†Œ ì˜ˆìƒ",
                implementation={
                    "config_key": "STOP_LOSS_THRESHOLD",
                    "current_value": 0.1,
                    "suggested_value": 0.07,
                    "method": "config.update()"
                }
            ))
        
        # 3. ìŠ¹ë¥  ë‚®ì„ ë•Œ ì „ëµ ë³€ê²½
        if perf.win_rate < 0.4:
            suggestion_id += 1
            suggestions.append(ImprovementSuggestion(
                id=f"sug_{suggestion_id}",
                improvement_type=ImprovementType.STRATEGY_CHANGE,
                priority=Priority.HIGH,
                title="ì§„ì… ì¡°ê±´ ê°•í™”",
                description=f"ìŠ¹ë¥  {perf.win_rate:.1%}ê°€ ëª©í‘œ {self.TARGET_WIN_RATE:.0%} ë¯¸ë‹¬",
                rationale="ë‚®ì€ ìŠ¹ë¥ ì€ ì§„ì… ì‹ í˜¸ê°€ ë¶€ì •í™•í•¨ì„ ì˜ë¯¸",
                expected_impact="ê±°ë˜ ìˆ˜ -20%, ìŠ¹ë¥  +15% ì˜ˆìƒ",
                implementation={
                    "config_key": "MIN_CONSENSUS_STRENGTH",
                    "current_value": 0.6,
                    "suggested_value": 0.75,
                    "method": "consensus_engine.set_threshold()"
                }
            ))
        
        # ìš°ì„ ìˆœìœ„ ì •ë ¬
        priority_order = {
            Priority.CRITICAL: 0,
            Priority.HIGH: 1,
            Priority.MEDIUM: 2,
            Priority.LOW: 3
        }
        suggestions.sort(key=lambda x: priority_order[x.priority])
        
        return suggestions
    
    def _generate_summary(
        self,
        perf: PerformanceSnapshot,
        suggestions: List[ImprovementSuggestion]
    ) -> str:
        """ìš”ì•½ ìƒì„±"""
        parts = []
        
        # ì„±ê³¼ ìš”ì•½
        if perf.total_trades == 0:
            return "ê±°ë˜ ì—†ìŒ. ë¶„ì„ ë¶ˆê°€."
        
        status = "ì–‘í˜¸" if perf.win_rate >= 0.5 and perf.total_return >= 0 else "ê°œì„  í•„ìš”"
        parts.append(f"ì „ì²´ ìƒíƒœ: {status}")
        parts.append(f"ê±°ë˜ {perf.total_trades}ê±´, ìŠ¹ë¥  {perf.win_rate:.1%}, ìˆ˜ìµë¥  {perf.total_return:.2%}")
        
        # í•µì‹¬ ì œì•ˆ
        critical = [s for s in suggestions if s.priority == Priority.CRITICAL]
        if critical:
            parts.append(f"ê¸´ê¸‰ ì¡°ì¹˜ í•„ìš”: {critical[0].title}")
        
        return " | ".join(parts)
    
    def apply_suggestion(self, suggestion: ImprovementSuggestion) -> bool:
        """ê°œì„ ì•ˆ ì ìš© (ì‹œë®¬ë ˆì´ì…˜)"""
        logger.info(f"Applying suggestion: {suggestion.title}")
        
        suggestion.applied = True
        self._applied_suggestions.append(suggestion)
        
        # ì‹¤ì œ ì ìš©ì€ ê° ëª¨ë“ˆì˜ ë©”ì„œë“œ í˜¸ì¶œ í•„ìš”
        # ì—¬ê¸°ì„œëŠ” ë¡œê¹…ë§Œ
        logger.info(f"Suggestion applied: {suggestion.implementation}")
        
        return True
    
    def _save_report(self, report: ReflectionReport):
        """ë³´ê³ ì„œ ì €ì¥"""
        filename = f"report_{report.period.value}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = self.data_dir / filename
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(report.to_dict(), f, ensure_ascii=False, indent=2)
        
        logger.info(f"Report saved: {filepath}")
    
    def get_latest_report(
        self,
        period: Optional[ReviewPeriod] = None
    ) -> Optional[ReflectionReport]:
        """ìµœê·¼ ë³´ê³ ì„œ ì¡°íšŒ"""
        reports = self._reports
        if period:
            reports = [r for r in reports if r.period == period]
        return reports[-1] if reports else None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Global Singleton
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_strategy_refiner: Optional[StrategyRefiner] = None


def get_strategy_refiner() -> StrategyRefiner:
    """StrategyRefiner ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤"""
    global _strategy_refiner
    if _strategy_refiner is None:
        _strategy_refiner = StrategyRefiner()
    return _strategy_refiner


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í…ŒìŠ¤íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import random
    
    refiner = StrategyRefiner()
    
    print("=== Strategy Refiner Test ===\n")
    
    # í…ŒìŠ¤íŠ¸ ê±°ë˜ ìƒì„±
    tickers = ["AAPL", "NVDA", "TSLA", "GOOGL", "MSFT"]
    for i in range(20):
        pnl = random.uniform(-0.15, 0.2)
        trade = TradeRecord(
            trade_id=f"trade_{i}",
            ticker=random.choice(tickers),
            action="BUY" if pnl > 0 else "SELL",
            entry_price=100,
            exit_price=100 * (1 + pnl),
            pnl=pnl,
            ai_votes={
                "claude": random.choice(["BUY", "SELL", "HOLD"]),
                "chatgpt": random.choice(["BUY", "SELL", "HOLD"]),
                "gemini": random.choice(["BUY", "SELL", "HOLD"])
            },
            ai_confidences={
                "claude": random.uniform(0.5, 0.95),
                "chatgpt": random.uniform(0.5, 0.95),
                "gemini": random.uniform(0.5, 0.95)
            },
            timestamp=datetime.now() - timedelta(days=random.randint(0, 7)),
            holding_period_days=random.randint(1, 10)
        )
        refiner.add_trade(trade)
    
    # ì£¼ê°„ ë¦¬ë·° ìƒì„±
    report = refiner.generate_weekly_review()
    
    print(f"Period: {report.period.value}")
    print(f"Summary: {report.summary}")
    print(f"\nFindings:")
    for f in report.key_findings:
        print(f"  - {f}")
    
    print(f"\nLessons:")
    for l in report.lessons_learned:
        print(f"  - {l}")
    
    print(f"\nSuggestions ({len(report.suggestions)}):")
    for s in report.suggestions[:5]:
        priority_emoji = {"critical": "ğŸ”´", "high": "ğŸŸ ", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}
        print(f"  {priority_emoji[s.priority.value]} [{s.priority.value}] {s.title}")
        print(f"     {s.description}")
