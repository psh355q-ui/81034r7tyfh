"""
Stealth Web Crawler for Premium News Sites

ì‹¤ì‹œê°„ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ìŠ¤í…”ìŠ¤ í¬ë¡¤ëŸ¬:
- User-Agent ë¡œí…Œì´ì…˜
- ëœë¤ ë”œë ˆì´ (2.5~3.5ë¶„)
- ë¸Œë¼ìš°ì € í—¤ë” ìŠ¤í‘¸í•‘
- í”„ë¡ì‹œ ì§€ì›
- Rate Limiting

ì‚¬ìš©ë²•:
    crawler = StealthWebCrawler(
        url="https://www.ft.com/content/xxxxx",
        interval_minutes=3
    )
    await crawler.start_monitoring()

ì‘ì„±ì¼: 2026-01-21
"""

import os
import logging
import asyncio
import random
import hashlib
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List, Callable
from urllib.parse import urlparse

# HTTP í´ë¼ì´ì–¸íŠ¸
import aiohttp
from bs4 import BeautifulSoup

# Database
from backend.database.repository import get_sync_session, NewsRepository

logger = logging.getLogger(__name__)


class StealthWebCrawler:
    """
    ìŠ¤í…”ìŠ¤ ì›¹ í¬ë¡¤ëŸ¬ - íŠ¹ì • URLì„ ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§

    íŠ¹ì§•:
    - ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ í–‰ë™
    - ëœë¤ ë”œë ˆì´ë¡œ íŒ¨í„´ ìˆ¨ê¸°ê¸°
    - User-Agent ë¡œí…Œì´ì…˜
    - ì½˜í…ì¸  ë³€ê²½ ê°ì§€
    """

    # ì‹¤ì œ ë¸Œë¼ìš°ì € User-Agent ëª©ë¡
    USER_AGENTS = [
        # Chrome on Windows
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        # Chrome on Mac
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        # Firefox on Windows
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
        # Safari on Mac
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
        # Edge on Windows
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    ]

    def __init__(
        self,
        url: str,
        interval_minutes: float = 3.0,
        variance_minutes: float = 0.5,
        proxy: Optional[str] = None,
        callback: Optional[Callable] = None
    ):
        """
        Args:
            url: ëª¨ë‹ˆí„°ë§í•  URL
            interval_minutes: ê¸°ë³¸ ê°„ê²© (ë¶„)
            variance_minutes: ëœë¤ í¸ì°¨ (ë¶„) - Â±ê°’
            proxy: í”„ë¡ì‹œ ì„œë²„ (ì˜ˆ: "http://proxy.example.com:8080")
            callback: ìƒˆ ì½˜í…ì¸  ë°œê²¬ ì‹œ í˜¸ì¶œí•  í•¨ìˆ˜
        """
        self.url = url
        self.interval_minutes = interval_minutes
        self.variance_minutes = variance_minutes
        self.proxy = proxy
        self.callback = callback

        # ìƒíƒœ ê´€ë¦¬
        self.is_running = False
        self.last_content_hash = None
        self.fetch_count = 0
        self.last_fetch_time = None

        # ë„ë©”ì¸ ì¶”ì¶œ (ì†ŒìŠ¤ëª…ìœ¼ë¡œ ì‚¬ìš©)
        parsed_url = urlparse(url)
        self.source_name = parsed_url.netloc.replace('www.', '').upper()

        logger.info(f"StealthWebCrawler initialized for {self.source_name}")
        logger.info(f"Interval: {interval_minutes}Â±{variance_minutes} minutes")

    def _get_random_headers(self) -> Dict[str, str]:
        """ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ëŠ” ëœë¤ í—¤ë” ìƒì„±"""
        user_agent = random.choice(self.USER_AGENTS)

        headers = {
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,ko;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',  # Do Not Track
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        }

        # Referer ì¶”ê°€ (ë„ë©”ì¸ì—ì„œ ì˜¨ ê²ƒì²˜ëŸ¼)
        parsed_url = urlparse(self.url)
        headers['Referer'] = f"{parsed_url.scheme}://{parsed_url.netloc}/"

        return headers

    def _calculate_next_delay(self) -> float:
        """ë‹¤ìŒ í¬ë¡¤ë§ê¹Œì§€ ëŒ€ê¸° ì‹œê°„ ê³„ì‚° (ì´ˆ ë‹¨ìœ„)"""
        # ê¸°ë³¸ ê°„ê²© Â± ëœë¤ í¸ì°¨
        base_seconds = self.interval_minutes * 60
        variance_seconds = self.variance_minutes * 60

        delay = base_seconds + random.uniform(-variance_seconds, variance_seconds)

        # ìµœì†Œ 1ë¶„ ë³´ì¥
        return max(delay, 60)

    def _calculate_content_hash(self, content: str) -> str:
        """ì½˜í…ì¸  í•´ì‹œ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)"""
        return hashlib.sha256(content.encode('utf-8')).hexdigest()

    async def _fetch_content(self) -> Optional[Dict[str, Any]]:
        """
        URLì—ì„œ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°

        Returns:
            {
                'title': str,
                'content': str,
                'html': str,
                'content_hash': str
            }
        """
        try:
            headers = self._get_random_headers()

            # í”„ë¡ì‹œ ì„¤ì •
            connector = None
            if self.proxy:
                connector = aiohttp.TCPConnector()

            timeout = aiohttp.ClientTimeout(total=30)

            async with aiohttp.ClientSession(
                connector=connector,
                timeout=timeout
            ) as session:
                proxy_url = self.proxy if self.proxy else None

                async with session.get(
                    self.url,
                    headers=headers,
                    proxy=proxy_url,
                    ssl=True  # SSL ê²€ì¦
                ) as response:
                    if response.status != 200:
                        logger.warning(f"HTTP {response.status} from {self.url}")
                        return None

                    html = await response.text()

                    # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
                    soup = BeautifulSoup(html, 'html.parser')

                    # ë©”íƒ€ íƒœê·¸ ìš°ì„  ì¶”ì¶œ
                    title = None
                    description = None

                    # Open Graph íƒœê·¸
                    og_title = soup.find('meta', property='og:title')
                    og_description = soup.find('meta', property='og:description')

                    if og_title:
                        title = og_title.get('content')
                    if og_description:
                        description = og_description.get('content')

                    # ë©”íƒ€ description
                    if not description:
                        meta_desc = soup.find('meta', attrs={'name': 'description'})
                        if meta_desc:
                            description = meta_desc.get('content')

                    # <title> íƒœê·¸
                    if not title and soup.title:
                        title = soup.title.string

                    # ë³¸ë¬¸ ì¶”ì¶œ (article, main íƒœê·¸ ìš°ì„ )
                    content = ""
                    article = soup.find('article')
                    if article:
                        # script, style íƒœê·¸ ì œê±°
                        for tag in article(['script', 'style', 'nav', 'header', 'footer']):
                            tag.decompose()
                        content = article.get_text(separator='\n', strip=True)
                    else:
                        # article ì—†ìœ¼ë©´ main íƒœê·¸
                        main = soup.find('main')
                        if main:
                            for tag in main(['script', 'style', 'nav', 'header', 'footer']):
                                tag.decompose()
                            content = main.get_text(separator='\n', strip=True)

                    # ì½˜í…ì¸ ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì „ì²´ body
                    if not content:
                        body = soup.find('body')
                        if body:
                            for tag in body(['script', 'style', 'nav', 'header', 'footer']):
                                tag.decompose()
                            content = body.get_text(separator='\n', strip=True)

                    # í•´ì‹œ ê³„ì‚°
                    content_hash = self._calculate_content_hash(content)

                    return {
                        'title': title or 'No Title',
                        'description': description or '',
                        'content': content,
                        'html': html,
                        'content_hash': content_hash
                    }

        except asyncio.TimeoutError:
            logger.error(f"Timeout fetching {self.url}")
        except aiohttp.ClientError as e:
            logger.error(f"HTTP error fetching {self.url}: {e}")
        except Exception as e:
            logger.error(f"Error fetching {self.url}: {e}")

        return None

    async def _save_to_db(self, data: Dict[str, Any]) -> bool:
        """
        DBì— ì €ì¥

        Args:
            data: _fetch_content()ì˜ ê²°ê³¼

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            session = get_sync_session()
            repo = NewsRepository(session)

            try:
                # URLë¡œ ì¤‘ë³µ ì²´í¬
                if repo.exists_by_url(self.url):
                    # ì´ë¯¸ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸ (content_hash ë‹¤ë¥´ë©´)
                    existing = repo.get_by_url(self.url)
                    if existing and existing.content_hash != data['content_hash']:
                        logger.info(f"Content changed, updating article: {self.url}")
                        # Update logic would go here, but repo doesn't have update method
                        # For now, we'll skip if already exists
                        return False
                    else:
                        logger.debug(f"Article unchanged: {self.url}")
                        return False

                # ìƒˆ ê¸°ì‚¬ ì €ì¥
                news_data = {
                    'title': data['title'],
                    'summary': data['description'],
                    'content': data['content'],
                    'url': self.url,
                    'source': self.source_name,
                    'published_at': datetime.now(),  # í¬ë¡¤ë§ ì‹œê°„ì„ ë°œí–‰ ì‹œê°„ìœ¼ë¡œ
                    'author': None,
                    'tags': [],
                    'processed_at': None,
                    'content_hash': data['content_hash']
                }

                saved_article = repo.save_processed_article(news_data)

                if saved_article:
                    logger.info(f"âœ… New article saved: {saved_article.title[:50]}...")
                    return True
                else:
                    logger.warning(f"Failed to save article (duplicate?): {self.url}")
                    return False

            finally:
                session.close()

        except Exception as e:
            logger.error(f"Error saving to DB: {e}")
            return False

    async def _monitor_once(self) -> bool:
        """
        í•œ ë²ˆ í¬ë¡¤ë§ ì‹¤í–‰

        Returns:
            ìƒˆ ì½˜í…ì¸ ê°€ ë°œê²¬ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€
        """
        self.fetch_count += 1
        self.last_fetch_time = datetime.now()

        logger.info(f"ğŸ” Fetching [{self.fetch_count}]: {self.url}")

        # ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
        data = await self._fetch_content()

        if not data:
            logger.warning(f"Failed to fetch content from {self.url}")
            return False

        # ì½˜í…ì¸  ë³€ê²½ ì²´í¬
        content_hash = data['content_hash']

        if self.last_content_hash is None:
            # ì²« í¬ë¡¤ë§
            self.last_content_hash = content_hash
            logger.info(f"ğŸ“ Initial content captured (hash: {content_hash[:8]}...)")

            # DB ì €ì¥
            saved = await self._save_to_db(data)

            # ì½œë°± í˜¸ì¶œ
            if saved and self.callback:
                try:
                    if asyncio.iscoroutinefunction(self.callback):
                        await self.callback(data)
                    else:
                        self.callback(data)
                except Exception as e:
                    logger.error(f"Callback error: {e}")

            return saved

        elif content_hash != self.last_content_hash:
            # ì½˜í…ì¸  ë³€ê²½ë¨!
            logger.info(f"ğŸ†• Content CHANGED! (old: {self.last_content_hash[:8]}... -> new: {content_hash[:8]}...)")
            self.last_content_hash = content_hash

            # DB ì €ì¥
            saved = await self._save_to_db(data)

            # ì½œë°± í˜¸ì¶œ
            if saved and self.callback:
                try:
                    if asyncio.iscoroutinefunction(self.callback):
                        await self.callback(data)
                    else:
                        self.callback(data)
                except Exception as e:
                    logger.error(f"Callback error: {e}")

            return saved

        else:
            # ë³€ê²½ ì—†ìŒ
            logger.debug(f"No content change (hash: {content_hash[:8]}...)")
            return False

    async def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ë¬´í•œ ë£¨í”„)"""
        if self.is_running:
            logger.warning("Crawler already running")
            return

        self.is_running = True
        logger.info(f"ğŸš€ Starting stealth monitoring: {self.url}")
        logger.info(f"   Interval: {self.interval_minutes}Â±{self.variance_minutes} minutes")

        try:
            while self.is_running:
                # í¬ë¡¤ë§ ì‹¤í–‰
                new_content = await self._monitor_once()

                if new_content:
                    logger.info(f"âœ¨ New content detected and saved!")

                # ë‹¤ìŒ í¬ë¡¤ë§ê¹Œì§€ ëŒ€ê¸°
                delay = self._calculate_next_delay()
                next_time = datetime.now() + timedelta(seconds=delay)

                logger.info(f"â° Next fetch in {delay/60:.1f} minutes (at {next_time.strftime('%H:%M:%S')})")

                await asyncio.sleep(delay)

        except asyncio.CancelledError:
            logger.info("Monitoring cancelled")
        except Exception as e:
            logger.error(f"Monitoring error: {e}", exc_info=True)
        finally:
            self.is_running = False
            logger.info("ğŸ›‘ Monitoring stopped")

    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        if self.is_running:
            self.is_running = False
            logger.info("Stopping monitoring...")


class MultiSiteMonitor:
    """
    ì—¬ëŸ¬ ì‚¬ì´íŠ¸ë¥¼ ë™ì‹œì— ëª¨ë‹ˆí„°ë§
    """

    def __init__(self):
        self.crawlers: List[StealthWebCrawler] = []
        self.tasks: List[asyncio.Task] = []

    def add_site(
        self,
        url: str,
        interval_minutes: float = 3.0,
        variance_minutes: float = 0.5,
        callback: Optional[Callable] = None
    ):
        """ì‚¬ì´íŠ¸ ì¶”ê°€"""
        crawler = StealthWebCrawler(
            url=url,
            interval_minutes=interval_minutes,
            variance_minutes=variance_minutes,
            callback=callback
        )
        self.crawlers.append(crawler)
        logger.info(f"Added site: {url}")

    async def start_all(self):
        """ëª¨ë“  ì‚¬ì´íŠ¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        logger.info(f"ğŸš€ Starting monitoring for {len(self.crawlers)} sites")

        # ê° í¬ë¡¤ëŸ¬ë¥¼ ë³„ë„ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
        for crawler in self.crawlers:
            task = asyncio.create_task(crawler.start_monitoring())
            self.tasks.append(task)

        # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸° (ì‹¤ì œë¡œëŠ” ë¬´í•œ ë£¨í”„)
        try:
            await asyncio.gather(*self.tasks)
        except asyncio.CancelledError:
            logger.info("All monitoring cancelled")

    def stop_all(self):
        """ëª¨ë“  ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        logger.info("Stopping all crawlers...")
        for crawler in self.crawlers:
            crawler.stop_monitoring()

        for task in self.tasks:
            task.cancel()


# ============================================================================
# í…ŒìŠ¤íŠ¸ ë° ì‚¬ìš© ì˜ˆì‹œ
# ============================================================================

async def test_ft_monitoring():
    """Financial Times ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸"""
    print("=" * 70)
    print("Stealth Web Crawler Test - Financial Times")
    print("=" * 70)

    # ì½œë°± í•¨ìˆ˜ (ìƒˆ ì½˜í…ì¸  ë°œê²¬ ì‹œ)
    def on_new_content(data: Dict[str, Any]):
        print("\nğŸ”” NEW CONTENT DETECTED!")
        print(f"   Title: {data['title'][:80]}...")
        print(f"   Length: {len(data['content'])} chars")
        print(f"   Hash: {data['content_hash'][:16]}...")

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = StealthWebCrawler(
        url="https://www.ft.com/content/1369a45e-e39b-4aaa-a347-b1800da7fd31",
        interval_minutes=3.0,
        variance_minutes=0.5,
        callback=on_new_content
    )

    # ëª¨ë‹ˆí„°ë§ ì‹œì‘ (Ctrl+Cë¡œ ì¤‘ì§€)
    try:
        await crawler.start_monitoring()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Monitoring stopped by user")
        crawler.stop_monitoring()


async def test_multi_site():
    """ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ë™ì‹œ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸"""
    print("=" * 70)
    print("Multi-Site Monitoring Test")
    print("=" * 70)

    monitor = MultiSiteMonitor()

    # ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ì¶”ê°€
    monitor.add_site(
        url="https://www.ft.com/content/1369a45e-e39b-4aaa-a347-b1800da7fd31",
        interval_minutes=3.0
    )

    # ë‹¤ë¥¸ ì‚¬ì´íŠ¸ë„ ì¶”ê°€ ê°€ëŠ¥
    # monitor.add_site(
    #     url="https://www.reuters.com/markets/...",
    #     interval_minutes=5.0
    # )

    # ëª¨ë“  ì‚¬ì´íŠ¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    try:
        await monitor.start_all()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  All monitoring stopped by user")
        monitor.stop_all()


if __name__ == "__main__":
    import sys

    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    if len(sys.argv) > 1 and sys.argv[1] == 'multi':
        asyncio.run(test_multi_site())
    else:
        asyncio.run(test_ft_monitoring())
