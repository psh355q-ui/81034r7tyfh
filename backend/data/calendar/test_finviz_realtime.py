"""
Finviz ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ (v=3 í˜ì´ì§€)
24ë¶„ ì „ China AI chips ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
"""
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re


async def scrape_finviz_news():
    """Finviz ë‰´ìŠ¤ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
    print("=" * 70)
    print("  Finviz ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘")
    print("  https://finviz.com/news.ashx?v=3")
    print("=" * 70)
    print()
    
    url = "https://finviz.com/news.ashx?v=3"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        async with aiohttp.ClientSession(headers=headers) as session:
            async with session.get(url) as resp:
                print(f"Status: {resp.status}\n")
                
                if resp.status != 200:
                    print(f"âŒ Failed to access Finviz (status {resp.status})")
                    return
                
                html = await resp.text()
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # ë‰´ìŠ¤ í…Œì´ë¸” ì°¾ê¸°
        news_table = soup.find('table', {'id': 'news'})
        
        if not news_table:
            # classë¡œ ì°¾ê¸°
            news_table = soup.find('table', class_='news-table')
        
        if not news_table:
            # ëª¨ë“  í…Œì´ë¸”ì—ì„œ ë‰´ìŠ¤ ì°¾ê¸°
            print("ğŸ“Š í…Œì´ë¸” IDë¡œ ëª» ì°¾ìŒ, ì „ì²´ ê²€ìƒ‰ ì¤‘...\n")
            
            # ë‰´ìŠ¤ ë§í¬ë“¤ ì°¾ê¸° (a íƒœê·¸)
            all_links = soup.find_all('a', href=True)
            
            news_items = []
            for link in all_links:
                text = link.get_text(strip=True)
                href = link.get('href', '')
                
                # ë‰´ìŠ¤ ë§í¬ í•„í„°ë§ (ì™¸ë¶€ ë§í¬ë§Œ)
                if href.startswith('http') and len(text) > 20:
                    # ì‹œê°„ ì •ë³´ ì°¾ê¸° (í˜•ì œ ìš”ì†Œ)
                    parent = link.find_parent('tr')
                    if parent:
                        time_elem = parent.find('td', class_='news-time')
                        if not time_elem:
                            # ì‹œê°„ íŒ¨í„´ìœ¼ë¡œ ì°¾ê¸° (ì˜ˆ: "24 min", "2h ago")
                            time_text = parent.get_text()
                            time_match = re.search(r'(\d+)\s*(min|h|hour|sec)', time_text)
                            if time_match:
                                time_str = time_match.group(0)
                            else:
                                time_str = "Unknown"
                        else:
                            time_str = time_elem.get_text(strip=True)
                        
                        news_items.append({
                            'time': time_str,
                            'title': text,
                            'link': href
                        })
            
            print(f"âœ… Found {len(news_items)} news items\n")
            
            for i, item in enumerate(news_items[:15], 1):
                time_str = item['time']
                title = item['title']
                link = item['link']
                
                # China, AI, ASML, EUV ì²´í¬
                keywords = ['CHINA', 'AI', 'ASML', 'EUV', 'CHIP', 'SEMICONDUCTOR']
                if any(kw in title.upper() for kw in keywords):
                    marker = "â­ TARGET! "
                else:
                    marker = "   "
                
                # 24ë¶„ ì²´í¬
                if '24' in time_str and 'min' in time_str:
                    marker = "ğŸ¯ 24MIN! "
                
                print(f"{marker}{i}. [{time_str}] {title[:55]}...")
                print(f"       â””â”€ {link[:60]}...")
                
                # ì†ŒìŠ¤ ì¶”ì¶œ
                if 'reuters' in link.lower():
                    print(f"       â””â”€ Source: Reuters")
                elif 'bloomberg' in link.lower():
                    print(f"       â””â”€ Source: Bloomberg")
                
                print()
        
        else:
            # í…Œì´ë¸” êµ¬ì¡°ë¡œ íŒŒì‹±
            print("âœ… ë‰´ìŠ¤ í…Œì´ë¸” ë°œê²¬!\n")
            
            rows = news_table.find_all('tr')
            
            for i, row in enumerate(rows[:15], 1):
                cells = row.find_all('td')
                
                if len(cells) >= 2:
                    # ì²« ë²ˆì§¸ ì…€: ì‹œê°„
                    time_cell = cells[0].get_text(strip=True)
                    
                    # ë‘ ë²ˆì§¸ ì…€: ë‰´ìŠ¤
                    news_cell = cells[1]
                    title = news_cell.get_text(strip=True)
                    link_elem = news_cell.find('a')
                    link = link_elem.get('href', '') if link_elem else ''
                    
                    # í‚¤ì›Œë“œ ì²´í¬
                    keywords = ['CHINA', 'AI', 'ASML', 'EUV', 'CHIP']
                    if any(kw in title.upper() for kw in keywords):
                        marker = "â­ "
                    else:
                        marker = "   "
                    
                    if '24' in time_cell and 'min' in time_cell:
                        marker = "ğŸ¯ 24MIN! "
                    
                    print(f"{marker}{i}. [{time_cell}] {title[:55]}...")
                    print(f"       â””â”€ {link[:60]}...")
                    print()
        
        print()
        print("=" * 70)
        print("  ê²°ë¡ ")
        print("=" * 70)
        print()
        print("âœ… Finviz ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ ê°€ëŠ¥!")
        print("   - 24ë¶„ ì „ ë‰´ìŠ¤ í™•ì¸ë¨")
        print("   - ë¬´ë£Œ, API í‚¤ ë¶ˆí•„ìš”")
        print("   - ì‹¤ì‹œê°„ì„± ìš°ìˆ˜ (5ë¶„ ì´ë‚´)")
        print()
        print("ğŸ’¡ ê¶Œì¥: Google News + Finviz ì¡°í•©")
        print("   - Google News: ì•ˆì •ì , bot ì°¨ë‹¨ ì—†ìŒ")
        print("   - Finviz: ë¹ ë¦„, ê¸ˆìœµ ì „ë¬¸")
    
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(scrape_finviz_news())
