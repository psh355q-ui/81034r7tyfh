"""
Unified News Processor

í†µí•© ë‰´ìŠ¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸:
í¬ë¡¤ë§ â†’ ì¤‘ë³µ ì œê±° â†’ GLM ë¶„ì„ â†’ ì €ìž¥ì„ ì›ìžì ìœ¼ë¡œ ì²˜ë¦¬

Features:
- URL + Content Hash + Semantic ì¤‘ë³µ ì²´í¬
- GLM-4.7 ì¢…ëª©/ì„¹í„° ì¶”ì¶œ (ëª¨ë“  ë‰´ìŠ¤)
- ì„ íƒì  Deep Analysis (ì¤‘ìš” ë‰´ìŠ¤ë§Œ)
- ì›ìžì  DB ì €ìž¥
- ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
"""

import asyncio
import logging
import os
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
from dataclasses import dataclass

from sqlalchemy.orm import Session
# Use PostgreSQL models (backend.database.models) instead of SQLite (backend.data.news_models)
from backend.database.models import NewsArticle, NewsAnalysis, NewsTickerRelevance
from backend.data.rss_crawler import generate_content_hash
from backend.data.news_analyzer import NewsDeepAnalyzer
from backend.ai.llm.local_embeddings import LocalEmbeddingService
from backend.ai.llm.ollama_client import OllamaClient

# GLM-4.7 Client (Phase 1 Integration)
try:
    from backend.ai.glm_client import GLMClient, MockGLMClient
    GLM_AVAILABLE = True
except ImportError:
    GLM_AVAILABLE = False
    logger.warning("GLM client not available, using fallback")

logger = logging.getLogger(__name__)


@dataclass
class ProcessedNews:
    """ì²˜ë¦¬ëœ ë‰´ìŠ¤ ê²°ê³¼"""
    article: NewsArticle
    analysis: Optional[NewsAnalysis]
    was_analyzed: bool
    glm_analysis: Optional[Dict] = None  # GLM ë¶„ì„ ê²°ê³¼ ì¶”ê°€
    skipped_reason: Optional[str] = None


@dataclass
class ProcessingResult:
    """ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼"""
    processed: List[ProcessedNews]
    skipped: List[Dict[str, Any]]
    errors: List[Exception]


class UnifiedNewsProcessor:
    """
    í†µí•© ë‰´ìŠ¤ ì²˜ë¦¬ê¸°

    í¬ë¡¤ë§ëœ ì›ì‹œ ê¸°ì‚¬ë¥¼ ë°›ì•„ì„œ:
    1. URL ì¤‘ë³µ ì²´í¬
    2. Content Hash ì¤‘ë³µ ì²´í¬
    3. Semantic ì¤‘ë³µ ì²´í¬ (ì„ íƒ)
    4. GLM ë¶„ì„ (ì¢…ëª©/ì„¹í„° ì¶”ì¶œ)
    5. ìž„ë² ë”© ìƒì„±
    6. Deep Analysis (ì„ íƒì )
    7. DB ì €ìž¥ (ì›ìžì )
    """

    def __init__(
        self,
        db: Session,
        semantic_dedup: bool = False,
        semantic_threshold: float = 0.95,
        analyze_all: bool = False,
        glm_rate_limit: float = None
    ):
        self.db = db
        self.semantic_dedup = semantic_dedup
        self.semantic_threshold = semantic_threshold
        self.analyze_all = analyze_all

        # Rate Limiting for GLM API (prevent Concurrency Limit exceeded)
        # GLM-4-Plus has Concurrency Limit 20
        # Default: 3.0 seconds between calls (very conservative for stability)
        self.glm_rate_limit = glm_rate_limit or float(os.environ.get("NEWS_GLM_RATE_LIMIT", "3.0"))

        # Concurrency Control: Semaphore to limit simultaneous GLM API calls
        # Prevents bursting requests that exceed GLM's Concurrency Limit
        # Recommended: 3-5 for stability (GLM-4-Plus has limit of 20)
        glm_concurrency_limit = int(os.environ.get("NEWS_GLM_CONCURRENCY", "3"))
        self.glm_semaphore = asyncio.Semaphore(glm_concurrency_limit)
        logger.info(f"âœ… GLM Concurrency Limit: {glm_concurrency_limit} simultaneous requests")

        # Services
        self.embedding_service = LocalEmbeddingService()
        self.ollama_client = OllamaClient()
        self.analyzer = NewsDeepAnalyzer(db)

        # LLM Client Selection
        # 1. GLM API (ìœ ë£Œ, ì •í™•ë„ ë†’ìŒ)
        # 2. Ollama Local LLM (ë¬´ë£Œ, ë¡œì»¬ ì‹¤í–‰)
        self.use_ollama = os.environ.get("NEWS_USE_OLLAMA", "true").lower() == "true"
        self.glm_enabled = os.environ.get("NEWS_GLM_ENABLED", "true").lower() == "true" and not self.use_ollama

        # Ollama Local LLM (ë¬´ë£Œ, ì¢…ëª©/ì„¹í„° ì¶”ì¶œ)
        if self.use_ollama:
            self.llm_client = self.ollama_client
            logger.info("âœ… Using Ollama Local LLM for ticker/sector extraction (COST: $0)")
        # GLM API (ìœ ë£Œ)
        elif GLM_AVAILABLE and self.glm_enabled:
            glm_api_key = os.environ.get("GLM_API_KEY")
            glm_model = os.environ.get("NEWS_GLM_MODEL", "glm-4-plus")
            if glm_api_key and glm_api_key != "your-glm-api-key-here":
                try:
                    self.llm_client = GLMClient(api_key=glm_api_key, model=glm_model)
                    logger.info(f"âœ… GLM Client initialized (Real API) with model: {glm_model}")
                except Exception as e:
                    logger.warning(f"GLM init failed, using Ollama: {e}")
                    self.llm_client = self.ollama_client
            else:
                self.llm_client = self.ollama_client
                logger.warning("âš ï¸ GLM_API_KEY not set, using Ollama (COST: $0)")
        else:
            # Fallback to Ollama
            self.llm_client = self.ollama_client
            logger.info("â„¹ï¸ Using Ollama Local LLM (COST: $0)")

        # Stats
        self.stats = {
            "total": 0,
            "skipped_url": 0,
            "skipped_hash": 0,
            "skipped_semantic": 0,
            "saved": 0,
            "analyzed": 0,
            "glm_analyzed": 0,  # GLM ë¶„ì„ í†µê³„ ì¶”ê°€
            "errors": 0
        }

    def _check_url_duplicate(self, url: str) -> Optional[NewsArticle]:
        """
        URL ì¤‘ë³µ ì²´í¬ (ìƒì„¸ ë¡œê¹… í¬í•¨)

        Returns:
            NewsArticle: ì¤‘ë³µ ê¸°ì‚¬ê°€ ì¡´ìž¬í•˜ë©´ í•´ë‹¹ ê¸°ì‚¬ ê°ì²´ ë°˜í™˜
            None: ì¤‘ë³µì´ ì—†ìœ¼ë©´ None ë°˜í™˜
        """
        existing = self.db.query(NewsArticle).filter(NewsArticle.url == url).first()
        if existing:
            logger.info(f"ðŸ”„ Duplicate URL found: {url}")
            logger.info(f"   Existing: {existing.title[:80]}...")
            logger.info(f"   Article ID: {existing.id} | Published: {existing.published_date}")

            # GLM ë¶„ì„ ë°ì´í„° í™•ì¸
            if existing.glm_analysis:
                tickers = existing.glm_analysis.get('tickers', [])
                sectors = existing.glm_analysis.get('sectors', [])
                confidence = existing.glm_analysis.get('confidence', 0)
                logger.info(f"   Existing data: GLM analysis âœ…")
                logger.info(f"      - Tickers: {tickers}")
                logger.info(f"      - Sectors: {sectors}")
                logger.info(f"      - Confidence: {confidence:.2f}")
            else:
                logger.info(f"   Existing data: GLM analysis âŒ")

            # Deep Analysis ë°ì´í„° í™•ì¸
            if existing.analysis:
                logger.info(f"   Existing data: Deep analysis âœ…")
                logger.info(f"      - Sentiment: {existing.analysis.sentiment_overall}")
                logger.info(f"      - Score: {existing.analysis.sentiment_score:.2f}")
                logger.info(f"      - Urgency: {existing.analysis.urgency}")
            else:
                logger.info(f"   Existing data: Deep analysis âŒ")

        return existing
    
    def _check_hash_duplicate(self, content_hash: str) -> Optional[NewsArticle]:
        """
        Content Hash ì¤‘ë³µ ì²´í¬ (ìƒì„¸ ë¡œê¹… í¬í•¨)

        Returns:
            NewsArticle: ì¤‘ë³µ ê¸°ì‚¬ê°€ ì¡´ìž¬í•˜ë©´ í•´ë‹¹ ê¸°ì‚¬ ê°ì²´ ë°˜í™˜
            None: ì¤‘ë³µì´ ì—†ìœ¼ë©´ None ë°˜í™˜
        """
        existing = self.db.query(NewsArticle).filter(
            NewsArticle.content_hash == content_hash
        ).first()
        if existing:
            logger.info(f"ðŸ”„ Duplicate content hash found")
            logger.info(f"   Existing: {existing.title[:80]}...")
            logger.info(f"   Article ID: {existing.id} | URL: {existing.url}")

            # GLM ë¶„ì„ ë°ì´í„° í™•ì¸
            if existing.glm_analysis:
                tickers = existing.glm_analysis.get('tickers', [])
                logger.info(f"   Existing data: GLM analysis âœ… (Tickers: {tickers})")
            else:
                logger.info(f"   Existing data: GLM analysis âŒ")

            # Deep Analysis ë°ì´í„° í™•ì¸
            if existing.analysis:
                logger.info(f"   Existing data: Deep analysis âœ… (Sentiment: {existing.analysis.sentiment_overall})")
            else:
                logger.info(f"   Existing data: Deep analysis âŒ")

        return existing
    
    def _check_semantic_duplicate(
        self,
        title: str,
        content: str,
        embedding: List[float]
    ) -> Optional[NewsArticle]:
        """
        ì˜ë¯¸ì  ì¤‘ë³µ ì²´í¬ (ìž„ë² ë”© ìœ ì‚¬ë„)
        
        ìµœê·¼ 24ì‹œê°„ ê¸°ì‚¬ì™€ ë¹„êµí•˜ì—¬ ìœ ì‚¬ë„ > thresholdë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
        """
        if not self.semantic_dedup:
            return None
        
        # ìµœê·¼ 24ì‹œê°„ ê¸°ì‚¬ ì¡°íšŒ
        cutoff = datetime.utcnow() - timedelta(hours=24)
        recent_articles = (
            self.db.query(NewsArticle)
            .filter(NewsArticle.published_date >= cutoff)
            .filter(NewsArticle.embedding.isnot(None))
            .limit(100)  # ìµœëŒ€ 100ê°œë§Œ ë¹„êµ
            .all()
        )
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        import numpy as np
        new_emb = np.array(embedding)
        
        for article in recent_articles:
            if article.embedding:
                old_emb = np.array(article.embedding)
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                similarity = np.dot(new_emb, old_emb) / (
                    np.linalg.norm(new_emb) * np.linalg.norm(old_emb)
                )
                
                if similarity > self.semantic_threshold:
                    logger.info(f"ðŸ”„ Semantic duplicate found (similarity: {similarity:.3f})")
                    logger.info(f"   New: {title[:80]}...")
                    logger.info(f"   Existing: {article.title[:80]}...")
                    logger.info(f"   Article ID: {article.id} | Similarity: {similarity:.3f}")

                    # GLM ë¶„ì„ ë°ì´í„° í™•ì¸
                    if article.glm_analysis:
                        tickers = article.glm_analysis.get('tickers', [])
                        logger.info(f"   Existing data: GLM analysis âœ… (Tickers: {tickers})")
                    else:
                        logger.info(f"   Existing data: GLM analysis âŒ")

                    # Deep Analysis ë°ì´í„° í™•ì¸
                    if article.analysis:
                        logger.info(f"   Existing data: Deep analysis âœ… (Sentiment: {article.analysis.sentiment_overall})")
                    else:
                        logger.info(f"   Existing data: Deep analysis âŒ")

                    return article
        
        return None
    
    def _should_analyze(self, article_data: Dict[str, Any]) -> bool:
        """
        ë¶„ì„ ì—¬ë¶€ ê²°ì •
        
        analyze_all=Trueë©´ ëª¨ë‘ ë¶„ì„
        ì•„ë‹ˆë©´ ì¤‘ìš”í•œ ê²ƒë§Œ ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜)
        """
        if self.analyze_all:
            return True
        
        # ì¤‘ìš” í‚¤ì›Œë“œ ì²´í¬
        important_keywords = [
            "earnings", "merger", "acquisition", "lawsuit", "bankruptcy",
            "FDA", "approval", "recall", "layoff", "CEO", "dividend",
            "ì‹¤ì ", "ì¸ìˆ˜", "í•©ë³‘", "ì†Œì†¡", "íŒŒì‚°", "ìŠ¹ì¸", "ë¦¬ì½œ", "í•´ê³ ", "ë°°ë‹¹"
        ]
        
        title = article_data.get("title", "").lower()
        content = article_data.get("content", "").lower()
        
        for keyword in important_keywords:
            if keyword.lower() in title or keyword.lower() in content:
                return True
        
        return False
    
    async def process_article(
        self,
        raw_article: Dict[str, Any]
    ) -> Optional[ProcessedNews]:
        """
        ë‹¨ì¼ ê¸°ì‚¬ ì²˜ë¦¬
        
        Args:
            raw_article: í¬ë¡¤ë§ëœ ì›ì‹œ ê¸°ì‚¬ ë°ì´í„°
            
        Returns:
            ProcessedNews: ì²˜ë¦¬ëœ ê²°ê³¼
            None: ì¤‘ë³µìœ¼ë¡œ ìŠ¤í‚µë¨
        """
        self.stats["total"] += 1
        
        url = raw_article.get("url", "")
        title = raw_article.get("title", "")
        content = raw_article.get("content", "")
        
        if not url or not title:
            logger.warning("Skipped: Missing URL or title")
            return None
        
        try:
            # Stage 1: URL ì¤‘ë³µ ì²´í¬
            if existing_article := self._check_url_duplicate(url):
                self.stats["skipped_url"] += 1
                logger.info(f"â­ï¸  Skipping duplicate article: {title[:80]}...")
                logger.info(f"   Reason: URL already exists with ID {existing_article.id}")
                logger.info(f"   New URL: {url}")
                logger.info(f"   Existing URL: {existing_article.url}")
                return None

            # Stage 2: Content Hash ì¤‘ë³µ ì²´í¬
            # Always generate content_hash (DB requires NOT NULL)
            # Use title+content if available, otherwise title only
            hash_input = f"{title}\n{content}" if content and len(content) > 50 else title
            content_hash = generate_content_hash(hash_input, "")

            # Only check duplicate if we have substantial content
            if content and len(content) > 50:
                if existing_article := self._check_hash_duplicate(content_hash):
                    self.stats["skipped_hash"] += 1
                    logger.info(f"â­ï¸  Skipping duplicate article: {title[:80]}...")
                    logger.info(f"   Reason: Content hash already exists with ID {existing_article.id}")
                    logger.info(f"   New: {title[:80]}...")
                    logger.info(f"   Existing: {existing_article.title[:80]}...")
                    return None
            
            # Stage 3: ìž„ë² ë”© ìƒì„±
            embedding_text = f"{title}\n{content[:500]}" if content else title
            embedding = self.embedding_service.get_embedding(embedding_text)
            
            # Stage 4: Semantic ì¤‘ë³µ ì²´í¬ (ì„ íƒì )
            if self.semantic_dedup:
                semantic_dup = self._check_semantic_duplicate(title, content, embedding)
                if semantic_dup:
                    self.stats["skipped_semantic"] += 1
                    logger.info(f"â­ï¸  Skipping duplicate article: {title[:80]}...")
                    logger.info(f"   Reason: Semantic duplicate with ID {semantic_dup.id}")
                    return None
            
            # Stage 5: ê¸°ì‚¬ ì €ìž¥ (ë¶„ì„ ì „)
            # Map raw_article fields to PostgreSQL NewsArticle model
            author_val = raw_article.get("author", [])
            if isinstance(author_val, list):
                author_val = ", ".join(author_val) if author_val else None

            news_article = NewsArticle(
                url=url,
                title=title,
                source=raw_article.get("source", ""),
                published_date=raw_article.get("published_date"),
                content=content,
                summary=raw_article.get("summary", ""),
                author=author_val,
                content_hash=content_hash,
                embedding=embedding,
                tags=raw_article.get("keywords", []),  # keywords -> tags
                metadata_={  # Store extra fields in JSONB metadata
                    "feed_source": raw_article.get("feed_source", "rss"),
                    "top_image": raw_article.get("top_image", ""),
                }
            )
            
            self.db.add(news_article)
            self.db.flush()  # ID ìƒì„±

            # Stage 5.5: GLM ë¶„ì„ (ì¢…ëª©/ì„¹í„° ì¶”ì¶œ) - ëª¨ë“  ë‰´ìŠ¤ ìˆ˜í–‰
            # Concurrency Control: Semaphore + Rate Limiting
            # - Semaphore: ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ (ê¸°ë³¸ 3ê°œ)
            # - Rate Limit: ìš”ì²­ ê°„ ì§€ì—° (ê¸°ë³¸ 3ì´ˆ)
            glm_analysis = None
            if self.llm_client:
                try:
                    # Semaphoreë¡œ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ (GLM Concurrency Limit ì´ˆê³¼ ë°©ì§€)
                    async with self.glm_semaphore:
                        news_text = f"{title}\n{content[:1000] if content else ''}"
                        glm_analysis = await self.llm_client.analyze_news(news_text)

                        # Rate Limiting: ë‹¤ìŒ ìš”ì²­ ì „ ì§€ì—°
                        await asyncio.sleep(self.glm_rate_limit)

                    if glm_analysis:
                        self.stats["glm_analyzed"] += 1

                        # DBì˜ glm_analysis ì»¬ëŸ¼ì— ì €ìž¥
                        # Note: NewsArticle ëª¨ë¸ì— glm_analysis ì»¬ëŸ¼ì´ ìžˆì–´ì•¼ í•¨
                        try:
                            from backend.database.models import NewsArticle as DBNewsArticle
                            from backend.database.repository import NewsRepository

                            repo = NewsRepository(session=self.db)
                            # ì´ë¯¸ flushëœ news_articleì˜ ID ì‚¬ìš©
                            repo.save_glm_analysis(news_article.id, glm_analysis)
                        except Exception as db_err:
                            logger.warning(f"GLM DB save failed: {db_err}")

                        logger.info(
                            f"[GLM] {title[:40]}... | "
                            f"Tickers: {glm_analysis.get('tickers', [])} | "
                            f"Confidence: {glm_analysis.get('confidence', 0):.2f}"
                        )
                except Exception as e:
                    logger.error(f"GLM analysis failed for {title[:50]}: {e}")

            # Stage 6: Deep Analysis (ì„ íƒì )
            analysis = None
            if self._should_analyze(raw_article):
                try:
                    analysis = self.analyzer.analyze_article(news_article)
                    if analysis:
                        self.stats["analyzed"] += 1
                except Exception as e:
                    logger.error(f"Analysis failed for {title[:50]}: {e}")

            # Commit
            self.db.commit()
            self.db.refresh(news_article)

            self.stats["saved"] += 1
            logger.info(f"âœ… Saved: {title[:50]}... (GLM: {glm_analysis is not None}, Deep: {analysis is not None})")

            return ProcessedNews(
                article=news_article,
                analysis=analysis,
                was_analyzed=analysis is not None,
                glm_analysis=glm_analysis
            )

        except Exception as e:
            self.db.rollback()
            self.stats["errors"] += 1
            logger.error(f"Failed to process article {title[:50]}: {e}")
            raise
    
    async def process_batch(
        self,
        raw_articles: List[Dict[str, Any]],
        max_concurrent: int = 5
    ) -> ProcessingResult:
        """
        ë°°ì¹˜ ì²˜ë¦¬
        
        Args:
            raw_articles: í¬ë¡¤ë§ëœ ì›ì‹œ ê¸°ì‚¬ ëª©ë¡
            max_concurrent: ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬ ìˆ˜
            
        Returns:
            ProcessingResult: ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        logger.info(f"Starting batch processing: {len(raw_articles)} articles")
        
        processed = []
        skipped = []
        errors = []
        
        # ìˆœì°¨ ì²˜ë¦¬ (DB íŠ¸ëžœìž­ì…˜ ë•Œë¬¸ì—)
        for i, article in enumerate(raw_articles, 1):
            try:
                result = await self.process_article(article)
                
                if result:
                    processed.append(result)
                else:
                    skipped.append({
                        "title": article.get("title", ""),
                        "url": article.get("url", "")
                    })
                
                # ì§„í–‰ ìƒí™© ë¡œê·¸
                if i % 10 == 0:
                    logger.info(f"Progress: {i}/{len(raw_articles)} articles processed")
                    
            except Exception as e:
                errors.append(e)
                logger.error(f"Error processing article {i}: {e}")
        
        # ìµœì¢… í†µê³„
        logger.info(f"""
Batch processing complete:
  Total: {self.stats['total']}
  Saved: {self.stats['saved']}
  Skipped (URL): {self.stats['skipped_url']}
  Skipped (Hash): {self.stats['skipped_hash']}
  Skipped (Semantic): {self.stats['skipped_semantic']}
  GLM Analyzed: {self.stats['glm_analyzed']}
  Deep Analyzed: {self.stats['analyzed']}
  Errors: {self.stats['errors']}
""")
        
        return ProcessingResult(
            processed=processed,
            skipped=skipped,
            errors=errors
        )
    
    def get_stats(self) -> Dict[str, int]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        return self.stats.copy()
