"""
RSS Crawler Service

Features:
- Ïã§ÏãúÍ∞Ñ RSS ÌîºÎìú ÌÅ¨Î°§ÎßÅ (ÏßÄÏó∞ ÏóÜÏùå)
- newspaper3kÎ°ú Î≥∏Î¨∏ Ï†ÑÏ≤¥ Ï∂îÏ∂ú
- Î¨¥Ï†úÌïú ÏöîÏ≤≠ (Î¨¥Î£å)
- Ï§ëÎ≥µ Ï†úÍ±∞ (URL Í∏∞Î∞ò)
"""

import asyncio
import feedparser
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
from pathlib import Path
import json
import time
from urllib.parse import urlparse

# newspaper3k for content extraction
try:
    from newspaper import Article, Config
    NEWSPAPER_AVAILABLE = True
except ImportError:
    NEWSPAPER_AVAILABLE = False
    Article = None
    Config = None

from sqlalchemy.orm import Session
from backend.data.news_models import NewsArticle, RSSFeed, SessionLocal, init_db


# ============================================================================
# Configuration
# ============================================================================

# Newspaper3k config
if NEWSPAPER_AVAILABLE:
    newspaper_config = Config()
    newspaper_config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    newspaper_config.request_timeout = 10
    newspaper_config.fetch_images = False  # Ïù¥ÎØ∏ÏßÄ Îã§Ïö¥Î°úÎìú ÎπÑÌôúÏÑ±Ìôî (ÏÜçÎèÑ Ìñ•ÏÉÅ)
    newspaper_config.memoize_articles = False


# ============================================================================
# RSS Crawler
# ============================================================================

class RSSCrawler:
    """
    ÏûêÏ≤¥ RSS ÌÅ¨Î°§Îü¨
    
    Features:
    - Ïã§ÏãúÍ∞Ñ Îâ¥Ïä§ ÏàòÏßë (ÏßÄÏó∞ ÏóÜÏùå)
    - Î≥∏Î¨∏ Ï†ÑÏ≤¥ Ï∂îÏ∂ú (newspaper3k)
    - Î¨¥Ï†úÌïú ÏöîÏ≤≠ (Î¨¥Î£å)
    - Ï§ëÎ≥µ Ï†úÍ±∞ (URL Í∏∞Î∞ò)
    """
    
    def __init__(self, db: Session):
        self.db = db
        self.stats = {
            "feeds_processed": 0,
            "articles_found": 0,
            "articles_new": 0,
            "articles_skipped": 0,
            "content_extracted": 0,
            "errors": []
        }
    
    def fetch_feed(self, feed_url: str, feed_name: str = "") -> List[Dict[str, Any]]:
        """RSS ÌîºÎìú ÌååÏã±"""
        try:
            feed = feedparser.parse(feed_url)
            
            if feed.bozo:  # ÌååÏã± ÏóêÎü¨
                self.stats["errors"].append({
                    "feed": feed_name,
                    "error": str(feed.bozo_exception)
                })
            
            articles = []
            for entry in feed.entries[:20]:  # ÏµúÏã† 20Í∞ú
                # Î∞úÌñâÏùº ÌååÏã±
                published = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    published = datetime(*entry.published_parsed[:6])
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    published = datetime(*entry.updated_parsed[:6])
                else:
                    published = datetime.utcnow()
                
                article = {
                    "title": entry.get("title", "").strip(),
                    "url": entry.get("link", "").strip(),
                    "summary": entry.get("summary", "").strip(),
                    "published_at": published,
                    "source": feed.feed.get("title", feed_name),
                    "feed_source": "rss",
                }
                
                if article["url"]:
                    articles.append(article)
            
            self.stats["articles_found"] += len(articles)
            return articles
            
        except Exception as e:
            self.stats["errors"].append({
                "feed": feed_name,
                "error": str(e)
            })
            return []
    
    def extract_full_content(self, url: str) -> Dict[str, Any]:
        """
        Îâ¥Ïä§ Î≥∏Î¨∏ Ï†ÑÏ≤¥ Ï∂îÏ∂ú (newspaper3k)
        
        Returns:
            {
                "title": str,
                "text": str,  # Ï†ÑÏ≤¥ Î≥∏Î¨∏
                "authors": List[str],
                "publish_date": datetime,
                "top_image": str,
                "keywords": List[str],
                "summary": str  # ÏûêÎèô ÏöîÏïΩ
            }
        """
        if not NEWSPAPER_AVAILABLE:
            return {"error": "newspaper3k not installed"}
        
        try:
            article = Article(url, config=newspaper_config)
            article.download()
            article.parse()
            
            # NLP Î∂ÑÏÑù (ÌÇ§ÏõåÎìú, ÏöîÏïΩ)
            try:
                article.nlp()
                keywords = article.keywords[:10] if article.keywords else []
                summary = article.summary if article.summary else ""
            except:
                keywords = []
                summary = ""
            
            self.stats["content_extracted"] += 1
            
            return {
                "title": article.title,
                "text": article.text,  # Ï†ÑÏ≤¥ Î≥∏Î¨∏
                "authors": article.authors or [],
                "publish_date": article.publish_date,
                "top_image": article.top_image or "",
                "keywords": keywords,
                "summary": summary,
            }
            
        except Exception as e:
            return {
                "error": str(e),
                "url": url,
                "text": "",
                "keywords": [],
                "summary": ""
            }
    
    def save_article(self, article_data: Dict[str, Any]) -> Optional[NewsArticle]:
        """Í∏∞ÏÇ¨ DB Ï†ÄÏû• (Ï§ëÎ≥µ Ï≤¥ÌÅ¨)"""
        url = article_data.get("url", "")
        if not url:
            return None
        
        # Ï§ëÎ≥µ Ï≤¥ÌÅ¨
        existing = self.db.query(NewsArticle).filter(NewsArticle.url == url).first()
        if existing:
            self.stats["articles_skipped"] += 1
            return existing
        
        # ÏÉà Í∏∞ÏÇ¨ Ï†ÄÏû•
        news_article = NewsArticle(
            url=url,
            title=article_data.get("title", ""),
            source=article_data.get("source", ""),
            feed_source=article_data.get("feed_source", "rss"),
            published_at=article_data.get("published_at"),
            content_text=article_data.get("text", ""),
            content_summary=article_data.get("summary", ""),
            keywords=article_data.get("keywords", []),
            authors=article_data.get("authors", []),
            top_image=article_data.get("top_image", ""),
        )
        
        self.db.add(news_article)
        self.db.commit()
        self.db.refresh(news_article)
        
        self.stats["articles_new"] += 1
        return news_article
    
    def crawl_feed(self, feed: RSSFeed, extract_content: bool = True) -> List[NewsArticle]:
        """Îã®Ïùº ÌîºÎìú ÌÅ¨Î°§ÎßÅ"""
        articles = self.fetch_feed(feed.url, feed.name)
        saved_articles = []
        
        for article_data in articles:
            # Î≥∏Î¨∏ Ï∂îÏ∂ú
            if extract_content and article_data.get("url"):
                full_content = self.extract_full_content(article_data["url"])
                article_data.update(full_content)
            
            # DB Ï†ÄÏû•
            saved = self.save_article(article_data)
            if saved and saved.id:
                saved_articles.append(saved)
        
        # ÌîºÎìú ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
        feed.last_fetched = datetime.utcnow()
        feed.total_articles += len(saved_articles)
        self.db.commit()
        
        self.stats["feeds_processed"] += 1
        return saved_articles
    
    def crawl_all_feeds(self, extract_content: bool = True) -> Dict[str, Any]:
        """Î™®Îì† ÌôúÏÑ±ÌôîÎêú ÌîºÎìú ÌÅ¨Î°§ÎßÅ"""
        feeds = self.db.query(RSSFeed).filter(RSSFeed.enabled == True).all()
        
        all_articles = []
        for feed in feeds:
            print(f"üì∞ Crawling: {feed.name}...")
            articles = self.crawl_feed(feed, extract_content)
            all_articles.extend(articles)
            time.sleep(0.5)  # Rate limiting (ÏòàÏùò)
        
        return {
            "total_articles": len(all_articles),
            "stats": self.stats,
            "timestamp": datetime.utcnow().isoformat()
        }
    
    def crawl_ticker_news(self, ticker: str) -> List[NewsArticle]:
        """ÌäπÏ†ï Ìã∞Ïª§ Í¥ÄÎ†® Îâ¥Ïä§ (Yahoo Finance RSS)"""
        yahoo_url = f"https://finance.yahoo.com/rss/headline?s={ticker}"
        
        articles = self.fetch_feed(yahoo_url, f"Yahoo Finance - {ticker}")
        saved_articles = []
        
        for article_data in articles:
            full_content = self.extract_full_content(article_data["url"])
            article_data.update(full_content)
            article_data["source"] = f"Yahoo Finance ({ticker})"
            
            saved = self.save_article(article_data)
            if saved:
                saved_articles.append(saved)
        
        return saved_articles


# ============================================================================
# Utility Functions
# ============================================================================

def get_recent_articles(
    db: Session,
    limit: int = 50,
    hours: int = 24,
    source: Optional[str] = None
) -> List[NewsArticle]:
    """ÏµúÍ∑º Í∏∞ÏÇ¨ Ï°∞Ìöå"""
    cutoff = datetime.utcnow() - timedelta(hours=hours)
    
    query = db.query(NewsArticle).filter(NewsArticle.published_at >= cutoff)
    
    if source:
        query = query.filter(NewsArticle.source.ilike(f"%{source}%"))
    
    return query.order_by(NewsArticle.published_at.desc()).limit(limit).all()


def get_unanalyzed_articles(db: Session, limit: int = 100) -> List[NewsArticle]:
    """Î∂ÑÏÑùÎêòÏßÄ ÏïäÏùÄ Í∏∞ÏÇ¨ Ï°∞Ìöå"""
    return (
        db.query(NewsArticle)
        .outerjoin(NewsArticle.analysis)
        .filter(NewsArticle.analysis == None)
        .filter(NewsArticle.content_text != None)
        .filter(NewsArticle.content_text != "")
        .order_by(NewsArticle.published_at.desc())
        .limit(limit)
        .all()
    )


def get_feed_stats(db: Session) -> List[Dict[str, Any]]:
    """ÌîºÎìúÎ≥Ñ ÌÜµÍ≥Ñ"""
    feeds = db.query(RSSFeed).all()
    return [
        {
            "id": f.id,
            "name": f.name,
            "url": f.url,
            "category": f.category,
            "enabled": f.enabled,
            "last_fetched": f.last_fetched.isoformat() if f.last_fetched else None,
            "total_articles": f.total_articles,
            "error_count": f.error_count,
        }
        for f in feeds
    ]


# ============================================================================
# CLI for testing
# ============================================================================

if __name__ == "__main__":
    print("üöÄ RSS Crawler Test")
    
    # Initialize DB
    init_db()
    
    # Create session
    db = SessionLocal()
    
    try:
        # Create crawler
        crawler = RSSCrawler(db)
        
        # Crawl all feeds
        print("\nüì° Crawling all RSS feeds...")
        result = crawler.crawl_all_feeds(extract_content=True)
        
        print(f"\n‚úÖ Crawling Complete!")
        print(f"  Feeds processed: {result['stats']['feeds_processed']}")
        print(f"  Articles found: {result['stats']['articles_found']}")
        print(f"  New articles: {result['stats']['articles_new']}")
        print(f"  Skipped (duplicate): {result['stats']['articles_skipped']}")
        print(f"  Content extracted: {result['stats']['content_extracted']}")
        
        if result['stats']['errors']:
            print(f"\n‚ö†Ô∏è Errors:")
            for err in result['stats']['errors']:
                print(f"  - {err['feed']}: {err['error']}")
        
        # Show recent articles
        print("\nüì∞ Recent Articles:")
        recent = get_recent_articles(db, limit=5)
        for article in recent:
            print(f"  - {article.title[:60]}...")
            print(f"    Source: {article.source}")
            print(f"    Content: {len(article.content_text or '')} chars")
            print()
        
    finally:
        db.close()
