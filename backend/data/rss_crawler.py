"""
RSS Crawler Service

Features:
- ì‹¤ì‹œê°„ RSS í”¼ë“œ í¬ë¡¤ë§ (ì§€ì—° ì—†ìŒ)
- newspaper3kë¡œ ë³¸ë¬¸ ì „ì²´ ì¶”ì¶œ
- ë¬´ì œí•œ ìš”ì²­ (ë¬´ë£Œ)
- ì¤‘ë³µ ì œê±° (URL ê¸°ë°˜)
"""

import asyncio
import logging
import feedparser
import hashlib
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
from pathlib import Path
import json
import time
from urllib.parse import urlparse

# newspaper3k for content extraction
try:
    from newspaper import Article, Config
    NEWSPAPER_AVAILABLE = True
except ImportError:
    NEWSPAPER_AVAILABLE = False
    Article = None
    Config = None

from sqlalchemy.orm import Session
# Use PostgreSQL models instead of SQLite
from backend.database.models import NewsArticle, RSSFeed

logger = logging.getLogger(__name__)


# ============================================================================
# Utility Functions
# ============================================================================

def generate_content_hash(title: str, content: str) -> str:
    """
    ì½˜í…ì¸  í•´ì‹œ ìƒì„± (ì¤‘ë³µ ê°ì§€ìš©)
    
    ì œëª© + ë³¸ë¬¸ì˜ ì²˜ìŒ 1000ìë¡œ SHA256 í•´ì‹œ ìƒì„±
    ê°™ì€ ë‚´ìš©ì˜ ë‹¤ë¥¸ URL ê¸°ì‚¬ë„ ì¤‘ë³µìœ¼ë¡œ ê°ì§€ ê°€ëŠ¥
    """
    # ì œëª©ê³¼ ë³¸ë¬¸ ê²°í•© (ë³¸ë¬¸ì€ ì²« 1000ìë§Œ)
    text = f"{title.strip()}\n{content.strip()[:1000]}"
    return hashlib.sha256(text.encode('utf-8')).hexdigest()


# ============================================================================
# Configuration
# ============================================================================

# Newspaper3k config
if NEWSPAPER_AVAILABLE:
    newspaper_config = Config()
    newspaper_config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    newspaper_config.request_timeout = 10
    newspaper_config.fetch_images = False  # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
    newspaper_config.memoize_articles = False


# ============================================================================
# RSS Crawler
# ============================================================================

class RSSCrawler:
    """
    ìì²´ RSS í¬ë¡¤ëŸ¬
    
    Features:
    - ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì§€ì—° ì—†ìŒ)
    - ë³¸ë¬¸ ì „ì²´ ì¶”ì¶œ (newspaper3k)
    - ë¬´ì œí•œ ìš”ì²­ (ë¬´ë£Œ)
    - ì¤‘ë³µ ì œê±° (URL ê¸°ë°˜)
    """
    
    def __init__(self, db: Session):
        self.db = db
        self.stats = {
            "feeds_processed": 0,
            "articles_found": 0,
            "articles_new": 0,
            "articles_skipped": 0,
            "content_extracted": 0,
            "errors": []
        }
    
    def fetch_feed(self, feed_url: str, feed_name: str = "") -> List[Dict[str, Any]]:
        """RSS í”¼ë“œ íŒŒì‹±"""
        try:
            feed = feedparser.parse(feed_url)
            
            if feed.bozo:  # íŒŒì‹± ì—ëŸ¬
                self.stats["errors"].append({
                    "feed": feed_name,
                    "error": str(feed.bozo_exception)
                })
            
            articles = []
            for entry in feed.entries[:20]:  # ìµœì‹  20ê°œ
                # ë°œí–‰ì¼ íŒŒì‹±
                published = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    published = datetime(*entry.published_parsed[:6])
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    published = datetime(*entry.updated_parsed[:6])
                else:
                    published = datetime.utcnow()
                
                article = {
                    "title": entry.get("title", "").strip(),
                    "url": entry.get("link", "").strip(),
                    "summary": entry.get("summary", "").strip(),
                    "published_date": published,
                    "source": feed.feed.get("title", feed_name),
                    "feed_source": "rss",
                }
                
                if article["url"]:
                    articles.append(article)
            
            self.stats["articles_found"] += len(articles)
            return articles
            
        except Exception as e:
            self.stats["errors"].append({
                "feed": feed_name,
                "error": str(e)
            })
            return []
    


    def extract_full_content(self, url: str) -> Dict[str, Any]:
        """
        ë‰´ìŠ¤ ë³¸ë¬¸ ì „ì²´ ì¶”ì¶œ (newspaper3k)
        
        Returns:
            {
                "title": str,
                "text": str,  # ì „ì²´ ë³¸ë¬¸
                "authors": List[str],
                "publish_date": datetime,
                "top_image": str,
                "keywords": List[str],
                "summary": str  # ìë™ ìš”ì•½
            }
        """
        if not NEWSPAPER_AVAILABLE:
            return {"error": "newspaper3k not installed"}
        
        try:
            article = Article(url, config=newspaper_config)
            article.download()
            article.parse()
            
            # NLP ë¶„ì„ (í‚¤ì›Œë“œ, ìš”ì•½)
            try:
                article.nlp()
                keywords = article.keywords[:10] if article.keywords else []
                summary = article.summary if article.summary else ""
            except:
                keywords = []
                summary = ""
            
            self.stats["content_extracted"] += 1
            
            return {
                "title": article.title,
                "content": article.text,  # ì „ì²´ ë³¸ë¬¸
                "author": article.authors or [],
                "published_date": article.publish_date,
                "top_image": article.top_image or "",
                "keywords": keywords,
                "summary": summary,
            }
            
        except Exception as e:
            return {
                "error": str(e),
                "url": url,
                "text": "",
                "keywords": [],
                "summary": ""
            }
    
    def save_article(self, article_data: Dict[str, Any]) -> Optional[NewsArticle]:
        """ê¸°ì‚¬ DB ì €ì¥ (ê°œì„ ëœ ì¤‘ë³µ ì²´í¬)"""
        url = article_data.get("url", "")
        title = article_data.get("title", "")
        content = article_data.get("content", "")
        
        if not url or not title:
            return None
        
        # 1. URL ì¤‘ë³µ ì²´í¬ (ê°€ì¥ ë¹ ë¦„)
        existing = self.db.query(NewsArticle).filter(NewsArticle.url == url).first()
        if existing:
            self.stats["articles_skipped"] += 1
            logger.debug(f"Skipped (URL duplicate): {title[:50]}...")
            return existing
        
        # 2. Content Hash ì¤‘ë³µ ì²´í¬ (ë‚´ìš© ê¸°ë°˜)
        content_hash = None
        if content and len(content) > 50:  # ë³¸ë¬¸ì´ ì¶©ë¶„íˆ ê¸´ ê²½ìš°ë§Œ
            content_hash = generate_content_hash(title, content)
            
            existing_by_hash = self.db.query(NewsArticle).filter(
                NewsArticle.content_hash == content_hash
            ).first()
            
            if existing_by_hash:
                self.stats["articles_skipped"] += 1
                logger.info(f"âœ“ Skipped (Content duplicate): {title[:50]}... (different URL!)")
                return existing_by_hash
        
        # 3. ìƒˆ ê¸°ì‚¬ ì €ì¥
        news_article = NewsArticle(
            url=url,
            title=title,
            source=article_data.get("source", ""),
            feed_source=article_data.get("feed_source", "rss"),
            published_date=article_data.get("published_date"),
            content=content,
            summary=article_data.get("summary", ""),
            keywords=article_data.get("keywords", []),
            author=article_data.get("author", []),
            top_image=article_data.get("top_image", ""),
            content_hash=content_hash,  # âœ… í•´ì‹œ ì €ì¥
        )
        
        self.db.add(news_article)
        self.db.commit()
        self.db.refresh(news_article)
        
        self.stats["articles_new"] += 1
        logger.info(f"âœ… New article saved: {title[:50]}...")
        return news_article
    
    def crawl_feed(self, feed: RSSFeed, extract_content: bool = True) -> List[NewsArticle]:
        """ë‹¨ì¼ í”¼ë“œ í¬ë¡¤ë§"""
        articles = self.fetch_feed(feed.url, feed.name)
        saved_articles = []
        
        for article_data in articles:
            # ë³¸ë¬¸ ì¶”ì¶œ
            if extract_content and article_data.get("url"):
                full_content = self.extract_full_content(article_data["url"])
                article_data.update(full_content)
            
            # DB ì €ì¥
            saved = self.save_article(article_data)
            if saved and saved.id:
                saved_articles.append(saved)
        
        # í”¼ë“œ í†µê³„ ì—…ë°ì´íŠ¸
        feed.last_fetched = datetime.utcnow()
        feed.total_articles += len(saved_articles)
        self.db.commit()
        
        self.stats["feeds_processed"] += 1
        return saved_articles
    
    def crawl_all_feeds(self, extract_content: bool = True) -> List[NewsArticle]:
        """ëª¨ë“  í™œì„±í™”ëœ í”¼ë“œ í¬ë¡¤ë§"""
        feeds = self.db.query(RSSFeed).filter(RSSFeed.enabled == True).all()
        
        all_articles = []
        for feed in feeds:
            print(f"ğŸ“° Crawling: {feed.name}...")
            articles = self.crawl_feed(feed, extract_content)
            all_articles.extend(articles)
            time.sleep(0.5)  # Rate limiting (ì˜ˆì˜)
        
        return all_articles
    
    def crawl_ticker_news(self, ticker: str) -> List[NewsArticle]:
        """íŠ¹ì • í‹°ì»¤ ê´€ë ¨ ë‰´ìŠ¤ (Yahoo Finance RSS)"""
        yahoo_url = f"https://finance.yahoo.com/rss/headline?s={ticker}"
        
        articles = self.fetch_feed(yahoo_url, f"Yahoo Finance - {ticker}")
        saved_articles = []
        
        for article_data in articles:
            full_content = self.extract_full_content(article_data["url"])
            article_data.update(full_content)
            article_data["source"] = f"Yahoo Finance ({ticker})"
            
            saved = self.save_article(article_data)
            if saved:
                saved_articles.append(saved)
        
        return saved_articles
    
    def fetch_all_feeds(self, extract_content: bool = True) -> List[Dict[str, Any]]:
        """
        ëª¨ë“  RSS í”¼ë“œ í¬ë¡¤ë§ (DB ì €ì¥ ì•ˆí•¨)
        
        UnifiedNewsProcessorì™€ í•¨ê»˜ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë©”ì„œë“œ
        ì›ì‹œ ê¸°ì‚¬ ë°ì´í„°ë§Œ ë°˜í™˜í•˜ê³  DB ì €ì¥ì€ í•˜ì§€ ì•ŠìŒ
        
        Returns:
            List[Dict]: í¬ë¡¤ë§ëœ ì›ì‹œ ê¸°ì‚¬ ëª©ë¡
        """
        feeds = self.db.query(RSSFeed).filter(RSSFeed.enabled == True).all()
        
        all_raw_articles = []
        for feed in feeds:
            logger.info(f"ğŸ“¡ Fetching: {feed.name}...")
            
            # RSS í”¼ë“œ íŒŒì‹±
            articles = self.fetch_feed(feed.url, feed.name)
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            for article_data in articles:
                if extract_content and article_data.get("url"):
                    full_content = self.extract_full_content(article_data["url"])
                    article_data.update(full_content)
                
                all_raw_articles.append(article_data)
            
            # í”¼ë“œ í†µê³„ ì—…ë°ì´íŠ¸ (ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì‹œê°„ë§Œ)
            feed.last_fetched = datetime.utcnow()
            self.db.commit()
            
            time.sleep(0.5)  # Rate limiting
            
            self.stats["feeds_processed"] += 1
        
        logger.info(f"âœ… Fetched {len(all_raw_articles)} raw articles from {len(feeds)} feeds")
        return all_raw_articles


# ============================================================================
# Utility Functions
# ============================================================================

async def get_recent_articles(
    db: Session,
    limit: int = 50,
    hours: int = 24,
    source: Optional[str] = None
) -> List[NewsArticle]:
    """
    ìµœê·¼ ê¸°ì‚¬ ì¡°íšŒ (ë¹„ë™ê¸°)

    AsyncSessionì„ ì‚¬ìš©í•˜ë¯€ë¡œ select() ë¬¸ë²• ì‚¬ìš©
    """
    from sqlalchemy import select, func

    cutoff = datetime.utcnow() - timedelta(hours=hours)

    # select() ë¬¸ë²•ìœ¼ë¡œ ì¿¼ë¦¬ ìƒì„±
    stmt = select(NewsArticle).filter(NewsArticle.published_date >= cutoff)

    if source:
        stmt = stmt.filter(NewsArticle.source.ilike(f"%{source}%"))

    stmt = stmt.order_by(NewsArticle.published_date.desc()).limit(limit)

    # ë¹„ë™ê¸° ì‹¤í–‰
    result = await db.execute(stmt)
    return result.scalars().all()


async def get_unanalyzed_articles(db: Session, limit: int = 100) -> List[NewsArticle]:
    """
    ë¶„ì„ë˜ì§€ ì•Šì€ ê¸°ì‚¬ ì¡°íšŒ (ë¹„ë™ê¸°)

    AsyncSessionì„ ì‚¬ìš©í•˜ë¯€ë¡œ select() ë¬¸ë²• ì‚¬ìš©
    """
    from sqlalchemy import select

    stmt = (
        select(NewsArticle)
        .outerjoin(NewsArticle.analysis)
        .filter(NewsArticle.analysis == None)
        .filter(NewsArticle.content != None)
        .filter(NewsArticle.content != "")
        .order_by(NewsArticle.published_date.desc())
        .limit(limit)
    )

    result = await db.execute(stmt)
    return result.scalars().all()


async def get_feed_stats(db: Session) -> List[Dict[str, Any]]:
    """
    í”¼ë“œë³„ í†µê³„ (ë¹„ë™ê¸°)

    AsyncSessionì„ ì‚¬ìš©í•˜ë¯€ë¡œ select() ë¬¸ë²• ì‚¬ìš©
    """
    from sqlalchemy import select

    stmt = select(RSSFeed)
    result = await db.execute(stmt)
    feeds = result.scalars().all()

    return [
        {
            "id": f.id,
            "name": f.name,
            "url": f.url,
            "category": f.category,
            "enabled": f.enabled,
            "last_fetched": f.last_fetched.isoformat() if f.last_fetched else None,
            "total_articles": f.total_articles,
            "error_count": f.error_count,
        }
        for f in feeds
    ]


# ============================================================================
# CLI for testing
# ============================================================================

if __name__ == "__main__":
    print("ğŸš€ RSS Crawler Test")
    
    # Initialize DB
    init_db()
    
    # Create session
    db = SessionLocal()
    
    try:
        # Create crawler
        crawler = RSSCrawler(db)
        
        # Crawl all feeds
        print("\nğŸ“¡ Crawling all RSS feeds...")
        result = crawler.crawl_all_feeds(extract_content=True)
        
        print(f"\nâœ… Crawling Complete!")
        print(f"  Feeds processed: {result['stats']['feeds_processed']}")
        print(f"  Articles found: {result['stats']['articles_found']}")
        print(f"  New articles: {result['stats']['articles_new']}")
        print(f"  Skipped (duplicate): {result['stats']['articles_skipped']}")
        print(f"  Content extracted: {result['stats']['content_extracted']}")
        
        if result['stats']['errors']:
            print(f"\nâš ï¸ Errors:")
            for err in result['stats']['errors']:
                print(f"  - {err['feed']}: {err['error']}")
        
        # Show recent articles
        print("\nğŸ“° Recent Articles:")
        recent = get_recent_articles(db, limit=5)
        for article in recent:
            print(f"  - {article.title[:60]}...")
            print(f"    Source: {article.source}")
            print(f"    Content: {len(article.content_text or '')} chars")
            print()
        
    finally:
        db.close()
