"""
Accountability Scheduler - Automated News Interpretation Accuracy Tracking

1ì‹œê°„ë§ˆë‹¤ ìžë™ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ ë‰´ìŠ¤ í•´ì„ì˜ ì •í™•ë„ë¥¼ ì¶”ì í•©ë‹ˆë‹¤.

Key Features:
- 1ì‹œê°„ë§ˆë‹¤ ìžë™ ì‹¤í–‰ (ë§¤ì‹œ ì •ê°)
- 1h/1d/3d time horizon ê²€ì¦
- NIA (News Interpretation Accuracy) ìžë™ ê³„ì‚°
- Failure Learning Agent íŠ¸ë¦¬ê±° (í‹€ë¦° íŒë‹¨ ë°œê²¬ ì‹œ)

Integration:
- Daily Learning Schedulerì™€ í†µí•© ê°€ëŠ¥
- FastAPI ì„œë²„ì™€ ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥

Author: AI Trading System
Date: 2025-12-30
Phase: Accountability System (Phase 26)
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, Optional

from backend.automation.price_tracking_verifier import PriceTrackingVerifier

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class AccountabilityScheduler:
    """
    Accountability ì‹œìŠ¤í…œ ìžë™í™” ìŠ¤ì¼€ì¤„ëŸ¬

    ë§¤ì‹œê°„ ì‹¤í–‰ë˜ì–´ ë‰´ìŠ¤ í•´ì„ í›„ 1h/1d/3d ê°€ê²© ë³€í™”ë¥¼ ì¶”ì í•˜ê³ 
    AI í•´ì„ì˜ ì •í™•ë„ë¥¼ ìžë™ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤.

    Example:
        scheduler = AccountabilityScheduler()
        await scheduler.start()  # Runs indefinitely
    """

    def __init__(
        self,
        run_interval_minutes: int = 60,  # 1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰
        retry_on_failure: bool = True,
        max_retries: int = 3,
        trigger_failure_learning: bool = True  # í‹€ë¦° íŒë‹¨ ë°œê²¬ ì‹œ Failure Learning Agent íŠ¸ë¦¬ê±°
    ):
        """
        Initialize scheduler.

        Args:
            run_interval_minutes: ì‹¤í–‰ ê°„ê²© (ë¶„) - ê¸°ë³¸ 60ë¶„ (1ì‹œê°„)
            retry_on_failure: ì‹¤íŒ¨ ì‹œ ìž¬ì‹œë„ ì—¬ë¶€
            max_retries: ìµœëŒ€ ìž¬ì‹œë„ íšŸìˆ˜
            trigger_failure_learning: í‹€ë¦° íŒë‹¨ ë°œê²¬ ì‹œ Failure Learning Agent íŠ¸ë¦¬ê±° ì—¬ë¶€
        """
        self.run_interval_minutes = run_interval_minutes
        self.retry_on_failure = retry_on_failure
        self.max_retries = max_retries
        self.trigger_failure_learning = trigger_failure_learning

        self.verifier = PriceTrackingVerifier()
        self.is_running = False

        logger.info(
            f"AccountabilityScheduler initialized: "
            f"interval={run_interval_minutes}min, "
            f"retry={retry_on_failure}, "
            f"max_retries={max_retries}, "
            f"failure_learning={trigger_failure_learning}"
        )

    async def start(self):
        """
        Start the scheduler (runs indefinitely).

        ë§¤ì‹œê°„ ì •ê°ì— ì‹¤í–‰ë˜ë„ë¡ ì¡°ì •ë©ë‹ˆë‹¤.
        ì˜ˆ: 03:00, 04:00, 05:00...

        This should be run in a background task:
            asyncio.create_task(scheduler.start())

        Example:
            >>> scheduler = AccountabilityScheduler()
            >>> await scheduler.start()  # Blocks until stopped
        """
        self.is_running = True
        logger.info(f"â° Accountability scheduler started (every {self.run_interval_minutes} min)")

        while self.is_running:
            # Calculate time until next run (next hour)
            now = datetime.now()

            # Next hour at 00 minutes
            next_run = now.replace(minute=0, second=0, microsecond=0) + timedelta(hours=1)
            wait_seconds = (next_run - now).total_seconds()

            logger.info(f"â° Next accountability verification: {next_run.strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"â±ï¸  Waiting {wait_seconds/60:.1f} minutes...")

            # Wait until next run time
            await asyncio.sleep(wait_seconds)

            # Run verification
            if self.is_running:
                await self._execute_with_retry()

    async def _execute_with_retry(self):
        """Execute accountability verification with retry logic (non-blocking)."""
        for attempt in range(1, self.max_retries + 1):
            try:
                logger.info(f"ðŸš€ Executing accountability verification (attempt {attempt}/{self.max_retries})")
                logger.info("âš¡ Running in background - main server remains responsive")

                # Execute all horizon verifications (1h, 1d, 3d)
                results = await self.verifier.verify_all_horizons()

                # Log results
                logger.info("=" * 60)
                logger.info("âœ… Accountability Verification Complete")
                logger.info("=" * 60)

                total_verified = 0
                total_correct = 0
                failed_interpretations = []

                for horizon, result in results.items():
                    logger.info(
                        f"{horizon}: {result['correct_count']}/{result['verified_count']} correct "
                        f"({result['accuracy']*100:.1f}%)"
                    )
                    total_verified += result['verified_count']
                    total_correct += result['correct_count']

                    # Track failed interpretations (for 1d only - main metric)
                    if horizon == "1d" and result['verified_count'] > 0:
                        failed_count = result['verified_count'] - result['correct_count']
                        if failed_count > 0:
                            logger.warning(f"âš ï¸  {failed_count} failed interpretations detected on 1d horizon")
                            # TODO: Query DB for failed interpretation IDs

                # Calculate overall NIA (News Interpretation Accuracy)
                if total_verified > 0:
                    nia = (total_correct / total_verified) * 100
                    logger.info(f"ðŸ“Š Overall NIA (News Interpretation Accuracy): {nia:.1f}%")

                    # Alert if NIA is below threshold
                    if nia < 50.0:
                        logger.warning(f"âš ï¸  NIA below threshold (50%): {nia:.1f}%")
                else:
                    logger.info("ðŸ“Š No interpretations to verify at this time")

                logger.info("=" * 60)

                # Trigger Failure Learning Agent if enabled and failures detected
                if self.trigger_failure_learning and total_verified > 0 and total_correct < total_verified:
                    failed_count = total_verified - total_correct
                    logger.info(f"ðŸ” Triggering Failure Learning Agent for {failed_count} failed interpretations...")

                    # TODO: Implement Failure Learning Agent trigger
                    # await self._trigger_failure_learning(failed_interpretations)

                # Success - exit retry loop
                break

            except Exception as e:
                logger.error(f"âŒ Accountability verification failed (attempt {attempt}): {str(e)}", exc_info=True)

                if attempt < self.max_retries and self.retry_on_failure:
                    wait_minutes = attempt * 5  # Exponential backoff: 5, 10, 15 min
                    logger.info(f"â³ Retrying in {wait_minutes} minutes...")
                    await asyncio.sleep(wait_minutes * 60)
                else:
                    logger.error("âŒ Max retries exceeded. Accountability verification skipped for this hour.")
                    break

    async def _trigger_failure_learning(self, failed_interpretation_ids: list):
        """
        Trigger Failure Learning Agent for failed interpretations.

        Args:
            failed_interpretation_ids: List of NewsInterpretation IDs that were incorrect
        """
        # TODO: Implement Failure Learning Agent integration
        logger.info(f"ðŸ§  Failure Learning Agent would analyze {len(failed_interpretation_ids)} failures")
        pass

    def stop(self):
        """Stop the scheduler."""
        self.is_running = False
        logger.info("ðŸ›‘ Accountability scheduler stopped")

    async def run_once(self):
        """
        Run accountability verification once (for testing).

        Example:
            >>> scheduler = AccountabilityScheduler()
            >>> results = await scheduler.run_once()
        """
        logger.info("ðŸ§ª Running single accountability verification (test mode)")
        results = await self.verifier.verify_all_horizons()

        # Calculate NIA
        total_verified = sum(r['verified_count'] for r in results.values())
        total_correct = sum(r['correct_count'] for r in results.values())

        if total_verified > 0:
            nia = (total_correct / total_verified) * 100
            results['nia'] = nia
        else:
            results['nia'] = 0.0

        return results


# Example usage
if __name__ == "__main__":

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("ðŸ§ª Testing AccountabilityScheduler\n")

    async def test_scheduler():
        scheduler = AccountabilityScheduler(run_interval_minutes=60)

        # Run once for testing
        print("Running single accountability verification...")
        results = await scheduler.run_once()

        print(f"\nResults:")
        for horizon, result in results.items():
            if horizon != 'nia':
                print(f"{horizon}: {result['verified_count']} verified, {result['correct_count']} correct ({result['accuracy']*100:.1f}%)")

        if 'nia' in results:
            print(f"\nOverall NIA: {results['nia']:.1f}%")

    # Run test
    asyncio.run(test_scheduler())
