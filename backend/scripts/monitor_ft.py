"""
Financial Times Stealth Monitor

ë°±ì•…ê´€ ì—°ì„¤ ë“± ì¤‘ìš” ë‰´ìŠ¤ë¥¼ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

ì‚¬ìš©ë²•:
    # ë‹¨ì¼ URL ëª¨ë‹ˆí„°ë§
    python backend/scripts/monitor_ft.py

    # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (Windows)
    start /B python backend/scripts/monitor_ft.py

    # ë¡œê·¸ íŒŒì¼ë¡œ ì¶œë ¥
    python backend/scripts/monitor_ft.py > logs/ft_monitor.log 2>&1

ìž‘ì„±ì¼: 2026-01-21
"""

import os
import sys
import asyncio
import logging
from datetime import datetime
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.insert(0, project_root)

load_dotenv()

from backend.data.collectors.stealth_web_crawler import StealthWebCrawler, MultiSiteMonitor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/ft_monitor.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


def on_new_content(data: dict):
    """
    ìƒˆ ì½˜í…ì¸  ë°œê²¬ ì‹œ í˜¸ì¶œë˜ëŠ” ì½œë°±

    ì—¬ê¸°ì„œ ì¶”ê°€ ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
    - AI ë¶„ì„ íŠ¸ë¦¬ê±°
    - ì•Œë¦¼ ì „ì†¡
    - íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ ìƒì„± ë“±
    """
    logger.info("=" * 70)
    logger.info("ðŸ”” NEW CONTENT DETECTED!")
    logger.info("=" * 70)
    logger.info(f"Title: {data['title']}")
    logger.info(f"Content Length: {len(data['content'])} chars")
    logger.info(f"Hash: {data['content_hash'][:16]}...")
    logger.info("=" * 70)

    # TODO: ì—¬ê¸°ì„œ AI ë¶„ì„ íŒŒì´í”„ë¼ì¸ íŠ¸ë¦¬ê±°
    # from backend.ai.intelligence.enhanced_news_pipeline import EnhancedNewsPipeline
    # pipeline = EnhancedNewsPipeline()
    # await pipeline.process_urgent_news(data)


async def monitor_single_url():
    """ë‹¨ì¼ URL ëª¨ë‹ˆí„°ë§"""
    logger.info("=" * 70)
    logger.info("FT Stealth Monitor Starting...")
    logger.info("=" * 70)

    # ë°±ì•…ê´€ ì—°ì„¤ ê´€ë ¨ FT ê¸°ì‚¬
    url = "https://www.ft.com/content/1369a45e-e39b-4aaa-a347-b1800da7fd31"

    crawler = StealthWebCrawler(
        url=url,
        interval_minutes=3.0,      # 3ë¶„ ê°„ê²©
        variance_minutes=0.5,      # Â±30ì´ˆ ëžœë¤
        callback=on_new_content
    )

    logger.info(f"Monitoring URL: {url}")
    logger.info(f"Interval: 3Â±0.5 minutes (stealth mode)")
    logger.info(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 70)
    logger.info("\nPress Ctrl+C to stop\n")

    try:
        await crawler.start_monitoring()
    except KeyboardInterrupt:
        logger.info("\n\nâ¹ï¸  Monitoring stopped by user")
        crawler.stop_monitoring()
    except Exception as e:
        logger.error(f"Error: {e}", exc_info=True)
        crawler.stop_monitoring()


async def monitor_multiple_urls():
    """ì—¬ëŸ¬ URL ë™ì‹œ ëª¨ë‹ˆí„°ë§"""
    logger.info("=" * 70)
    logger.info("Multi-Site Stealth Monitor Starting...")
    logger.info("=" * 70)

    monitor = MultiSiteMonitor()

    # ë°±ì•…ê´€ ì—°ì„¤ ê´€ë ¨ FT ê¸°ì‚¬
    monitor.add_site(
        url="https://www.ft.com/content/1369a45e-e39b-4aaa-a347-b1800da7fd31",
        interval_minutes=3.0,
        variance_minutes=0.5,
        callback=on_new_content
    )

    # í•„ìš”ì‹œ ë‹¤ë¥¸ URL ì¶”ê°€
    # monitor.add_site(
    #     url="https://www.reuters.com/...",
    #     interval_minutes=5.0,
    #     callback=on_new_content
    # )

    logger.info(f"Monitoring {len(monitor.crawlers)} sites")
    logger.info(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 70)
    logger.info("\nPress Ctrl+C to stop\n")

    try:
        await monitor.start_all()
    except KeyboardInterrupt:
        logger.info("\n\nâ¹ï¸  All monitoring stopped by user")
        monitor.stop_all()
    except Exception as e:
        logger.error(f"Error: {e}", exc_info=True)
        monitor.stop_all()


if __name__ == "__main__":
    # logs ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('logs', exist_ok=True)

    # ë‹¨ì¼ URL ëª¨ë‹ˆí„°ë§ (ê¸°ë³¸)
    if len(sys.argv) > 1 and sys.argv[1] == 'multi':
        asyncio.run(monitor_multiple_urls())
    else:
        asyncio.run(monitor_single_url())
