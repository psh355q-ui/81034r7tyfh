# MVP Architecture - Deep Dive

**Version**: 1.0
**Last Updated**: 2026-01-04
**Author**: AI Trading System Development Team
**Status**: âœ… Production Ready

---

## ğŸ“‹ Executive Summary

**MVP ì „í™˜ ë°°ê²½**: 2025-12-31, Legacy 8-Agent War Room ì‹œìŠ¤í…œì„ **3+1 MVP Agent** êµ¬ì¡°ë¡œ í†µí•©í•˜ì—¬ **ë¹„ìš© 67% ì ˆê°**, **ì†ë„ 67% í–¥ìƒ** (30s â†’ 10s), **API í˜¸ì¶œ 62.5% ê°ì†Œ** (8íšŒ â†’ 3íšŒ)ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

**í•µì‹¬ ì² í•™**:
- **Attack (35%)**: Trader MVP - ê³µê²©ì  ê¸°íšŒ í¬ì°©
- **Defense (35%)**: Risk MVP - ë°©ì–´ì  ë¦¬ìŠ¤í¬ ê´€ë¦¬ + Position Sizing
- **Information (30%)**: Analyst MVP - ì¢…í•© ì •ë³´ ë¶„ì„
- **Final Decision**: PM Agent MVP - Hard Rules ê²€ì¦ + ìµœì¢… ìŠ¹ì¸

**Shadow Trading**: 2026-01-01ë¶€í„° 3ê°œì›” ê²€ì¦ ì§„í–‰ ì¤‘ (Day 4/90, P&L +$1,274.85)

---

## ğŸ¯ Table of Contents

1. [MVP ì „í™˜ ë°°ê²½](#mvp-ì „í™˜-ë°°ê²½)
2. [Agent ì„¤ê³„ ì² í•™](#agent-ì„¤ê³„-ì² í•™)
3. [3+1 Agent ìƒì„¸ ìŠ¤í™](#31-agent-ìƒì„¸-ìŠ¤í™)
4. [Position Sizing ì•Œê³ ë¦¬ì¦˜](#position-sizing-ì•Œê³ ë¦¬ì¦˜)
5. [Execution Layer](#execution-layer)
6. [Voting Mechanism](#voting-mechanism)
7. [Legacy vs MVP ë¹„êµ](#legacy-vs-mvp-ë¹„êµ)
8. [êµ¬í˜„ ì„¸ë¶€ì‚¬í•­](#êµ¬í˜„-ì„¸ë¶€ì‚¬í•­)
9. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
10. [í–¥í›„ ê³„íš](#í–¥í›„-ê³„íš)

---

## ğŸ”„ MVP ì „í™˜ ë°°ê²½

### ë¬¸ì œì  (Legacy 8-Agent System)

**1. ë¹„ìš© ë¬¸ì œ**
```
8ê°œ Agent Ã— $0.013/call = $0.104 per deliberation
ì›” 100íšŒ ì‹¤í–‰ ì‹œ = $10.40/month
```

**2. ì†ë„ ë¬¸ì œ**
- 8ê°œ Agent ìˆœì°¨ ì‹¤í–‰: ~30ì´ˆ
- API í˜¸ì¶œ 8íšŒ (ê° Agent 1íšŒì”©)
- ì‚¬ìš©ì ëŒ€ê¸° ì‹œê°„ ê³¼ë‹¤

**3. ë³µì¡ë„ ë¬¸ì œ**
- 8ê°œ ì˜ê²¬ í†µí•©ì˜ ì–´ë ¤ì›€
- íˆ¬í‘œ ê°€ì¤‘ì¹˜ ì¡°ì •ì˜ ë³µì¡ì„±
- Agent ê°„ ì—­í•  ì¤‘ë³µ (News + Macro, Trader + ChipWar)

**4. ìœ ì§€ë³´ìˆ˜ ë¬¸ì œ**
- 8ê°œ Agent ê°ê° ì—…ë°ì´íŠ¸ í•„ìš”
- ì¼ê´€ì„± ìœ ì§€ ì–´ë ¤ì›€
- í…ŒìŠ¤íŠ¸ ë³µì¡ë„ ì¦ê°€

### í•´ê²° ë°©ì•ˆ: 3+1 MVP Agent

**ì„¤ê³„ ì›ì¹™**:
1. **ì—­í•  í†µí•©**: ìœ ì‚¬ ê¸°ëŠ¥ Agent ë³‘í•©
2. **ë‹¨ì¼ ëª¨ë¸**: Gemini 2.0 Flash Experimental í†µì¼
3. **ë³‘ë ¬ ì‹¤í–‰**: 3ê°œ Agent ë™ì‹œ í˜¸ì¶œ
4. **ëª…í™•í•œ ë¶„ë¦¬**: Attack / Defense / Information
5. **ìµœì¢… ê²€ì¦**: PM Agentì˜ Hard Rules ê²€ì¦

**ê¸°ëŒ€ íš¨ê³¼**:
- âœ… ë¹„ìš©: 67% ì ˆê° ($0.104 â†’ $0.035)
- âœ… ì†ë„: 67% í–¥ìƒ (30s â†’ 10s)
- âœ… API í˜¸ì¶œ: 62.5% ê°ì†Œ (8íšŒ â†’ 3íšŒ)
- âœ… ìœ ì§€ë³´ìˆ˜: ê°„ì†Œí™” (8ê°œ â†’ 4ê°œ íŒŒì¼)

---

## ğŸ§  Agent ì„¤ê³„ ì² í•™

### 3+1 êµ¬ì¡° ì„¤ê³„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   War Room MVP System                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ Trader â”‚        â”‚   Risk   â”‚       â”‚ Analyst   â”‚
    â”‚  MVP   â”‚        â”‚   MVP    â”‚       â”‚   MVP     â”‚
    â”‚ (35%)  â”‚        â”‚  (35%)   â”‚       â”‚  (30%)    â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚                   â”‚
        â”‚ Attack           â”‚ Defense +         â”‚ Information
        â”‚ (ê¸°íšŒ í¬ì°©)       â”‚ Position Sizing   â”‚ (ì •ë³´ ë¶„ì„)
        â”‚                  â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   PM Agent    â”‚
                    â”‚     MVP       â”‚
                    â”‚ (Final Decide)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    Hard Rules Check
                    Approve / Reject
```

### Attack (Trader MVP - 35%)

**ì—­í• **: ê³µê²©ì  ê¸°íšŒ í¬ì°©
**í¡ìˆ˜í•œ Agent**: Trader (15%) + ChipWar Opportunity (12%)

**í•µì‹¬ ì§ˆë¬¸**:
- "ì§€ê¸ˆ ì§„ì…í•´ì•¼ í•˜ëŠ”ê°€?"
- "ì´ íŒ¨í„´ì€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ê°€?"
- "ë°˜ë„ì²´ ì „ìŸì´ ê¸°íšŒë¥¼ ë§Œë“œëŠ”ê°€?"

**ê°•ì **:
- ê¸°ìˆ ì  ë¶„ì„ (RSI, MACD, ì°¨íŠ¸ íŒ¨í„´)
- ë‹¨ê¸° ëª¨ë©˜í…€ í¬ì°©
- ChipWar ê´€ë ¨ ê¸°íšŒ (NVIDIA, AMD ë“±)

**ì•½ì **:
- ë¦¬ìŠ¤í¬ ê³¼ì†Œí‰ê°€ ê°€ëŠ¥
- ë‹¨ê¸° ë…¸ì´ì¦ˆì— ë¯¼ê°
- ì†ì‹¤ ì‹œë‚˜ë¦¬ì˜¤ ê³ ë ¤ ë¶€ì¡±

â†’ **Risk MVPê°€ ê· í˜• ì œê³µ**

### Defense (Risk MVP - 35%)

**ì—­í• **: ë°©ì–´ì  ë¦¬ìŠ¤í¬ ê´€ë¦¬ + Position Sizing
**í¡ìˆ˜í•œ Agent**: Risk (20%) + Sentiment (8%)

**í•µì‹¬ ì§ˆë¬¸**:
- "ì–¼ë§ˆë‚˜ íˆ¬ìí•´ì•¼ í•˜ëŠ”ê°€?" (Position Sizing)
- "Stop LossëŠ” ì–´ë””ì— ì„¤ì •í•´ì•¼ í•˜ëŠ”ê°€?"
- "ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤ëŠ” ë¬´ì—‡ì¸ê°€?"

**ê°•ì **:
- **Position Sizing ìë™í™”** (ì‹ ê·œ ê¸°ëŠ¥!)
- ë³€ë™ì„± ë¶„ì„ (ë² íƒ€, í‘œì¤€í¸ì°¨)
- ì‹œì¥ ì‹¬ë¦¬ ë°˜ì˜ (VIX, ê³µí¬/íƒìš• ì§€ìˆ˜)

**ì•½ì **:
- ê³¼ë„í•œ ë³´ìˆ˜ì„± (ê¸°íšŒ ìƒì‹¤)
- ë‹¨ê¸° ë³€ë™ì„±ì— ê³¼ë¯¼ ë°˜ì‘

â†’ **Trader MVPê°€ ê· í˜• ì œê³µ**

### Information (Analyst MVP - 30%)

**ì—­í• **: ì¢…í•© ì •ë³´ ë¶„ì„
**í¡ìˆ˜í•œ Agent**: News (10%) + Macro (10%) + Institutional (10%) + ChipWar Geopolitics

**í•µì‹¬ ì§ˆë¬¸**:
- "ë‰´ìŠ¤ê°€ ì£¼ê°€ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ”ê°€?"
- "ê±°ì‹œê²½ì œ í™˜ê²½ì€ ìœ ë¦¬í•œê°€?"
- "ê¸°ê´€ íˆ¬ììë“¤ì€ ì–´ë–»ê²Œ ì›€ì§ì´ëŠ”ê°€?"
- "ì§€ì •í•™ì  ë¦¬ìŠ¤í¬ëŠ” ë¬´ì—‡ì¸ê°€?"

**ê°•ì **:
- ë‹¤ì¸µì  ì •ë³´ ë¶„ì„ (ë‰´ìŠ¤ + ê±°ì‹œ + ê¸°ê´€ + ì§€ì •í•™)
- Macro Context í†µí•©
- ì¥ê¸° íŠ¸ë Œë“œ íŒŒì•…

**ì•½ì **:
- ì •ë³´ ê³¼ë¶€í•˜ ê°€ëŠ¥
- ë‹¨ê¸° ê¸°ìˆ ì  ì‹ í˜¸ ê°„ê³¼

â†’ **Trader MVPê°€ ë³´ì™„**

### Final Decision (PM Agent MVP)

**ì—­í• **: ìµœì¢… ì˜ì‚¬ê²°ì • + Hard Rules ê²€ì¦
**ì‹ ê·œ ì¶”ê°€**: MVP ì „í™˜ ì‹œ ì¶”ê°€ë¨

**í•µì‹¬ ì§ˆë¬¸**:
- "3ê°œ Agent ì˜ê²¬ì„ ì–´ë–»ê²Œ ì¢…í•©í•˜ëŠ”ê°€?"
- "Hard Rulesë¥¼ ìœ„ë°˜í•˜ì§€ ì•ŠëŠ”ê°€?"
- "Execution RouterëŠ” Fast Trackì¸ê°€ Deep Diveì¸ê°€?"

**ê°•ì **:
- **8ê°œ Hard Rules ìë™ ê²€ì¦** (ì•ˆì „ì¥ì¹˜)
- Weighted Voting (35% + 35% + 30%)
- ìµœì¢… ìŠ¹ì¸/ê±°ë¶€ ê¶Œí•œ

**ì•½ì **:
- ì—†ìŒ (ê·œì¹™ ê¸°ë°˜, AI íŒë‹¨ ì•„ë‹˜)

---

## ğŸ“Š 3+1 Agent ìƒì„¸ ìŠ¤í™

### Trader MVP

**File**: `backend/ai/mvp/trader_agent_mvp.py` (485 lines)
**Model**: Gemini 2.0 Flash Experimental
**Vote Weight**: 35%

**Input**:
```python
{
    'symbol': 'AAPL',
    'price_data': {
        'current_price': 150.0,
        'high_52w': 180.0,
        'low_52w': 120.0,
        'volume': 50_000_000
    },
    'technical_data': {
        'rsi': 28.5,
        'macd': {'value': -2.1, 'signal': -1.5},
        'bollinger': {'upper': 155, 'lower': 145, 'middle': 150}
    },
    'chipwar_events': [
        {'title': 'US eases AI chip export restrictions', 'date': '2026-01-03'}
    ]
}
```

**Analyze() ë¡œì§**:
```python
def analyze(self, symbol, price_data, technical_data=None, chipwar_events=None):
    # 1. ê¸°ìˆ ì  ë¶„ì„
    technical_score = self._analyze_technicals(technical_data)

    # 2. ì°¨íŠ¸ íŒ¨í„´ ì¸ì‹
    pattern_score = self._detect_chart_patterns(price_data)

    # 3. ChipWar ê¸°íšŒ í¬ì°©
    chipwar_score = self._assess_chipwar_impact(chipwar_events)

    # 4. ì¢…í•© ì ìˆ˜ ê³„ì‚°
    opportunity_score = (
        technical_score * 0.5 +
        pattern_score * 0.3 +
        chipwar_score * 0.2
    )

    # 5. Action ê²°ì •
    if opportunity_score > 7.0:
        action = 'buy'
        confidence = min(opportunity_score / 10.0, 0.95)
    elif opportunity_score < 3.0:
        action = 'sell'
        confidence = min((10.0 - opportunity_score) / 10.0, 0.95)
    else:
        action = 'hold'
        confidence = 0.5

    return {
        'agent': 'trader_mvp',
        'action': action,
        'confidence': confidence,
        'opportunity_score': opportunity_score,
        'reasoning': f"Technical: {technical_score}, Pattern: {pattern_score}, ChipWar: {chipwar_score}",
        'risk_factors': self._identify_risks()
    }
```

**Output Example**:
```json
{
  "agent": "trader_mvp",
  "action": "buy",
  "confidence": 0.85,
  "reasoning": "ì´ì¤‘ ë°”ë‹¥ íŒ¨í„´ ì™„ì„±, RSI 28.5 (ê³¼ë§¤ë„), MACD ê³¨ë“ í¬ë¡œìŠ¤ ì„ë°•",
  "opportunity_score": 8.5,
  "risk_factors": ["ì‹¤ì  ë°œí‘œ D-3", "ê±°ë˜ëŸ‰ í‰ê·  ëŒ€ë¹„ 70%"],
  "chipwar_impact": "NVIDIA AI ì¹© ìˆ˜ì¶œ ê·œì œ ì™„í™”ë¡œ ìˆ˜í˜œ ì˜ˆìƒ",
  "key_signals": [
    "RSI: 28.5 (ê³¼ë§¤ë„ êµ¬ê°„)",
    "Volume: 50M (í‰ê·  70M)",
    "52w Low ëŒ€ë¹„ +25% ë°˜ë“±"
  ]
}
```

---

### Risk MVP

**File**: `backend/ai/mvp/risk_agent_mvp.py` (612 lines)
**Model**: Gemini 2.0 Flash Experimental
**Vote Weight**: 35%

**Input**:
```python
{
    'symbol': 'AAPL',
    'price_data': {...},
    'portfolio_state': {
        'total_value': 100000,
        'available_cash': 50000,
        'positions': {...}
    },
    'market_conditions': {
        'vix': 18.5,
        'market_regime': 'RISK_ON',
        'fed_stance': 'HAWKISH'
    }
}
```

**Analyze() ë¡œì§**:
```python
def analyze(self, symbol, price_data, portfolio_state, market_conditions):
    # 1. ë¦¬ìŠ¤í¬ í‰ê°€
    risk_score = self._calculate_risk_score(price_data, market_conditions)

    # 2. **Position Sizing ê³„ì‚°** (í•µì‹¬ ê¸°ëŠ¥!)
    position_size = self._calculate_position_size(
        price_data=price_data,
        portfolio_state=portfolio_state,
        risk_score=risk_score
    )

    # 3. Stop Loss ì„¤ì •
    stop_loss = self._calculate_stop_loss(price_data)

    # 4. ì‹œì¥ ì‹¬ë¦¬ ë¶„ì„
    sentiment = self._analyze_sentiment(market_conditions)

    # 5. Action ê²°ì •
    if risk_score < 5.0 and position_size > 0:
        action = 'buy'
        confidence = (10.0 - risk_score) / 10.0
    elif risk_score > 7.0:
        action = 'sell' if has_position else 'pass'
        confidence = risk_score / 10.0
    else:
        action = 'hold'
        confidence = 0.5

    return {
        'agent': 'risk_mvp',
        'action': action,
        'confidence': confidence,
        'risk_score': risk_score,
        'position_size': position_size,
        'position_size_pct': position_size / portfolio_state['total_value'] * 100,
        'stop_loss': stop_loss,
        'sentiment': sentiment,
        'risk_factors': self._identify_risk_factors()
    }
```

**Output Example**:
```json
{
  "agent": "risk_mvp",
  "action": "buy",
  "confidence": 0.75,
  "reasoning": "VIX 18.5 (ì •ìƒ ë²”ìœ„), ìœ ë™ì„± ì¶©ë¶„, ë³€ë™ì„± ë‚®ìŒ",
  "risk_score": 4.2,
  "position_size": 10000,
  "position_size_pct": 10.0,
  "stop_loss": 142.50,
  "stop_loss_distance": 5.0,
  "risk_factors": [
    "ì‹¤ì  ë°œí‘œ ì„ë°• (D-3)",
    "Fed ê¸ˆë¦¬ ê²°ì • ëŒ€ê¸° (D-7)"
  ],
  "sentiment": "NEUTRAL",
  "volatility": {
    "beta": 1.05,
    "annual_volatility": 24.3
  }
}
```

---

### Analyst MVP

**File**: `backend/ai/mvp/analyst_agent_mvp.py` (548 lines)
**Model**: Gemini 2.0 Flash Experimental
**Vote Weight**: 30%

**Input**:
```python
{
    'symbol': 'AAPL',
    'news_articles': [...],  # ìµœê·¼ 24ì‹œê°„ ë‰´ìŠ¤
    'macro_context': {
        'regime': 'RISK_ON',
        'fed_stance': 'HAWKISH',
        'vix': 18.5
    },
    'institutional_flow': {...},  # 13F filings
    'chipwar_geopolitics': {...}
}
```

**Analyze() ë¡œì§**:
```python
def analyze(self, symbol, news_articles, macro_context, institutional_flow, chipwar_geopolitics):
    # 1. ë‰´ìŠ¤ ë¶„ì„
    news_score = self._analyze_news(news_articles)

    # 2. ê±°ì‹œê²½ì œ ë¶„ì„
    macro_score = self._analyze_macro(macro_context)

    # 3. ê¸°ê´€ íˆ¬ìì ë™í–¥
    institutional_score = self._analyze_institutional_flow(institutional_flow)

    # 4. ChipWar ì§€ì •í•™
    chipwar_score = self._analyze_chipwar_geopolitics(chipwar_geopolitics)

    # 5. ì¢…í•© ì ìˆ˜
    information_score = (
        news_score * 0.35 +
        macro_score * 0.30 +
        institutional_score * 0.20 +
        chipwar_score * 0.15
    )

    # 6. Action ê²°ì •
    if information_score > 6.5:
        action = 'buy'
        confidence = min(information_score / 10.0, 0.90)
    elif information_score < 3.5:
        action = 'sell'
        confidence = min((10.0 - information_score) / 10.0, 0.90)
    else:
        action = 'hold'
        confidence = 0.5

    return {
        'agent': 'analyst_mvp',
        'action': action,
        'confidence': confidence,
        'information_score': information_score,
        'news_summary': self._summarize_news(news_articles),
        'macro_context': macro_context,
        'institutional_flow': institutional_flow,
        'chipwar_geopolitics': chipwar_geopolitics
    }
```

**Output Example**:
```json
{
  "agent": "analyst_mvp",
  "action": "buy",
  "confidence": 0.70,
  "reasoning": "ê¸ì • ë‰´ìŠ¤ 3ê±´, Fed ì¤‘ë¦½ ê¸°ì¡° ìœ ì§€, ê¸°ê´€ ìœ ì… ì§€ì†",
  "information_score": 7.0,
  "news_summary": "AI ì¹© ìˆ˜ìš” ì¦ê°€ ì „ë§ (Bloomberg), ì‹¤ì  ìƒí–¥ ì¡°ì • (2 analysts)",
  "macro_context": {
    "regime": "RISK_ON",
    "fed_stance": "HAWKISH",
    "vix": 18.5,
    "narrative": "ë‹¨ê¸° ê°•í•œ ëª¨ë©˜í…€, ë‚®ì€ ë³€ë™ì„± ê²¬ì¸"
  },
  "institutional_flow": {
    "net_flow": 1200000,
    "trend": "inflow",
    "period": "3 days"
  },
  "chipwar_geopolitics": "ë¯¸êµ­ AI ë°˜ë„ì²´ ìˆ˜ì¶œ ê·œì œ ì™„í™” ì „ë§ (ê¸ì •ì )"
}
```

---

### PM Agent MVP

**File**: `backend/ai/mvp/pm_agent_mvp.py` (427 lines)
**Model**: Gemini 2.0 Flash Experimental
**Vote Weight**: Final Decision

**Input**:
```python
{
    'symbol': 'AAPL',
    'action_context': 'new_position',
    'agent_opinions': {
        'trader_mvp': {...},
        'risk_mvp': {...},
        'analyst_mvp': {...}
    },
    'portfolio_state': {...},
    'market_conditions': {...}
}
```

**make_final_decision() ë¡œì§**:
```python
def make_final_decision(self, symbol, action_context, agent_opinions, portfolio_state, market_conditions):
    # 1. Weighted Voting
    weighted_score = (
        agent_opinions['trader_mvp']['confidence'] * 0.35 +
        agent_opinions['risk_mvp']['confidence'] * 0.35 +
        agent_opinions['analyst_mvp']['confidence'] * 0.30
    )

    # 2. Action ê²°ì • (ë‹¤ìˆ˜ê²°)
    actions = [op['action'] for op in agent_opinions.values()]
    final_action = max(set(actions), key=actions.count)

    # 3. Position Size í™•ì • (Risk MVP ì œì•ˆ ì‚¬ìš©)
    position_size = agent_opinions['risk_mvp']['position_size']
    stop_loss = agent_opinions['risk_mvp']['stop_loss']

    # 4. **8 Hard Rules ê²€ì¦** (í•µì‹¬!)
    hard_rules_passed, violation_reason = self._check_hard_rules(
        action=final_action,
        position_size=position_size,
        stop_loss=stop_loss,
        confidence=weighted_score,
        portfolio_state=portfolio_state,
        market_conditions=market_conditions
    )

    # 5. ìµœì¢… ìŠ¹ì¸/ê±°ë¶€
    if hard_rules_passed:
        final_decision = 'approve'
    else:
        final_decision = 'reject'
        final_action = 'pass'

    # 6. Execution Router ì„ íƒ
    execution_path = self._select_execution_path(action_context, market_conditions)

    return {
        'agent': 'pm_mvp',
        'final_decision': final_decision,
        'action': final_action,
        'confidence': weighted_score,
        'position_size': position_size if hard_rules_passed else 0,
        'stop_loss': stop_loss,
        'reasoning': self._generate_reasoning(agent_opinions),
        'voting_summary': {
            'trader_mvp': {'vote': agent_opinions['trader_mvp']['action'], 'weight': 0.35},
            'risk_mvp': {'vote': agent_opinions['risk_mvp']['action'], 'weight': 0.35},
            'analyst_mvp': {'vote': agent_opinions['analyst_mvp']['action'], 'weight': 0.30}
        },
        'weighted_score': weighted_score,
        'hard_rules_passed': hard_rules_passed,
        'violation_reason': violation_reason if not hard_rules_passed else None,
        'execution_path': execution_path
    }
```

**Output Example**:
```json
{
  "agent": "pm_mvp",
  "final_decision": "approve",
  "action": "buy",
  "confidence": 0.77,
  "position_size": 10000,
  "stop_loss": 142.50,
  "reasoning": "3ê°œ Agent ì¤‘ 2ê°œ BUY (Trader, Risk), 1ê°œ HOLD (Analyst). Weighted Score 7.7/10. Hard Rules í†µê³¼.",
  "voting_summary": {
    "trader_mvp": {"vote": "buy", "weight": 0.35, "confidence": 0.85},
    "risk_mvp": {"vote": "buy", "weight": 0.35, "confidence": 0.75},
    "analyst_mvp": {"vote": "hold", "weight": 0.30, "confidence": 0.70}
  },
  "weighted_score": 7.7,
  "hard_rules_passed": true,
  "violation_reason": null,
  "execution_path": "deep_dive"
}
```

---

## ğŸ² Position Sizing ì•Œê³ ë¦¬ì¦˜

**ìœ„ì¹˜**: Risk MVP ë‚´ì¥ ê¸°ëŠ¥
**ëª©ì **: ìë™í™”ëœ í¬ì§€ì…˜ í¬ê¸° ê³„ì‚°

### 4-Step Formula

```python
def calculate_position_size(self, price_data, portfolio_state, confidence, risk_multiplier):
    """
    4-Step Position Sizing Algorithm

    Returns:
        Final position size in dollars
    """

    # Step 1: Risk-based base sizing
    # ì›ì¹™: ê³„ì¢Œì˜ 2%ë§Œ ë¦¬ìŠ¤í¬ì— ë…¸ì¶œ
    account_risk_pct = 0.02  # 2%
    stop_loss_distance = self._calculate_stop_loss_distance(price_data)

    base_size = (account_risk_pct / stop_loss_distance) * portfolio_state['total_value']
    # ì˜ˆ: (0.02 / 0.05) Ã— $100,000 = $40,000

    # Step 2: Confidence adjustment
    # Agent ì‹ ë¢°ë„ì— ë”°ë¼ ì¡°ì •
    confidence_adjusted = base_size * confidence
    # ì˜ˆ: $40,000 Ã— 0.85 = $34,000

    # Step 3: Volatility adjustment
    # ì‹œì¥ ë³€ë™ì„±ì— ë”°ë¼ ì¡°ì •
    risk_multiplier = self._calculate_risk_multiplier(
        vix=market_conditions['vix'],
        market_regime=market_conditions['market_regime']
    )
    risk_adjusted = confidence_adjusted * risk_multiplier
    # ì˜ˆ: $34,000 Ã— 0.8 = $27,200

    # Step 4: Hard cap enforcement
    # í¬íŠ¸í´ë¦¬ì˜¤ì˜ 10%ë¥¼ ì ˆëŒ€ ì´ˆê³¼í•˜ì§€ ì•ŠìŒ
    max_position = portfolio_state['total_value'] * 0.10
    final_size = min(risk_adjusted, max_position)
    # ì˜ˆ: min($27,200, $10,000) = $10,000

    return final_size
```

### Risk Multiplier Calculation

```python
def _calculate_risk_multiplier(self, vix, market_regime):
    """
    Market conditionsì— ë”°ë¥¸ ë¦¬ìŠ¤í¬ ë°°ìœ¨ ì¡°ì •

    VIX Levels:
    - < 15: Low volatility â†’ 1.2x (ê³µê²©ì )
    - 15-25: Normal â†’ 1.0x (ê¸°ë³¸)
    - 25-35: Elevated â†’ 0.7x (ë³´ìˆ˜ì )
    - > 35: High â†’ 0.5x (ë§¤ìš° ë³´ìˆ˜ì )

    Market Regime:
    - RISK_ON: +0.1x
    - TRANSITION: 0x
    - RISK_OFF: -0.2x
    """

    # VIX ê¸°ë°˜ ê¸°ë³¸ ë°°ìœ¨
    if vix < 15:
        base_multiplier = 1.2
    elif vix < 25:
        base_multiplier = 1.0
    elif vix < 35:
        base_multiplier = 0.7
    else:
        base_multiplier = 0.5

    # Market Regime ì¡°ì •
    regime_adjustment = {
        'RISK_ON': 0.1,
        'TRANSITION': 0.0,
        'RISK_OFF': -0.2
    }.get(market_regime, 0.0)

    final_multiplier = base_multiplier + regime_adjustment

    return max(final_multiplier, 0.3)  # ìµœì†Œ 0.3x
```

### Stop Loss Distance Calculation

```python
def _calculate_stop_loss_distance(self, price_data):
    """
    ê¸°ìˆ ì  ë¶„ì„ ê¸°ë°˜ Stop Loss ê±°ë¦¬ ê³„ì‚°

    ë°©ë²•:
    1. ATR (Average True Range) ê¸°ë°˜
    2. ìµœê·¼ ì§€ì§€ì„  ê¸°ë°˜
    3. ê³ ì • % (5%) ê¸°ë³¸ê°’
    """

    current_price = price_data['current_price']

    # Method 1: ATR (Average True Range)
    if 'atr' in price_data:
        atr_stop_loss = current_price - (price_data['atr'] * 2)
        distance_atr = (current_price - atr_stop_loss) / current_price
    else:
        distance_atr = None

    # Method 2: Support Level (ì§€ì§€ì„ )
    if 'support_level' in price_data:
        support_stop_loss = price_data['support_level'] * 0.98  # ì§€ì§€ì„  ì•„ë˜ 2%
        distance_support = (current_price - support_stop_loss) / current_price
    else:
        distance_support = None

    # Method 3: Fixed 5% (ê¸°ë³¸ê°’)
    distance_fixed = 0.05

    # ìµœì¢… ì„ íƒ: ATR > Support > Fixed ìš°ì„ ìˆœìœ„
    if distance_atr:
        return min(max(distance_atr, 0.03), 0.10)  # 3-10% ë²”ìœ„
    elif distance_support:
        return min(max(distance_support, 0.03), 0.10)
    else:
        return distance_fixed
```

### Position Sizing Example

**ì‹œë‚˜ë¦¬ì˜¤**:
- Portfolio Value: $100,000
- Available Cash: $50,000
- Current Price: $150
- Stop Loss Distance: 5%
- Agent Confidence: 0.85
- VIX: 18.5 (NORMAL)
- Market Regime: RISK_ON

**ê³„ì‚°**:
```python
# Step 1: Base sizing
base_size = (0.02 / 0.05) Ã— 100,000 = $40,000

# Step 2: Confidence adjustment
confidence_adjusted = 40,000 Ã— 0.85 = $34,000

# Step 3: Risk multiplier
risk_multiplier = 1.0 (VIX 15-25) + 0.1 (RISK_ON) = 1.1
risk_adjusted = 34,000 Ã— 1.1 = $37,400

# Step 4: Hard cap (10%)
max_position = 100,000 Ã— 0.10 = $10,000
final_size = min(37,400, 10,000) = $10,000

# Quantity
quantity = 10,000 / 150 = 66 shares
```

**ê²°ê³¼**:
- Position Size: $10,000 (10%)
- Quantity: 66 shares
- Stop Loss: $142.50 (5% below entry)
- Risk Amount: $10,000 Ã— 0.05 = $500 (0.5% of portfolio) âœ…

---

## âš¡ Execution Layer

MVP ì „í™˜ê³¼ í•¨ê»˜ ì¶”ê°€ëœ ì‹¤í–‰ ê³„ì¸µì…ë‹ˆë‹¤.

### 1. Execution Router

**File**: `backend/execution/execution_router.py`

**ëª©ì **: ìƒí™©ì— ë”°ë¼ ì‹¤í–‰ ê²½ë¡œ ì„ íƒ

**Fast Track (< 1ì´ˆ)**:
```python
def should_use_fast_track(self, context):
    """
    Fast Track ì¡°ê±´:
    1. Stop Loss ë°œë™
    2. ì¼ì¼ ì†ì‹¤ > -5%
    3. VIX > 40 (ê·¹ë‹¨ì  ê³µí¬)
    4. ê¸´ê¸‰ ì²­ì‚° í•„ìš”
    """

    if context.get('stop_loss_hit'):
        return True, "Stop Loss hit"

    if context.get('daily_loss_pct', 0) < -5.0:
        return True, "Daily loss limit exceeded"

    if context.get('vix', 0) > 40:
        return True, "Extreme volatility (VIX > 40)"

    if context.get('emergency_exit'):
        return True, "Emergency exit requested"

    return False, None
```

**Deep Dive (~10ì´ˆ)**:
```python
def should_use_deep_dive(self, context):
    """
    Deep Dive ì¡°ê±´:
    1. ì‹ ê·œ í¬ì§€ì…˜ ì§„ì…
    2. ë¦¬ë°¸ëŸ°ì‹±
    3. ëŒ€í˜• í¬ì§€ì…˜ (>10% portfolio)
    4. ë³µì¡í•œ ì˜ì‚¬ê²°ì •
    """

    if context.get('action_context') == 'new_position':
        return True, "New position entry"

    if context.get('action_context') == 'rebalancing':
        return True, "Portfolio rebalancing"

    if context.get('position_size_pct', 0) > 10:
        return True, "Large position (>10%)"

    return True, "Default to Deep Dive"
```

### 2. Order Validator

**File**: `backend/execution/order_validator.py`

**ëª©ì **: ì£¼ë¬¸ ì‹¤í–‰ ì „ ìµœì¢… ê²€ì¦

**8 Hard Rules**:
```python
class OrderValidator:
    HARD_RULES = [
        "Position size must not exceed 30% of portfolio",
        "Position size must not exceed 10% if confidence < 0.7",
        "Must have Stop Loss for all positions",
        "Stop Loss must be within 10% of entry price",
        "No positions during earnings blackout (D-2 ~ D+1)",
        "Daily loss limit: -5% of portfolio",
        "VIX > 40: No new positions",
        "RISK_OFF + VIX > 30: No new positions"
    ]

    def validate(self, order, context):
        """
        Returns:
            (is_valid, error_message)
        """

        # Rule 1: Position size â‰¤ 30%
        if order['position_size'] > context['portfolio_value'] * 0.30:
            return False, "REJECT: Position size exceeds 30% of portfolio"

        # Rule 2: Position size â‰¤ 10% if confidence < 0.7
        if order['confidence'] < 0.7 and order['position_size'] > context['portfolio_value'] * 0.10:
            return False, "REJECT: Low confidence (< 0.7), position size must be â‰¤ 10%"

        # Rule 3: Stop Loss required
        if not order.get('stop_loss'):
            return False, "REJECT: No Stop Loss specified"

        # Rule 4: Stop Loss within 10%
        stop_loss_distance = abs(order['entry_price'] - order['stop_loss']) / order['entry_price']
        if stop_loss_distance > 0.10:
            return False, f"REJECT: Stop Loss too wide ({stop_loss_distance*100:.1f}% > 10%)"

        # Rule 5: Earnings blackout
        if self._is_earnings_blackout(order['symbol'], context['current_date']):
            return False, "REJECT: Earnings blackout period (D-2 ~ D+1)"

        # Rule 6: Daily loss limit
        if context.get('daily_loss_pct', 0) < -5.0:
            return False, "REJECT: Daily loss limit exceeded (-5%)"

        # Rule 7: VIX > 40
        if context.get('vix', 0) > 40 and order['action'] in ['buy', 'sell']:
            return False, "REJECT: VIX > 40 (extreme volatility), no new positions"

        # Rule 8: RISK_OFF + VIX > 30
        if context.get('market_regime') == 'RISK_OFF' and context.get('vix', 0) > 30:
            return False, "REJECT: RISK_OFF + VIX > 30, no new positions"

        return True, "APPROVED"
```

### 3. Shadow Trading Engine

**File**: `backend/execution/shadow_trading_engine.py`

**ëª©ì **: ê°€ìƒ ìê¸ˆìœ¼ë¡œ ì‹¤ì „ ê²€ì¦ (3ê°œì›”)

**ì¡°ê±´ë¶€ ì‹¤í–‰**:
```python
class ShadowTradingEngine:
    def execute_if_approved(self, pm_decision, market_data):
        """
        PM Agentì˜ ìŠ¹ì¸ì´ ìˆì„ ë•Œë§Œ ì‹¤í–‰
        """

        if pm_decision['final_decision'] != 'approve':
            self.log_rejected_proposal(pm_decision)
            return None

        if pm_decision['action'] == 'pass':
            return None

        # Hard Rules í†µê³¼ í™•ì¸
        if not pm_decision['hard_rules_passed']:
            return None

        # Shadow Trading ì‹¤í–‰
        trade = self._execute_shadow_trade(
            symbol=pm_decision['symbol'],
            action=pm_decision['action'],
            quantity=self._calculate_quantity(
                pm_decision['position_size'],
                market_data['current_price']
            ),
            entry_price=market_data['current_price'],
            stop_loss=pm_decision['stop_loss']
        )

        self.log_shadow_trade(trade)
        return trade
```

**Real-time P&L Tracking**:
```python
def update_positions(self):
    """
    ë§¤ì¼ ì‹¤í–‰í•˜ì—¬ í¬ì§€ì…˜ P&L ì—…ë°ì´íŠ¸
    """

    for position in self.get_open_positions():
        # ì‹¤ì‹œê°„ ê°€ê²© ì¡°íšŒ
        current_price = self.fetch_current_price(position.symbol)

        # P&L ê³„ì‚°
        unrealized_pnl = (current_price - position.entry_price) * position.quantity

        # Stop Loss ì²´í¬
        if current_price <= position.stop_loss:
            self.close_position(position, reason='stop_loss_hit')

        # ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
        self.update_position_pnl(position.id, unrealized_pnl, current_price)
```

---

## ğŸ—³ï¸ Voting Mechanism

### Weighted Voting Formula

```python
def calculate_weighted_score(agent_opinions):
    """
    Weighted Voting:
    - Trader MVP: 35%
    - Risk MVP: 35%
    - Analyst MVP: 30%

    Returns:
        weighted_score (0.0 ~ 1.0)
    """

    trader_confidence = agent_opinions['trader_mvp']['confidence']
    risk_confidence = agent_opinions['risk_mvp']['confidence']
    analyst_confidence = agent_opinions['analyst_mvp']['confidence']

    weighted_score = (
        trader_confidence * 0.35 +
        risk_confidence * 0.35 +
        analyst_confidence * 0.30
    )

    return weighted_score
```

### Action Consensus

```python
def determine_final_action(agent_opinions):
    """
    ë‹¤ìˆ˜ê²°ë¡œ ìµœì¢… Action ê²°ì •

    Examples:
    - BUY, BUY, HOLD â†’ BUY (2/3)
    - BUY, SELL, HOLD â†’ HOLD (ë™ë¥  ì‹œ ë³´ìˆ˜ì  ì„ íƒ)
    - SELL, SELL, SELL â†’ SELL (ë§Œì¥ì¼ì¹˜)
    """

    actions = [
        agent_opinions['trader_mvp']['action'],
        agent_opinions['risk_mvp']['action'],
        agent_opinions['analyst_mvp']['action']
    ]

    # ë‹¤ìˆ˜ê²°
    from collections import Counter
    vote_counts = Counter(actions)
    most_common = vote_counts.most_common(2)

    # ëª…í™•í•œ ë‹¤ìˆ˜ (2ê°œ ì´ìƒ ì¼ì¹˜)
    if most_common[0][1] >= 2:
        return most_common[0][0]

    # ë™ë¥  (1:1:1) â†’ ë³´ìˆ˜ì  ì„ íƒ (HOLD ë˜ëŠ” PASS)
    if 'hold' in actions:
        return 'hold'
    elif 'pass' in actions:
        return 'pass'
    else:
        # BUY vs SELL ë™ë¥  â†’ HOLD
        return 'hold'
```

### Voting Examples

**Example 1: ëª…í™•í•œ BUY ì‹ í˜¸**
```json
{
  "trader_mvp": {"action": "buy", "confidence": 0.85},
  "risk_mvp": {"action": "buy", "confidence": 0.75},
  "analyst_mvp": {"action": "buy", "confidence": 0.70}
}

â†’ Final Action: BUY
â†’ Weighted Score: 0.85Ã—0.35 + 0.75Ã—0.35 + 0.70Ã—0.30 = 0.77
â†’ Confidence: HIGH (unanimous)
```

**Example 2: ì˜ê²¬ ë¶„ì‚° (2:1)**
```json
{
  "trader_mvp": {"action": "buy", "confidence": 0.80},
  "risk_mvp": {"action": "hold", "confidence": 0.60},
  "analyst_mvp": {"action": "buy", "confidence": 0.75}
}

â†’ Final Action: BUY (2/3)
â†’ Weighted Score: 0.80Ã—0.35 + 0.60Ã—0.35 + 0.75Ã—0.30 = 0.72
â†’ Confidence: MEDIUM (majority but not unanimous)
```

**Example 3: ë™ë¥  (1:1:1)**
```json
{
  "trader_mvp": {"action": "buy", "confidence": 0.70},
  "risk_mvp": {"action": "sell", "confidence": 0.65},
  "analyst_mvp": {"action": "hold", "confidence": 0.60}
}

â†’ Final Action: HOLD (ë³´ìˆ˜ì  ì„ íƒ)
â†’ Weighted Score: 0.70Ã—0.35 + 0.65Ã—0.35 + 0.60Ã—0.30 = 0.65
â†’ Confidence: LOW (no consensus)
```

---

## ğŸ“Š Legacy vs MVP ë¹„êµ

### êµ¬ì¡° ë¹„êµ

| í•­ëª© | Legacy (8-Agent) | MVP (3+1) | ë³€í™” |
|------|------------------|-----------|------|
| **Agent ìˆ˜** | 8ê°œ ë…ë¦½ Agent | 3+1 í†µí•© Agent | -56% |
| **API í˜¸ì¶œ** | 8íšŒ (ìˆœì°¨) | 3íšŒ (ë³‘ë ¬) | -62.5% |
| **ì‘ë‹µ ì‹œê°„** | ~30ì´ˆ | ~10ì´ˆ | -67% |
| **ë¹„ìš©/íšŒ** | $0.105 | $0.035 | -67% |
| **ì›”ë¹„ìš©** (100íšŒ) | $10.50 | $3.50 | -67% |
| **íˆ¬í‘œ ê°€ì¤‘ì¹˜** | 8ê°œ ë¶„ì‚° | 3ê°œ ì§‘ì¤‘ | ë‹¨ìˆœí™” |
| **Position Sizing** | âŒ ì—†ìŒ | âœ… ìë™í™” | ì‹ ê·œ |
| **Hard Rules** | âŒ ì—†ìŒ | âœ… 8ê°œ ê²€ì¦ | ì‹ ê·œ |
| **Execution Router** | âŒ ì—†ìŒ | âœ… Fast/Deep | ì‹ ê·œ |

### Agent Mapping

```
Legacy 8-Agent                    â†’  MVP 3+1-Agent
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Trader (15%)                     â†’  Trader MVP (35%)
  + ChipWar Opportunity (12%)    â†’  (Attack)

Risk (20%)                       â†’  Risk MVP (35%)
  + Sentiment (8%)               â†’  (Defense + Position Sizing)

News (10%)                       â†’  Analyst MVP (30%)
  + Macro (10%)                  â†’  (Information)
  + Institutional (10%)
  + ChipWar Geopolitics

PM (15%)                         â†’  PM Agent MVP
                                 â†’  (Final Decision + Hard Rules)
```

### ê¸°ëŠ¥ ë¹„êµ

**Legacy ì¥ì **:
- âœ… ì„¸ë¶„í™”ëœ ì „ë¬¸ì„± (8ê°œ ê´€ì )
- âœ… ê° Agent ë…ë¦½ì  ê²€ì¦ ê°€ëŠ¥
- âœ… íŠ¹ì • Agentë§Œ êµì²´ ìš©ì´

**Legacy ë‹¨ì **:
- âŒ ë¹„ìš© ê³¼ë‹¤ ($10.50/month)
- âŒ ì†ë„ ëŠë¦¼ (30ì´ˆ)
- âŒ ë³µì¡ë„ ë†’ìŒ (8ê°œ ì˜ê²¬ í†µí•©)
- âŒ Position Sizing ìˆ˜ë™
- âŒ Hard Rules ì—†ìŒ

**MVP ì¥ì **:
- âœ… ë¹„ìš© 67% ì ˆê° ($3.50/month)
- âœ… ì†ë„ 67% í–¥ìƒ (10ì´ˆ)
- âœ… ë‹¨ìˆœí™”ëœ ì˜ì‚¬ê²°ì • (3ê°œ ì˜ê²¬)
- âœ… **Position Sizing ìë™í™”**
- âœ… **8 Hard Rules ê²€ì¦**
- âœ… **Execution Router**
- âœ… **Shadow Trading í†µí•©**

**MVP ë‹¨ì **:
- âŒ ì „ë¬¸ì„± ì¼ë¶€ ì†ì‹¤ (8â†’3 í†µí•©)
- âŒ Agentë³„ ë…ë¦½ ê²€ì¦ ì œí•œ
- âŒ í•˜ë‚˜ì˜ Agent ì˜¤ë¥˜ ì‹œ ì˜í–¥ ë²”ìœ„ ì¦ê°€

**ê²°ë¡ **: ì‹¤ìš©ì„±ê³¼ ì„±ëŠ¥ì—ì„œ MVPê°€ ì••ë„ì  ìš°ìœ„. ì „ë¬¸ì„± ì†ì‹¤ì€ Agent í†µí•© ì„¤ê³„ë¡œ ìµœì†Œí™”.

---

## ğŸ”§ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### File Structure

```
backend/
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ mvp/                          # MVP Agent êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ trader_agent_mvp.py      # 485 lines
â”‚   â”‚   â”œâ”€â”€ risk_agent_mvp.py        # 612 lines
â”‚   â”‚   â”œâ”€â”€ analyst_agent_mvp.py     # 548 lines
â”‚   â”‚   â”œâ”€â”€ pm_agent_mvp.py          # 427 lines
â”‚   â”‚   â””â”€â”€ war_room_mvp.py          # 723 lines (orchestrator)
â”‚   â”‚
â”‚   â”œâ”€â”€ skills/                       # Skills Architecture
â”‚   â”‚   â””â”€â”€ war_room_mvp/
â”‚   â”‚       â”œâ”€â”€ trader_agent_mvp/
â”‚   â”‚       â”‚   â”œâ”€â”€ SKILL.md
â”‚   â”‚       â”‚   â””â”€â”€ handler.py
â”‚   â”‚       â”œâ”€â”€ risk_agent_mvp/
â”‚   â”‚       â”‚   â”œâ”€â”€ SKILL.md
â”‚   â”‚       â”‚   â””â”€â”€ handler.py
â”‚   â”‚       â”œâ”€â”€ analyst_agent_mvp/
â”‚   â”‚       â”‚   â”œâ”€â”€ SKILL.md
â”‚   â”‚       â”‚   â””â”€â”€ handler.py
â”‚   â”‚       â”œâ”€â”€ pm_agent_mvp/
â”‚   â”‚       â”‚   â”œâ”€â”€ SKILL.md
â”‚   â”‚       â”‚   â””â”€â”€ handler.py
â”‚   â”‚       â””â”€â”€ orchestrator_mvp/
â”‚   â”‚           â”œâ”€â”€ SKILL.md
â”‚   â”‚           â””â”€â”€ handler.py
â”‚   â”‚
â”‚   â””â”€â”€ debate/                       # Legacy 8-Agent (ìœ ì§€)
â”‚       â”œâ”€â”€ trader_agent.py
â”‚       â”œâ”€â”€ risk_agent.py
â”‚       â””â”€â”€ ... (8ê°œ íŒŒì¼)
â”‚
â”œâ”€â”€ execution/                        # Execution Layer (ì‹ ê·œ)
â”‚   â”œâ”€â”€ execution_router.py          # 234 lines
â”‚   â”œâ”€â”€ order_validator.py           # 187 lines
â”‚   â””â”€â”€ shadow_trading_engine.py     # 456 lines
â”‚
â””â”€â”€ routers/
    â”œâ”€â”€ war_room_mvp_router.py       # MVP API (ì‹ ê·œ)
    â””â”€â”€ war_room_router.py           # Legacy API (ìœ ì§€)
```

### API Endpoints

**MVP System**:
```http
POST /api/war-room-mvp/deliberate
GET  /api/war-room-mvp/session/{session_id}
GET  /api/war-room-mvp/sessions
GET  /api/war-room-mvp/info
GET  /api/war-room-mvp/shadow/status
GET  /api/war-room-mvp/shadow/performance
GET  /api/war-room-mvp/shadow/positions
```

**Legacy System** (ìœ ì§€):
```http
POST /api/war-room/debate
GET  /api/war-room/session/{session_id}
GET  /api/war-room/sessions
```

### Environment Variables

```bash
# Dual Mode ì§€ì›
WAR_ROOM_MVP_USE_SKILLS=false  # true: Skill Handler, false: Direct Class

# AI Models
GEMINI_API_KEY=your_key_here

# Feature Flags
ENABLE_SHADOW_TRADING=true
ENABLE_DEEP_REASONING=true

# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/ai_trading
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### ë³‘ë ¬ ì‹¤í–‰

**Legacy (ìˆœì°¨ ì‹¤í–‰)**:
```python
# 30ì´ˆ ì†Œìš”
opinions = []
for agent in [trader, risk, analyst, macro, institutional, news, chipwar, pm]:
    opinion = agent.analyze(...)  # ê° 3-4ì´ˆ
    opinions.append(opinion)
```

**MVP (ë³‘ë ¬ ì‹¤í–‰)**:
```python
# 10ì´ˆ ì†Œìš”
import asyncio

async def parallel_analysis():
    trader_task = asyncio.create_task(trader_mvp.analyze(...))
    risk_task = asyncio.create_task(risk_mvp.analyze(...))
    analyst_task = asyncio.create_task(analyst_mvp.analyze(...))

    # 3ê°œ ë™ì‹œ ì‹¤í–‰
    trader_result, risk_result, analyst_result = await asyncio.gather(
        trader_task, risk_task, analyst_task
    )

    # PM AgentëŠ” ìˆœì°¨ (3ê°œ ê²°ê³¼ í•„ìš”)
    pm_result = await pm_mvp.make_final_decision(
        trader_result, risk_result, analyst_result
    )

    return pm_result
```

### ìºì‹± ì „ëµ

```python
from functools import lru_cache
from datetime import datetime, timedelta

# Market data 5ë¶„ ìºì‹±
@cache_with_ttl(300)
def get_market_conditions():
    return fetch_from_api()

# Macro context 1ì‹œê°„ ìºì‹±
@cache_with_ttl(3600)
def get_macro_context():
    return fetch_macro_data()
```

### Database ì¿¼ë¦¬ ìµœì í™”

```python
# N+1 ì¿¼ë¦¬ ì œê±°
from sqlalchemy.orm import selectinload

# Before (N+1)
sessions = db.query(WarRoomSession).all()
for session in sessions:
    opinions = session.agent_opinions  # Nê°œ ì¿¼ë¦¬!

# After (ë‹¨ì¼ ì¿¼ë¦¬)
sessions = db.query(WarRoomSession).options(
    selectinload(WarRoomSession.agent_opinions)
).all()
```

### ì„±ëŠ¥ ë©”íŠ¸ë¦­ (í˜„ì¬)

| ì§€í‘œ | ëª©í‘œ | í˜„ì¬ | Status |
|------|------|------|--------|
| **ì „ì²´ ì‘ë‹µ ì‹œê°„** | <15s | 12.76s | âœ… |
| **DB ì¿¼ë¦¬ ì‹œê°„** | <1s | 0.3-0.5s | âœ… |
| **Gemini API í˜¸ì¶œ** (3íšŒ) | <12s | ~9s | âœ… |
| **Processing ì‹œê°„** | <5s | ~3s | âœ… |
| **ë¹„ìš©/íšŒ** | <$0.05 | $0.035 | âœ… |

---

## ğŸ”® í–¥í›„ ê³„íš

### Short-term (1-2ê°œì›”)

1. **News Agent Enhancement** (P0 - ì¦‰ì‹œ ì°©ìˆ˜)
   - Analyst MVPì— Macro Context í†µí•©
   - Claude APIë¡œ ë‰´ìŠ¤ í•´ì„
   - DBì— í•´ì„ ê²°ê³¼ ì €ì¥

2. **Daily Report Generation** (P1)
   - PDF ë³´ê³ ì„œ ìë™ ìƒì„±
   - Shadow Trading ì„±ê³¼ ìš”ì•½
   - Telegram ë°°í¬

3. **Frontend Optimization** (P1)
   - War Room MVP UI ì—…ë°ì´íŠ¸
   - ë²ˆë“¤ í¬ê¸° 20% ê°ì†Œ
   - API í´ë§ â†’ WebSocket ì „í™˜

### Mid-term (3-6ê°œì›”)

4. **Database Phase 2 Optimization** (P2)
   - TimescaleDB hypertable í™œì„±í™”
   - pgvector ì„ë² ë”© ê²€ìƒ‰
   - Materialized Views

5. **Shadow Trading ê²€ì¦ ì™„ë£Œ** (3ê°œì›”, ~2026-04-01)
   - Success Criteria í‰ê°€
   - Live Trading ì „í™˜ ê²°ì •

6. **Test Coverage í–¥ìƒ** (P2)
   - 60% â†’ 90% coverage
   - MVP Agent ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
   - E2E í…ŒìŠ¤íŠ¸

### Long-term (6-12ê°œì›”)

7. **Production Deployment**
   - Shadow Trading ì„±ê³µ ì‹œ ì‹¤ì œ ìê¸ˆ íˆ¬ì…
   - Monitoring & Alerting (Prometheus + Grafana)
   - Sentry error tracking

8. **Advanced Features**
   - Multi-portfolio support
   - Options trading
   - Automated rebalancing
   - ML-based signal optimization

9. **MVP 2.0**
   - Agent ì¶”ê°€ (Crypto Agent, Options Agent)
   - Reinforcement Learning í†µí•©
   - Self-improvement loop

---

## ğŸ“š References

### ê´€ë ¨ ë¬¸ì„œ
- [260104_Current_System_State.md](260104_Current_System_State.md) - í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ
- [260104_Database_Schema.md](260104_Database_Schema.md) - ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ
- [2025_System_Overview.md](2025_System_Overview.md) - ì‹œìŠ¤í…œ ê°œìš”
- [2025_Agent_Catalog.md](2025_Agent_Catalog.md) - Agent ì¹´íƒˆë¡œê·¸
- [2025_Implementation_Progress.md](2025_Implementation_Progress.md) - êµ¬í˜„ ì§„í–‰ ìƒí™©

### ì½”ë“œ íŒŒì¼
- `backend/ai/mvp/` - MVP Agent êµ¬í˜„
- `backend/execution/` - Execution Layer
- `backend/routers/war_room_mvp_router.py` - MVP API
- `backend/ai/skills/war_room_mvp/` - Skills Architecture

### Work Logs
- [Work_Log_20260104.md](../Work_Log_20260104.md) - Shadow Trading ëª¨ë‹ˆí„°ë§
- [Work_Log_20260103.md](../Work_Log_20260103.md) - Shadow Trading ë°ì´í„° ë³µì›
- [Work_Log_20260102.md](../Work_Log_20260102.md) - DB ìµœì í™”

---

**Document Created**: 2026-01-04
**Next Review**: 2026-02-01
**Version**: 1.0
**Status**: âœ… Production Ready

---

**End of Document**
