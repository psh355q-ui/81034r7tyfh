# Performance Tuning Guide - AI Trading System

**Last Updated**: 2025-12-14
**Category**: Deployment & Optimization
**Audience**: Developers, DevOps Engineers

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”](#ë°ì´í„°ë² ì´ìŠ¤-ìµœì í™”)
3. [Redis ìºì‹± ì „ëµ](#redis-ìºì‹±-ì „ëµ)
4. [FastAPI ì„±ëŠ¥ íŠœë‹](#fastapi-ì„±ëŠ¥-íŠœë‹)
5. [AI API ìµœì í™”](#ai-api-ìµœì í™”)
6. [í”„ë¡ íŠ¸ì—”ë“œ ìµœì í™”](#í”„ë¡ íŠ¸ì—”ë“œ-ìµœì í™”)
7. [ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§](#ëª¨ë‹ˆí„°ë§-ë°-í”„ë¡œíŒŒì¼ë§)
8. [ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬](#ì„±ëŠ¥-ë²¤ì¹˜ë§ˆí¬)

---

## ê°œìš”

AI Trading Systemì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„ê³¼ ë†’ì€ ì²˜ë¦¬ëŸ‰ì„ ë‹¬ì„±í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

### ì„±ëŠ¥ ëª©í‘œ

| ë©”íŠ¸ë¦­ | ëª©í‘œê°’ | í˜„ì¬ê°’ |
|--------|--------|--------|
| Health Check ì‘ë‹µ | < 100ms | ~50ms âœ… |
| Dashboard ë¡œë”© | < 500ms | ~300ms âœ… |
| AI ë¶„ì„ (Deep Reasoning) | < 10s | ~5-8s âœ… |
| API ì—ëŸ¬ìœ¨ | < 1% | ~0.1% âœ… |
| ë™ì‹œ ì‚¬ìš©ì | > 100 | 500+ âœ… |

---

## ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

### 1. PostgreSQL ì„¤ì •

#### ë©”ëª¨ë¦¬ ì„¤ì •

```yaml
# docker-compose.yml
services:
  postgres:
    image: timescale/timescaledb:latest-pg15
    environment:
      # ë©”ëª¨ë¦¬ ì„¤ì • (ì‹œìŠ¤í…œ RAMì˜ 25%)
      POSTGRES_SHARED_BUFFERS: 512MB  # 2GB RAM ê¸°ì¤€
      POSTGRES_EFFECTIVE_CACHE_SIZE: 2GB  # 2GB RAM ê¸°ì¤€
      POSTGRES_WORK_MEM: 16MB
      POSTGRES_MAINTENANCE_WORK_MEM: 128MB

      # ì—°ê²° ì„¤ì •
      POSTGRES_MAX_CONNECTIONS: 200

      # WAL ì„¤ì •
      POSTGRES_WAL_BUFFERS: 16MB
      POSTGRES_CHECKPOINT_COMPLETION_TARGET: 0.9
```

#### ì¸ë±ìŠ¤ ìµœì í™”

```sql
-- ìì£¼ ì¿¼ë¦¬í•˜ëŠ” ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_news_articles_ticker ON news_articles(ticker);
CREATE INDEX idx_news_articles_published_at ON news_articles(published_at DESC);
CREATE INDEX idx_positions_user_ticker ON positions(user_id, ticker);

-- ë³µí•© ì¸ë±ìŠ¤
CREATE INDEX idx_news_articles_ticker_published
ON news_articles(ticker, published_at DESC);

-- ë¶€ë¶„ ì¸ë±ìŠ¤ (ì¡°ê±´ë¶€)
CREATE INDEX idx_active_positions
ON positions(user_id, ticker)
WHERE status = 'active';
```

#### ì¿¼ë¦¬ ìµœì í™”

```python
# âœ… íš¨ìœ¨ì ì¸ ì¿¼ë¦¬
from sqlalchemy import select
from sqlalchemy.orm import selectinload

# Eager loadingìœ¼ë¡œ N+1 ë¬¸ì œ ë°©ì§€
stmt = (
    select(Position)
    .options(selectinload(Position.user))
    .where(Position.status == "active")
    .limit(100)
)

# âŒ ë¹„íš¨ìœ¨ì ì¸ ì¿¼ë¦¬
positions = session.query(Position).all()  # ëª¨ë“  ë°ì´í„° ë¡œë“œ
for p in positions:
    print(p.user.name)  # N+1 ì¿¼ë¦¬ ë°œìƒ!
```

### 2. TimescaleDB ìµœì í™”

```sql
-- Hypertable ì••ì¶• (ì‹œê³„ì—´ ë°ì´í„°)
ALTER TABLE stock_prices
SET (timescaledb.compress,
     timescaledb.compress_segmentby = 'ticker');

-- ì••ì¶• ì •ì±… (7ì¼ ì´ìƒ ëœ ë°ì´í„°)
SELECT add_compression_policy('stock_prices', INTERVAL '7 days');

-- ìë™ vacuum ì„¤ì •
ALTER TABLE stock_prices
SET (autovacuum_vacuum_scale_factor = 0.01);
```

### 3. ì—°ê²° í’€ ìµœì í™”

```python
# backend/database/connection.py
from sqlalchemy.ext.asyncio import create_async_engine

engine = create_async_engine(
    DATABASE_URL,
    # ì—°ê²° í’€ ì„¤ì •
    pool_size=20,  # ê¸°ë³¸ ì—°ê²° ìˆ˜
    max_overflow=10,  # ìµœëŒ€ ì¶”ê°€ ì—°ê²°
    pool_timeout=30,  # ì—°ê²° ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    pool_recycle=3600,  # 1ì‹œê°„ë§ˆë‹¤ ì—°ê²° ì¬ìƒì„±
    pool_pre_ping=True,  # ì—°ê²° ìœ íš¨ì„± ì²´í¬
    echo=False,  # í”„ë¡œë•ì…˜ì—ì„œëŠ” False
)
```

---

## Redis ìºì‹± ì „ëµ

### 1. ìºì‹œ ë ˆì´ì–´ ì„¤ê³„

```python
# backend/core/cache.py
import redis
from functools import wraps
import json

class CacheManager:
    def __init__(self):
        self.redis_client = redis.from_url(
            os.getenv("REDIS_URL"),
            decode_responses=True,
            max_connections=50
        )

    def cached(self, ttl: int = 300):
        """ë°ì½”ë ˆì´í„°: í•¨ìˆ˜ ê²°ê³¼ ìºì‹±"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # ìºì‹œ í‚¤ ìƒì„±
                cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"

                # ìºì‹œ ì¡°íšŒ
                cached_value = self.redis_client.get(cache_key)
                if cached_value:
                    return json.loads(cached_value)

                # ìºì‹œ ë¯¸ìŠ¤: í•¨ìˆ˜ ì‹¤í–‰
                result = await func(*args, **kwargs)

                # ìºì‹œ ì €ì¥
                self.redis_client.setex(
                    cache_key,
                    ttl,
                    json.dumps(result, default=str)
                )

                return result
            return wrapper
        return decorator

cache_manager = CacheManager()
```

### 2. ì‚¬ìš© ì˜ˆì‹œ

```python
# ì£¼ê°€ ë°ì´í„° ìºì‹± (5ë¶„)
@cache_manager.cached(ttl=300)
async def get_stock_price(ticker: str):
    # ì™¸ë¶€ API í˜¸ì¶œ (ëŠë¦¼)
    return await yahoo_finance.get_price(ticker)

# ë‰´ìŠ¤ ìºì‹± (1ì‹œê°„)
@cache_manager.cached(ttl=3600)
async def get_news(ticker: str):
    return await news_api.fetch_news(ticker)

# Feature Store ìºì‹± (10ë¶„)
@cache_manager.cached(ttl=600)
async def get_features(ticker: str):
    return await feature_store.get_features(ticker)
```

### 3. ìºì‹œ ë¬´íš¨í™” ì „ëµ

```python
class CacheInvalidation:
    """ìºì‹œ ë¬´íš¨í™” ê´€ë¦¬"""

    def invalidate_pattern(self, pattern: str):
        """íŒ¨í„´ì— ë§ëŠ” ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        keys = self.redis_client.keys(pattern)
        if keys:
            self.redis_client.delete(*keys)

    def invalidate_ticker(self, ticker: str):
        """íŠ¹ì • ì¢…ëª© ê´€ë ¨ ìºì‹œ ì‚­ì œ"""
        patterns = [
            f"get_stock_price:*{ticker}*",
            f"get_news:*{ticker}*",
            f"get_features:*{ticker}*",
        ]
        for pattern in patterns:
            self.invalidate_pattern(pattern)
```

### 4. Redis ë©”ëª¨ë¦¬ ìµœì í™”

```bash
# redis.conf
maxmemory 512mb
maxmemory-policy allkeys-lru  # LRU ì •ì±…

# ì••ì¶• ì„¤ì •
list-compress-depth 1
list-max-ziplist-size -2
```

---

## FastAPI ì„±ëŠ¥ íŠœë‹

### 1. ë¹„ë™ê¸° ì²˜ë¦¬

```python
# âœ… ë¹„ë™ê¸° endpoint
@app.get("/api/analyze/{ticker}")
async def analyze_ticker(ticker: str):
    # ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬
    news, price, features = await asyncio.gather(
        get_news(ticker),
        get_stock_price(ticker),
        get_features(ticker)
    )

    return {
        "news": news,
        "price": price,
        "features": features
    }

# âŒ ë™ê¸° endpoint (ëŠë¦¼)
@app.get("/api/analyze/{ticker}")
def analyze_ticker_sync(ticker: str):
    news = get_news_sync(ticker)  # ëŒ€ê¸°
    price = get_stock_price_sync(ticker)  # ëŒ€ê¸°
    features = get_features_sync(ticker)  # ëŒ€ê¸°
    return {...}
```

### 2. Response Compression

```python
# backend/main.py
from fastapi.middleware.gzip import GZipMiddleware

app.add_middleware(
    GZipMiddleware,
    minimum_size=1000  # 1KB ì´ìƒë§Œ ì••ì¶•
)
```

### 3. Connection Pooling

```python
# HTTP í´ë¼ì´ì–¸íŠ¸ ì¬ì‚¬ìš©
import httpx

class APIClient:
    def __init__(self):
        self.client = httpx.AsyncClient(
            timeout=30.0,
            limits=httpx.Limits(
                max_keepalive_connections=20,
                max_connections=100
            )
        )

    async def fetch(self, url: str):
        response = await self.client.get(url)
        return response.json()

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
api_client = APIClient()
```

---

## AI API ìµœì í™”

### 1. ìš”ì²­ ë°°ì¹­

```python
# ì—¬ëŸ¬ ì¢…ëª©ì„ í•œ ë²ˆì— ë¶„ì„
class BatchAnalyzer:
    async def analyze_batch(self, tickers: List[str]) -> Dict:
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ API í˜¸ì¶œ ìµœì†Œí™”"""

        # ë‰´ìŠ¤ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
        all_news = await self.fetch_news_batch(tickers)

        # AI ë¶„ì„ (ë³‘ë ¬)
        tasks = [
            self.analyze_ticker(ticker, all_news[ticker])
            for ticker in tickers
        ]

        results = await asyncio.gather(*tasks)
        return dict(zip(tickers, results))
```

### 2. í”„ë¡¬í”„íŠ¸ ìµœì í™”

```python
# âœ… ì§§ê³  íš¨ìœ¨ì ì¸ í”„ë¡¬í”„íŠ¸
prompt = f"""Analyze {ticker}:
News: {news_summary}  # ìš”ì•½ë³¸ë§Œ
Action: buy/sell/hold
Reason: 1 sentence"""

# âŒ ê¸´ í”„ë¡¬í”„íŠ¸ (í† í° ë‚­ë¹„)
prompt = f"""Please provide a comprehensive analysis...
{full_news_articles}  # ì „ì²´ ê¸°ì‚¬
Please explain in detail..."""
```

### 3. ìºì‹± + Fallback

```python
async def get_ai_analysis(ticker: str):
    # 1. ìºì‹œ í™•ì¸
    cached = await cache.get(f"analysis:{ticker}")
    if cached:
        return cached

    try:
        # 2. Claude API í˜¸ì¶œ
        result = await claude_api.analyze(ticker)
    except Exception as e:
        # 3. Fallback: Gemini (ë” ì €ë ´)
        logger.warning(f"Claude failed, using Gemini: {e}")
        result = await gemini_api.analyze(ticker)

    # 4. ìºì‹œ ì €ì¥ (1ì‹œê°„)
    await cache.set(f"analysis:{ticker}", result, ttl=3600)

    return result
```

### 4. Rate Limiting

```python
from asyncio import Semaphore

class AIRateLimiter:
    def __init__(self, max_concurrent=5):
        self.semaphore = Semaphore(max_concurrent)

    async def call_api(self, func, *args, **kwargs):
        async with self.semaphore:
            # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ë™ì‹œ ì‹¤í–‰
            return await func(*args, **kwargs)

rate_limiter = AIRateLimiter(max_concurrent=5)
```

---

## í”„ë¡ íŠ¸ì—”ë“œ ìµœì í™”

### 1. Code Splitting

```typescript
// Lazy loading
const Dashboard = lazy(() => import('./pages/Dashboard'));
const Analysis = lazy(() => import('./pages/Analysis'));

function App() {
  return (
    <Suspense fallback={<Loading />}>
      <Routes>
        <Route path="/dashboard" element={<Dashboard />} />
        <Route path="/analysis" element={<Analysis />} />
      </Routes>
    </Suspense>
  );
}
```

### 2. API ìš”ì²­ ìµœì í™”

```typescript
// React Queryë¡œ ìºì‹±
import { useQuery } from '@tanstack/react-query';

function Dashboard() {
  const { data } = useQuery({
    queryKey: ['dashboard'],
    queryFn: fetchDashboard,
    staleTime: 5 * 60 * 1000,  // 5ë¶„ê°„ ìºì‹œ
    cacheTime: 10 * 60 * 1000,  // 10ë¶„ê°„ ìœ ì§€
  });

  return <div>{data}</div>;
}
```

### 3. ì´ë¯¸ì§€ ìµœì í™”

```typescript
// WebP í˜•ì‹ + Lazy loading
<img
  src="chart.webp"
  loading="lazy"
  alt="Stock Chart"
  width="600"
  height="400"
/>
```

---

## ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§

### 1. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
# backend/middleware/performance.py
import time
from fastapi import Request

@app.middleware("http")
async def track_performance(request: Request, call_next):
    start_time = time.time()

    response = await call_next(request)

    duration = time.time() - start_time

    # ë¡œê¹…
    logger.api_request(
        endpoint=request.url.path,
        method=request.method,
        status_code=response.status_code,
        duration=duration * 1000,  # ms
    )

    # ëŠë¦° ìš”ì²­ ê²½ê³ 
    if duration > 1.0:
        logger.warning(
            f"Slow request: {request.url.path} took {duration:.2f}s"
        )

    response.headers["X-Process-Time"] = str(duration)
    return response
```

### 2. í”„ë¡œíŒŒì¼ë§

```python
# í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
import cProfile
import pstats

def profile_function(func):
    profiler = cProfile.Profile()
    profiler.enable()

    result = func()

    profiler.disable()
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(10)  # ìƒìœ„ 10ê°œ

    return result
```

### 3. Database Query Logging

```python
# ëŠë¦° ì¿¼ë¦¬ ê°ì§€
import logging

logging.basicConfig()
logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)

# ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ë¡œê¹…
@event.listens_for(Engine, "before_cursor_execute")
def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    conn.info.setdefault('query_start_time', []).append(time.time())

@event.listens_for(Engine, "after_cursor_execute")
def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total = time.time() - conn.info['query_start_time'].pop()
    if total > 1.0:  # 1ì´ˆ ì´ìƒ
        logger.warning(f"Slow query ({total:.2f}s): {statement}")
```

---

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### 1. API ë¶€í•˜ í…ŒìŠ¤íŠ¸

```bash
# Locustë¡œ ë¶€í•˜ í…ŒìŠ¤íŠ¸
pip install locust

# locustfile.py
from locust import HttpUser, task, between

class TradingSystemUser(HttpUser):
    wait_time = between(1, 3)

    @task(3)
    def view_dashboard(self):
        self.client.get("/api/dashboard")

    @task(1)
    def analyze_stock(self):
        self.client.post("/api/reasoning/analyze", json={
            "ticker": "NVDA",
            "news_context": "Test news"
        })

# ì‹¤í–‰
locust -f locustfile.py --host=http://localhost:8001
```

### 2. ëª©í‘œ ì„±ëŠ¥

| Endpoint | p50 | p95 | p99 |
|----------|-----|-----|-----|
| /health | 10ms | 50ms | 100ms |
| /api/dashboard | 100ms | 300ms | 500ms |
| /api/reasoning/analyze | 3s | 8s | 10s |

### 3. ì„±ëŠ¥ ê°œì„  ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥

- [ ] Redis ìºì‹± í™œì„±í™”
- [ ] DB ì—°ê²° í’€ ì„¤ì •
- [ ] Response compression
- [ ] ì¸ë±ìŠ¤ ì¶”ê°€
- [ ] ë¶ˆí•„ìš”í•œ ë¡œê·¸ ì œê±°

#### ì¤‘ê¸° ê°œì„ 

- [ ] CDN ì‚¬ìš© (í”„ë¡ íŠ¸ì—”ë“œ)
- [ ] Database read replica
- [ ] API response ìºì‹±
- [ ] ì´ë¯¸ì§€ ìµœì í™”

#### ì¥ê¸° ê°œì„ 

- [ ] Microservices ë¶„ë¦¬
- [ ] Message Queue (Celery)
- [ ] Load Balancer
- [ ] Auto-scaling

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

```bash
# ì›ì¸ íŒŒì•…
docker stats

# PostgreSQL ë©”ëª¨ë¦¬ ì¤„ì´ê¸°
POSTGRES_SHARED_BUFFERS: 256MB  # 512MB â†’ 256MB

# Redis ë©”ëª¨ë¦¬ ì œí•œ
maxmemory 256mb
```

### 2. ëŠë¦° ì¿¼ë¦¬

```sql
-- ì‹¤í–‰ ê³„íš í™•ì¸
EXPLAIN ANALYZE
SELECT * FROM news_articles WHERE ticker = 'NVDA';

-- ì¸ë±ìŠ¤ ì¶”ê°€
CREATE INDEX idx_news_ticker ON news_articles(ticker);
```

### 3. AI API íƒ€ì„ì•„ì›ƒ

```python
# íƒ€ì„ì•„ì›ƒ ì¦ê°€
async with httpx.AsyncClient(timeout=60.0) as client:
    response = await client.post(...)

# ì¬ì‹œë„ ë¡œì§
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
async def call_ai_api():
    ...
```

---

## ì°¸ê³  ìë£Œ

- [FastAPI Performance](https://fastapi.tiangolo.com/async/)
- [PostgreSQL Performance](https://www.postgresql.org/docs/current/performance-tips.html)
- [Redis Best Practices](https://redis.io/docs/manual/performance/)
- [React Performance](https://react.dev/learn/render-and-commit)

---

**Last Updated**: 2025-12-14
**Maintained by**: AI Trading System Team
