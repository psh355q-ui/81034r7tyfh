# ğŸ·ï¸ í†µí•© íƒœê·¸ ì‹œìŠ¤í…œ (Unified Tagging System)

**Version**: 1.0  
**Created**: 2025-11-22  
**Purpose**: ì „ì²´ ë°ì´í„° ê³„ì¸µì— ìë™ íƒœê·¸ ì ìš©ìœ¼ë¡œ ì²´ê³„ì  ìºì‹± ë° ì¦ë¶„ ì—…ë°ì´íŠ¸ êµ¬í˜„

---

## ğŸ“‹ ëª©ì°¨

1. [í•µì‹¬ ì•„ì´ë””ì–´](#1-í•µì‹¬-ì•„ì´ë””ì–´)
2. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#2-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
3. [ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ](#3-ë°ì´í„°ë² ì´ìŠ¤-ìŠ¤í‚¤ë§ˆ)
4. [íƒœê·¸ ìƒì„± ì „ëµ](#4-íƒœê·¸-ìƒì„±-ì „ëµ)
5. [ì¦ë¶„ ì—…ë°ì´íŠ¸ êµ¬í˜„](#5-ì¦ë¶„-ì—…ë°ì´íŠ¸-êµ¬í˜„)
6. [API ì„¤ê³„](#6-api-ì„¤ê³„)
7. [êµ¬í˜„ ê³„íš](#7-êµ¬í˜„-ê³„íš)
8. [ë¹„ìš© ì ˆê° íš¨ê³¼](#8-ë¹„ìš©-ì ˆê°-íš¨ê³¼)

---

## 1. í•µì‹¬ ì•„ì´ë””ì–´

### 1.1 ë¬¸ì œì  (Before)

```
í˜„ì¬ ì‹œìŠ¤í…œ:
- SEC íŒŒì¼: íƒœê·¸ ì—†ìŒ â†’ ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ
- ë‰´ìŠ¤: íƒœê·¸ ì—†ìŒ â†’ ì „ì²´ ì¬ê²€ìƒ‰
- ì£¼ê°€ ë°ì´í„°: íƒœê·¸ ì—†ìŒ â†’ ê´€ë ¨ ì¢…ëª© ì°¾ê¸° ì–´ë ¤ì›€
- AI ë¶„ì„: íƒœê·¸ ì—†ìŒ â†’ ì¬ì‚¬ìš© ë¶ˆê°€

ê²°ê³¼:
â†’ API í˜¸ì¶œ 1000íšŒ/ì›”
â†’ ë¹„ìš© $10.55/ì›”
â†’ ê²€ìƒ‰ ë¹„íš¨ìœ¨
```

### 1.2 í•´ê²°ì±… (After)

```
í†µí•© íƒœê·¸ ì‹œìŠ¤í…œ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ëª¨ë“  ë°ì´í„°ì— ìë™ íƒœê·¸ ìƒì„±            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ SEC íŒŒì¼   â”‚  â”‚   ë‰´ìŠ¤     â”‚         â”‚
â”‚  â”‚ #AAPL      â”‚  â”‚ #AAPL      â”‚         â”‚
â”‚  â”‚ #Tech      â”‚  â”‚ #iPhone    â”‚         â”‚
â”‚  â”‚ #Q3-2024   â”‚  â”‚ #Earnings  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ ì£¼ê°€ ë°ì´í„° â”‚  â”‚ AI ë¶„ì„    â”‚         â”‚
â”‚  â”‚ #AAPL      â”‚  â”‚ #AAPL      â”‚         â”‚
â”‚  â”‚ #Tech      â”‚  â”‚ #BUY       â”‚         â”‚
â”‚  â”‚ #2024-11   â”‚  â”‚ #High-Conf â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  íƒœê·¸ ê¸°ë°˜ ì¦ë¶„ ì—…ë°ì´íŠ¸                 â”‚
â”‚  1. ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ë‚ ì§œ ì¡°íšŒ            â”‚
â”‚  2. ì‹ ê·œ ë°ì´í„°ë§Œ íƒœê·¸ ê²€ìƒ‰              â”‚
â”‚  3. ì¤‘ë³µ ìë™ ì œê±°                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ê²°ê³¼:
â†’ API í˜¸ì¶œ 30íšŒ/ì›” (97% ê°ì†Œ)
â†’ ë¹„ìš© $0.50/ì›” (95% ì ˆê°)
â†’ ê²€ìƒ‰ ì†ë„ 100ë°° ê°œì„ 
```

### 1.3 í•µì‹¬ ì›ì¹™

1. **ìë™ íƒœê·¸ ìƒì„±**: AI (Claude Haiku)ë¡œ ëª¨ë“  ë°ì´í„°ì— íƒœê·¸ ìë™ ì¶”ì¶œ
2. **ê³„ì¸µì  íƒœê·¸**: ticker â†’ sector â†’ topic â†’ entity
3. **ë‚ ì§œ íƒœê·¸**: ì‹œê°„ ê¸°ë°˜ ì¦ë¶„ ì—…ë°ì´íŠ¸ìš©
4. **ì‹ ë¢°ë„ ì ìˆ˜**: íƒœê·¸ í’ˆì§ˆ ì¶”ì  (0.0~1.0)
5. **ì¤‘ë³µ ì œê±°**: í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ê°ì§€

---

## 2. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Data Ingestion Layer                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ SEC Files   â”‚  â”‚    News     â”‚  â”‚ Stock Pricesâ”‚      â”‚
â”‚  â”‚   (Raw)     â”‚  â”‚   (Raw)     â”‚  â”‚    (Raw)    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Auto-Tagging Engine (AI)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Claude Haiku: Extract Tags                           â”‚â”‚
â”‚  â”‚ Input: "Apple reports Q3 revenue..."                 â”‚â”‚
â”‚  â”‚ Output: {                                             â”‚â”‚
â”‚  â”‚   "ticker": ["AAPL"],                                â”‚â”‚
â”‚  â”‚   "sector": ["Technology", "Consumer Electronics"],  â”‚â”‚
â”‚  â”‚   "topic": ["Earnings", "iPhone", "Revenue"],        â”‚â”‚
â”‚  â”‚   "entity": ["Tim Cook", "iPhone 15"],               â”‚â”‚
â”‚  â”‚   "date": "2024-11-22"                                â”‚â”‚
â”‚  â”‚ }                                                     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Unified Tag Storage                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ PostgreSQL: unified_tags Table                       â”‚â”‚
â”‚  â”‚ - document_type (sec | news | price | analysis)      â”‚â”‚
â”‚  â”‚ - document_id                                         â”‚â”‚
â”‚  â”‚ - tag_type (ticker | sector | topic | entity)        â”‚â”‚
â”‚  â”‚ - tag_value                                           â”‚â”‚
â”‚  â”‚ - confidence (0.0~1.0)                                â”‚â”‚
â”‚  â”‚ - created_at                                          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Tag-Based Query & Update                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Find Docs   â”‚  â”‚ Incremental â”‚  â”‚ Deduplicationâ”‚      â”‚
â”‚  â”‚ by Tags     â”‚  â”‚   Update    â”‚  â”‚   (Hash)     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ë°ì´í„° íë¦„

```
1. ë°ì´í„° ìˆ˜ì§‘
   â†“
2. í•´ì‹œ ê³„ì‚° (SHA-256)
   â†“
3. ì¤‘ë³µ ì²´í¬ (Hash in DB?)
   â”œâ”€ YES â†’ SKIP
   â””â”€ NO â†’ Continue
       â†“
4. AI íƒœê·¸ ìƒì„± (Claude Haiku)
   â†“
5. DB ì €ì¥
   â”œâ”€ ì›ë³¸ ë°ì´í„° (sec_filings / news_articles / stock_prices)
   â””â”€ íƒœê·¸ (unified_tags)
       â†“
6. ì¸ë±ìŠ¤ ìë™ ì—…ë°ì´íŠ¸
   â†“
7. ìºì‹œ ë¬´íš¨í™” (Redis)
```

---

## 3. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

### 3.1 í†µí•© íƒœê·¸ í…Œì´ë¸” (Universal)

```sql
-- ëª¨ë“  ë°ì´í„° íƒ€ì…ì— ì ìš© ê°€ëŠ¥í•œ í†µí•© íƒœê·¸ í…Œì´ë¸”
CREATE TABLE unified_tags (
    id SERIAL PRIMARY KEY,
    
    -- ë¬¸ì„œ ì‹ë³„ (Polymorphic)
    document_type VARCHAR(50) NOT NULL,  -- 'sec_filing' | 'news_article' | 'stock_price' | 'ai_analysis'
    document_id INTEGER NOT NULL,        -- í•´ë‹¹ í…Œì´ë¸”ì˜ ID
    
    -- íƒœê·¸ ì •ë³´
    tag_type VARCHAR(50) NOT NULL,       -- 'ticker' | 'sector' | 'topic' | 'entity' | 'date' | 'sentiment'
    tag_value VARCHAR(200) NOT NULL,     -- 'AAPL' | 'Technology' | 'Earnings' | 'Tim Cook'
    confidence REAL NOT NULL DEFAULT 1.0, -- AI ì‹ ë¢°ë„ (0.0~1.0)
    
    -- ë©”íƒ€ë°ì´í„°
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    created_by VARCHAR(50) DEFAULT 'auto',  -- 'auto' | 'manual' | 'user'
    
    -- ì œì•½ ì¡°ê±´
    CONSTRAINT valid_tag_type CHECK (
        tag_type IN ('ticker', 'sector', 'topic', 'entity', 'date', 'sentiment', 'geographic')
    ),
    CONSTRAINT valid_confidence CHECK (confidence >= 0.0 AND confidence <= 1.0),
    UNIQUE (document_type, document_id, tag_type, tag_value)
);

-- ì¸ë±ìŠ¤ (ê²€ìƒ‰ ìµœì í™”)
CREATE INDEX idx_tags_type_value ON unified_tags(tag_type, tag_value);
CREATE INDEX idx_tags_document ON unified_tags(document_type, document_id);
CREATE INDEX idx_tags_created ON unified_tags(created_at DESC);
CREATE INDEX idx_tags_confidence ON unified_tags(confidence DESC);

-- ë³µí•© ì¸ë±ìŠ¤ (ë‹¤ì¤‘ íƒœê·¸ ê²€ìƒ‰)
CREATE INDEX idx_tags_multi_lookup ON unified_tags(
    tag_type, tag_value, document_type, confidence
);
```

### 3.2 ë¬¸ì„œ í•´ì‹œ í…Œì´ë¸” (ì¤‘ë³µ ì œê±°)

```sql
-- ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ë°©ì§€ìš© í•´ì‹œ í…Œì´ë¸”
CREATE TABLE document_hashes (
    id SERIAL PRIMARY KEY,
    
    document_type VARCHAR(50) NOT NULL,
    document_id INTEGER NOT NULL,
    content_hash VARCHAR(64) NOT NULL,  -- SHA-256
    
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    UNIQUE (document_type, content_hash),
    UNIQUE (document_type, document_id)
);

CREATE INDEX idx_hash_lookup ON document_hashes(document_type, content_hash);
```

### 3.3 ì¦ë¶„ ì—…ë°ì´íŠ¸ ì¶”ì  í…Œì´ë¸”

```sql
-- ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œì  ì¶”ì 
CREATE TABLE tag_sync_status (
    id SERIAL PRIMARY KEY,
    
    data_source VARCHAR(50) NOT NULL,    -- 'sec' | 'news' | 'yahoo' | 'ai_analysis'
    tag_type VARCHAR(50),                 -- NULL = ì „ì²´, 'ticker' = íŠ¹ì • íƒœê·¸ íƒ€ì…
    tag_value VARCHAR(200),               -- NULL = ì „ì²´, 'AAPL' = íŠ¹ì • ì¢…ëª©
    
    last_sync_date TIMESTAMPTZ NOT NULL,
    last_document_date TIMESTAMPTZ,      -- ë§ˆì§€ë§‰ ì²˜ë¦¬ ë¬¸ì„œ ë‚ ì§œ
    documents_processed INTEGER NOT NULL DEFAULT 0,
    
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    UNIQUE (data_source, tag_type, tag_value)
);

CREATE INDEX idx_sync_lookup ON tag_sync_status(data_source, tag_type, tag_value);
```

### 3.4 íƒœê·¸ í†µê³„ ë·° (Materialized)

```sql
-- íƒœê·¸ ì‚¬ìš© í†µê³„ (ì„±ëŠ¥ ìµœì í™”)
CREATE MATERIALIZED VIEW tag_statistics AS
SELECT 
    tag_type,
    tag_value,
    document_type,
    COUNT(DISTINCT document_id) as doc_count,
    AVG(confidence) as avg_confidence,
    MAX(created_at) as last_used,
    MIN(created_at) as first_used
FROM unified_tags
GROUP BY tag_type, tag_value, document_type;

CREATE INDEX idx_tag_stats_lookup ON tag_statistics(tag_type, tag_value);

-- ì¼ì¼ ìë™ ìƒˆë¡œê³ ì¹¨
CREATE OR REPLACE FUNCTION refresh_tag_stats()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY tag_statistics;
END;
$$ LANGUAGE plpgsql;

-- cron job (ë§¤ì¼ 03:00)
-- SELECT cron.schedule('refresh-tag-stats', '0 3 * * *', 'SELECT refresh_tag_stats();');
```

---

## 4. íƒœê·¸ ìƒì„± ì „ëµ

### 4.1 AI ê¸°ë°˜ ìë™ íƒœê·¸ ìƒì„±

#### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

```python
# backend/ai/tag_generator.py
TAG_EXTRACTION_PROMPT = """
You are an expert financial analyst. Extract structured tags from the following content.

Content:
{content}

Extract tags in the following categories:
1. **ticker**: Stock tickers mentioned (e.g., AAPL, MSFT)
2. **sector**: Industry sectors (e.g., Technology, Healthcare)
3. **topic**: Main topics (e.g., Earnings, M&A, Product Launch)
4. **entity**: Named entities (e.g., Tim Cook, iPhone 15, Federal Reserve)
5. **sentiment**: Overall sentiment (POSITIVE | NEUTRAL | NEGATIVE)
6. **date**: Key dates mentioned (YYYY-MM-DD)

Output format (JSON only):
{
  "ticker": ["AAPL"],
  "sector": ["Technology", "Consumer Electronics"],
  "topic": ["Earnings", "Revenue Growth", "iPhone Sales"],
  "entity": ["Tim Cook", "iPhone 15", "App Store"],
  "sentiment": "POSITIVE",
  "date": ["2024-11-22"]
}

Rules:
- Output ONLY valid JSON
- ticker: Use official symbols only
- sector: Use standard GICS sectors
- topic: Maximum 5 topics
- entity: Only significant entities
- confidence: All tags have implicit confidence 0.9 (high quality from AI)

Output:
"""
```

#### êµ¬í˜„

```python
import anthropic
import hashlib
import json
from typing import Dict, List

class AutoTagger:
    """ìë™ íƒœê·¸ ìƒì„± ì—”ì§„"""
    
    def __init__(self):
        self.client = anthropic.Anthropic()
        self.model = "claude-haiku-4"
        
    async def generate_tags(
        self,
        content: str,
        document_type: str,
        document_id: int
    ) -> List[Dict]:
        """
        AIë¡œ íƒœê·¸ ìë™ ìƒì„±
        
        Returns:
            [
                {"tag_type": "ticker", "tag_value": "AAPL", "confidence": 0.95},
                {"tag_type": "sector", "tag_value": "Technology", "confidence": 0.90},
                ...
            ]
        """
        
        # 1. ì½˜í…ì¸  í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ì²´í¬)
        content_hash = hashlib.sha256(content.encode()).hexdigest()
        
        # 2. í•´ì‹œ ì¤‘ë³µ ì²´í¬
        existing = await self._check_duplicate(document_type, content_hash)
        if existing:
            return []  # ì´ë¯¸ ì²˜ë¦¬ë¨
        
        # 3. AI íƒœê·¸ ìƒì„±
        prompt = TAG_EXTRACTION_PROMPT.format(content=content[:2000])  # 2000ì ì œí•œ
        
        response = self.client.messages.create(
            model=self.model,
            max_tokens=1000,
            messages=[{"role": "user", "content": prompt}]
        )
        
        # 4. JSON íŒŒì‹±
        try:
            result_text = response.content[0].text
            result_text = result_text.replace("```json", "").replace("```", "").strip()
            tags_dict = json.loads(result_text)
        except Exception as e:
            logger.error(f"Tag parsing failed: {e}")
            return []
        
        # 5. íƒœê·¸ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
        tags = []
        
        # ticker
        for ticker in tags_dict.get("ticker", []):
            tags.append({
                "document_type": document_type,
                "document_id": document_id,
                "tag_type": "ticker",
                "tag_value": ticker.upper(),
                "confidence": 0.95
            })
        
        # sector
        for sector in tags_dict.get("sector", []):
            tags.append({
                "document_type": document_type,
                "document_id": document_id,
                "tag_type": "sector",
                "tag_value": sector,
                "confidence": 0.90
            })
        
        # topic
        for topic in tags_dict.get("topic", []):
            tags.append({
                "document_type": document_type,
                "document_id": document_id,
                "tag_type": "topic",
                "tag_value": topic,
                "confidence": 0.85
            })
        
        # entity
        for entity in tags_dict.get("entity", []):
            tags.append({
                "document_type": document_type,
                "document_id": document_id,
                "tag_type": "entity",
                "tag_value": entity,
                "confidence": 0.80
            })
        
        # sentiment
        sentiment = tags_dict.get("sentiment")
        if sentiment:
            tags.append({
                "document_type": document_type,
                "document_id": document_id,
                "tag_type": "sentiment",
                "tag_value": sentiment,
                "confidence": 0.85
            })
        
        # date
        for date_str in tags_dict.get("date", []):
            tags.append({
                "document_type": document_type,
                "document_id": document_id,
                "tag_type": "date",
                "tag_value": date_str,
                "confidence": 0.90
            })
        
        # 6. DB ì €ì¥
        await self._save_tags(tags)
        await self._save_hash(document_type, document_id, content_hash)
        
        return tags
    
    async def _check_duplicate(self, document_type: str, content_hash: str) -> bool:
        """í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬"""
        result = await db.execute(
            select(DocumentHash).where(
                DocumentHash.document_type == document_type,
                DocumentHash.content_hash == content_hash
            )
        )
        return result.scalar_one_or_none() is not None
    
    async def _save_tags(self, tags: List[Dict]):
        """íƒœê·¸ DB ì €ì¥"""
        for tag in tags:
            # INSERT ON CONFLICT DO NOTHING (ì¤‘ë³µ ë¬´ì‹œ)
            await db.execute(
                insert(UnifiedTag).values(**tag).on_conflict_do_nothing()
            )
        await db.commit()
    
    async def _save_hash(self, document_type: str, document_id: int, content_hash: str):
        """í•´ì‹œ ì €ì¥"""
        await db.execute(
            insert(DocumentHash).values(
                document_type=document_type,
                document_id=document_id,
                content_hash=content_hash
            ).on_conflict_do_nothing()
        )
        await db.commit()
```

### 4.2 ë£° ê¸°ë°˜ íƒœê·¸ ìƒì„± (ë³´ì¡°)

```python
class RuleBasedTagger:
    """ë£° ê¸°ë°˜ íƒœê·¸ ìƒì„± (AI ë¹„ìš© ì ˆê°)"""
    
    def generate_ticker_tags(self, ticker: str) -> List[Dict]:
        """í‹°ì»¤ ê¸°ë°˜ ìë™ íƒœê·¸"""
        
        # ì„¹í„° ë§¤í•‘
        TICKER_TO_SECTOR = {
            'AAPL': 'Technology',
            'MSFT': 'Technology',
            'GOOGL': 'Communication Services',
            'AMZN': 'Consumer Discretionary',
            'TSLA': 'Consumer Discretionary',
            # ... (S&P 500 ì „ì²´)
        }
        
        tags = [
            {"tag_type": "ticker", "tag_value": ticker, "confidence": 1.0}
        ]
        
        # ì„¹í„° ìë™ ì¶”ê°€
        sector = TICKER_TO_SECTOR.get(ticker)
        if sector:
            tags.append({
                "tag_type": "sector",
                "tag_value": sector,
                "confidence": 1.0
            })
        
        return tags
    
    def generate_date_tags(self, date: datetime) -> List[Dict]:
        """ë‚ ì§œ ê¸°ë°˜ ìë™ íƒœê·¸"""
        return [
            {"tag_type": "date", "tag_value": date.strftime("%Y-%m-%d"), "confidence": 1.0},
            {"tag_type": "date", "tag_value": date.strftime("%Y-%m"), "confidence": 1.0},
            {"tag_type": "date", "tag_value": date.strftime("%Y-Q%q"), "confidence": 1.0}
        ]
```

---

## 5. ì¦ë¶„ ì—…ë°ì´íŠ¸ êµ¬í˜„

### 5.1 SEC íŒŒì¼ ì¦ë¶„ ë‹¤ìš´ë¡œë“œ (íƒœê·¸ ê¸°ë°˜)

```python
# backend/data/sec_incremental.py
class SECIncrementalUpdater:
    """SEC íŒŒì¼ ì¦ë¶„ ì—…ë°ì´íŠ¸ (íƒœê·¸ ê¸°ë°˜)"""
    
    async def update_ticker(self, ticker: str):
        """
        1. ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ë‚ ì§œ ì¡°íšŒ (tag_sync_status)
        2. ì‹ ê·œ íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ
        3. ìë™ íƒœê·¸ ìƒì„±
        4. ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
        """
        
        # 1. ë§ˆì§€ë§‰ ë™ê¸°í™” ë‚ ì§œ
        sync_status = await db.execute(
            select(TagSyncStatus).where(
                TagSyncStatus.data_source == 'sec',
                TagSyncStatus.tag_type == 'ticker',
                TagSyncStatus.tag_value == ticker
            )
        )
        status = sync_status.scalar_one_or_none()
        
        if status:
            last_date = status.last_document_date
        else:
            last_date = datetime.now() - timedelta(days=365*5)  # 5ë…„ ì „
        
        # 2. SEC API í˜¸ì¶œ (ë‚ ì§œ í•„í„°)
        new_filings = await sec_api.get_filings(
            ticker=ticker,
            filing_type=['10-Q', '10-K'],
            after_date=last_date
        )
        
        # 3. ê° íŒŒì¼ ì²˜ë¦¬
        for filing in new_filings:
            # 3.1 ë‹¤ìš´ë¡œë“œ
            content = await self._download_filing(filing['url'])
            
            # 3.2 DB ì €ì¥
            filing_record = await self._save_filing(ticker, filing, content)
            
            # 3.3 íƒœê·¸ ìƒì„± (AI)
            tagger = AutoTagger()
            tags = await tagger.generate_tags(
                content=content[:5000],  # ì²« 5000ìë§Œ
                document_type='sec_filing',
                document_id=filing_record.id
            )
            
            logger.info(f"Generated {len(tags)} tags for {ticker} {filing['type']}")
        
        # 4. ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
        await self._update_sync_status(
            data_source='sec',
            tag_type='ticker',
            tag_value=ticker,
            documents_processed=len(new_filings),
            last_document_date=new_filings[-1]['date'] if new_filings else last_date
        )
        
        return len(new_filings)
```

### 5.2 ë‰´ìŠ¤ ì¦ë¶„ ìˆ˜ì§‘ (íƒœê·¸ ê¸°ë°˜)

```python
# backend/data/news_incremental.py
class NewsIncrementalCollector:
    """ë‰´ìŠ¤ ì¦ë¶„ ìˆ˜ì§‘ (íƒœê·¸ ê¸°ë°˜)"""
    
    async def collect_by_tags(
        self,
        tags: Dict[str, List[str]],  # {"ticker": ["AAPL"], "topic": ["Earnings"]}
        hours: int = 24
    ):
        """
        íƒœê·¸ ê¸°ë°˜ ë‰´ìŠ¤ ìˆ˜ì§‘
        
        Example:
            await collector.collect_by_tags(
                tags={"ticker": ["AAPL", "MSFT"], "topic": ["Earnings"]},
                hours=24
            )
        """
        
        # 1. ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        keywords = []
        if "ticker" in tags:
            keywords.extend(tags["ticker"])
        if "topic" in tags:
            keywords.extend(tags["topic"])
        
        query = " OR ".join(keywords)
        
        # 2. ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œì  í™•ì¸
        last_sync = await self._get_last_sync(tags)
        from_date = last_sync or datetime.now() - timedelta(hours=hours)
        
        # 3. NewsAPI í˜¸ì¶œ
        articles = await newsapi.get_articles(
            query=query,
            from_date=from_date,
            to_date=datetime.now()
        )
        
        # 4. ì¤‘ë³µ ì œê±° (URL í•´ì‹œ)
        new_articles = []
        for article in articles:
            content_hash = hashlib.sha256(article['url'].encode()).hexdigest()
            
            if not await self._is_duplicate('news_article', content_hash):
                new_articles.append(article)
        
        # 5. ì €ì¥ + íƒœê·¸ ìƒì„±
        tagger = AutoTagger()
        for article in new_articles:
            # 5.1 DB ì €ì¥
            article_record = await self._save_article(article)
            
            # 5.2 AI íƒœê·¸ ìƒì„±
            tags = await tagger.generate_tags(
                content=article['title'] + "\n" + article['description'],
                document_type='news_article',
                document_id=article_record.id
            )
        
        # 6. ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
        await self._update_sync_status('news', tags, len(new_articles))
        
        return len(new_articles)
```

### 5.3 ì£¼ê°€ ë°ì´í„° ì¦ë¶„ ì—…ë°ì´íŠ¸ (íƒœê·¸ ê¸°ë°˜)

```python
# backend/data/stock_price_incremental.py
class StockPriceIncrementalUpdater:
    """ì£¼ê°€ ë°ì´í„° ì¦ë¶„ ì—…ë°ì´íŠ¸"""
    
    async def update_ticker(self, ticker: str):
        """
        1. ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ë‚ ì§œ ì¡°íšŒ
        2. ì‹ ê·œ ë°ì´í„°ë§Œ ì¡°íšŒ
        3. ë£° ê¸°ë°˜ íƒœê·¸ ìƒì„± (AI ë¶ˆí•„ìš”)
        """
        
        # 1. ë§ˆì§€ë§‰ ë‚ ì§œ
        last_price = await db.execute(
            select(func.max(StockPrice.time))
            .where(StockPrice.ticker == ticker)
        )
        last_date = last_price.scalar()
        
        if last_date:
            start_date = last_date + timedelta(days=1)
        else:
            start_date = date.today() - timedelta(days=365*5)
        
        # 2. Yahoo Finance ì¡°íšŒ
        df = yf.download(ticker, start=start_date, end=date.today())
        
        if df.empty:
            return 0
        
        # 3. DB ì €ì¥
        new_rows = []
        for index, row in df.iterrows():
            price_record = StockPrice(
                time=index.to_pydatetime(),
                ticker=ticker,
                open=row['Open'],
                high=row['High'],
                low=row['Low'],
                close=row['Close'],
                volume=int(row['Volume'])
            )
            db.add(price_record)
            await db.flush()  # ID ìƒì„±
            
            # 4. ë£° ê¸°ë°˜ íƒœê·¸ (AI ë¶ˆí•„ìš”)
            rule_tagger = RuleBasedTagger()
            tags = []
            
            # ticker íƒœê·¸
            tags.extend(rule_tagger.generate_ticker_tags(ticker))
            
            # date íƒœê·¸
            tags.extend(rule_tagger.generate_date_tags(index.to_pydatetime()))
            
            # ì €ì¥
            for tag in tags:
                tag['document_type'] = 'stock_price'
                tag['document_id'] = price_record.id
                await db.execute(insert(UnifiedTag).values(**tag).on_conflict_do_nothing())
            
            new_rows.append(price_record)
        
        await db.commit()
        
        return len(new_rows)
```

---

## 6. API ì„¤ê³„

### 6.1 íƒœê·¸ ê²€ìƒ‰ API

#### GET /api/tags/search

**Query Parameters**:

```
?tag_type=ticker&tag_value=AAPL&document_type=news_article&min_confidence=0.8&limit=50
```

**Response**:

```json
{
  "total": 125,
  "documents": [
    {
      "document_type": "news_article",
      "document_id": 456,
      "tags": [
        {"type": "ticker", "value": "AAPL", "confidence": 0.95},
        {"type": "sector", "value": "Technology", "confidence": 0.90},
        {"type": "topic", "value": "Earnings", "confidence": 0.85}
      ],
      "created_at": "2024-11-22T10:00:00Z",
      "url": "https://reuters.com/article/123"
    }
  ]
}
```

**êµ¬í˜„**:

```python
@router.get("/api/tags/search")
async def search_by_tags(
    tag_type: str,
    tag_value: str,
    document_type: Optional[str] = None,
    min_confidence: float = 0.0,
    limit: int = 50
):
    """íƒœê·¸ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰"""
    
    query = select(UnifiedTag).where(
        UnifiedTag.tag_type == tag_type,
        UnifiedTag.tag_value == tag_value,
        UnifiedTag.confidence >= min_confidence
    )
    
    if document_type:
        query = query.where(UnifiedTag.document_type == document_type)
    
    query = query.order_by(UnifiedTag.created_at.desc()).limit(limit)
    
    results = await db.execute(query)
    tags = results.scalars().all()
    
    # ë¬¸ì„œ ì •ë³´ ì¡°íšŒ
    documents = []
    for tag in tags:
        doc = await get_document(tag.document_type, tag.document_id)
        documents.append({
            "document_type": tag.document_type,
            "document_id": tag.document_id,
            "tags": await get_all_tags(tag.document_type, tag.document_id),
            "created_at": tag.created_at,
            **doc
        })
    
    return {
        "total": len(documents),
        "documents": documents
    }
```

### 6.2 ë‹¤ì¤‘ íƒœê·¸ ê²€ìƒ‰ (AND/OR)

#### POST /api/tags/search/multi

**Request**:

```json
{
  "filters": [
    {"tag_type": "ticker", "tag_value": "AAPL"},
    {"tag_type": "topic", "tag_value": "Earnings"}
  ],
  "operator": "AND",
  "document_type": "news_article",
  "date_range": {
    "start": "2024-11-01",
    "end": "2024-11-22"
  }
}
```

**Response**:

```json
{
  "total": 15,
  "documents": [...]
}
```

### 6.3 íƒœê·¸ í†µê³„ API

#### GET /api/tags/stats

**Response**:

```json
{
  "by_type": {
    "ticker": 1250,
    "sector": 450,
    "topic": 3200,
    "entity": 890
  },
  "top_tickers": [
    {"tag_value": "AAPL", "count": 350},
    {"tag_value": "MSFT", "count": 280}
  ],
  "top_topics": [
    {"tag_value": "Earnings", "count": 450},
    {"tag_value": "M&A", "count": 320}
  ]
}
```

---

## 7. êµ¬í˜„ ê³„íš

### 7.1 Phase 1: ê¸°ë°˜ êµ¬ì¶• (1ì£¼)

**Day 1-2: DB ìŠ¤í‚¤ë§ˆ**

- [ ] `unified_tags` í…Œì´ë¸” ìƒì„±
- [ ] `document_hashes` í…Œì´ë¸” ìƒì„±
- [ ] `tag_sync_status` í…Œì´ë¸” ìƒì„±
- [ ] ì¸ë±ìŠ¤ ìƒì„±
- [ ] Materialized View ìƒì„±

**Day 3-4: íƒœê·¸ ìƒì„± ì—”ì§„**

- [ ] `AutoTagger` í´ë˜ìŠ¤ êµ¬í˜„
- [ ] Claude Haiku í”„ë¡¬í”„íŠ¸ ìµœì í™”
- [ ] `RuleBasedTagger` êµ¬í˜„
- [ ] í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬

**Day 5-7: ì¦ë¶„ ì—…ë°ì´íŠ¸ ë¡œì§**

- [ ] `SECIncrementalUpdater` êµ¬í˜„
- [ ] `NewsIncrementalCollector` êµ¬í˜„
- [ ] `StockPriceIncrementalUpdater` êµ¬í˜„

### 7.2 Phase 2: ê¸°ì¡´ ë°ì´í„° íƒœê·¸ ì ìš© (1ì£¼)

**Day 8-9: Backfill (ê³¼ê±° ë°ì´í„°)**

```bash
# ê¸°ì¡´ SEC íŒŒì¼ íƒœê·¸ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
python scripts/backfill_tags_sec.py

# ê¸°ì¡´ ë‰´ìŠ¤ íƒœê·¸ ìƒì„±
python scripts/backfill_tags_news.py

# ê¸°ì¡´ ì£¼ê°€ ë°ì´í„° íƒœê·¸ ìƒì„±
python scripts/backfill_tags_prices.py
```

**Day 10-11: ê²€ì¦ & ìµœì í™”**

- [ ] íƒœê·¸ í’ˆì§ˆ ê²€ì¦ (ìƒ˜í”Œë§)
- [ ] ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- [ ] ì¸ë±ìŠ¤ ìµœì í™”

### 7.3 Phase 3: API ë° í†µí•© (3ì¼)

**Day 12-13: REST API**

- [ ] `/api/tags/search` êµ¬í˜„
- [ ] `/api/tags/search/multi` êµ¬í˜„
- [ ] `/api/tags/stats` êµ¬í˜„

**Day 14: í†µí•© í…ŒìŠ¤íŠ¸**

- [ ] E2E í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- [ ] ë¹„ìš© ì¸¡ì •

### 7.4 ì¼ì • ìš”ì•½

| Week | ì‘ì—… | ì‚°ì¶œë¬¼ |
|------|------|--------|
| Week 1 | Phase 1: ê¸°ë°˜ êµ¬ì¶• | DB ìŠ¤í‚¤ë§ˆ, íƒœê·¸ ì—”ì§„, ì¦ë¶„ ì—…ë°ì´íŠ¸ |
| Week 2 | Phase 2: Backfill | ê¸°ì¡´ ë°ì´í„° íƒœê·¸ ì ìš© |
| Week 3 | Phase 3: API | REST API, í†µí•© í…ŒìŠ¤íŠ¸ |

**ì´ ì†Œìš” ì‹œê°„**: 3ì£¼

---

## 8. ë¹„ìš© ì ˆê° íš¨ê³¼

### 8.1 ì˜ˆìƒ ë¹„ìš© (ì›”ê°„)

#### Before (íƒœê·¸ ì—†ìŒ)

| ì‘ì—… | íšŸìˆ˜/ì›” | ë¹„ìš©/íšŒ | ì´ ë¹„ìš© |
|------|---------|---------|---------|
| SEC íŒŒì¼ ë‹¤ìš´ë¡œë“œ | 400íšŒ | $0.0075 | $3.00 |
| ë‰´ìŠ¤ ìˆ˜ì§‘ (ì¤‘ë³µ í¬í•¨) | 3000íšŒ | $0.002 | $6.00 |
| AI ë¶„ì„ (ì¤‘ë³µ í¬í•¨) | 1000íšŒ | $0.0143 | $14.30 |
| **í•©ê³„** | - | - | **$23.30** |

#### After (íƒœê·¸ ì ìš©)

| ì‘ì—… | íšŸìˆ˜/ì›” | ë¹„ìš©/íšŒ | ì´ ë¹„ìš© |
|------|---------|---------|---------|
| SEC íŒŒì¼ ë‹¤ìš´ë¡œë“œ | 100íšŒ | $0.0075 | $0.75 |
| ë‰´ìŠ¤ ìˆ˜ì§‘ (ì‹ ê·œë§Œ) | 300íšŒ | $0.002 | $0.60 |
| AI ë¶„ì„ (ì‹ ê·œë§Œ) | 100íšŒ | $0.0143 | $1.43 |
| **íƒœê·¸ ìƒì„±** (AI) | 500íšŒ | $0.0015 | $0.75 |
| **í•©ê³„** | - | - | **$3.53** |

**ì ˆê°ì•¡**: $23.30 - $3.53 = **$19.77/ì›” (85% ì ˆê°)**

### 8.2 ê²€ìƒ‰ ì„±ëŠ¥ ê°œì„ 

#### Before (ì „ì²´ ìŠ¤ìº”)

```sql
-- ë‰´ìŠ¤ ê²€ìƒ‰ (LIKE ì‚¬ìš©)
SELECT * FROM news_articles
WHERE title LIKE '%AAPL%' OR content LIKE '%AAPL%';
-- ì‹¤í–‰ ì‹œê°„: 2.5ì´ˆ (10ë§Œ ê±´ ê¸°ì¤€)
```

#### After (íƒœê·¸ ì¸ë±ìŠ¤)

```sql
-- íƒœê·¸ ê²€ìƒ‰ (ì¸ë±ìŠ¤ ì‚¬ìš©)
SELECT na.* FROM news_articles na
JOIN unified_tags ut ON ut.document_id = na.id
WHERE ut.document_type = 'news_article'
  AND ut.tag_type = 'ticker'
  AND ut.tag_value = 'AAPL';
-- ì‹¤í–‰ ì‹œê°„: 0.025ì´ˆ (100ë°° ë¹ ë¦„)
```

### 8.3 ì €ì¥ ê³µê°„

```
íƒœê·¸ í…Œì´ë¸” í¬ê¸° ì˜ˆìƒ:
- 100ì¢…ëª© Ã— 10 tags/document Ã— 1000 documents = 1M tags
- 1M tags Ã— 200 bytes/tag = 200 MB

ì¸ë±ìŠ¤ í¬ê¸°:
- ì•½ 300 MB

ì´ ì €ì¥ ê³µê°„: ~500 MB (ë§¤ìš° ì‘ìŒ)
```

---

## 9. ëª¨ë‹ˆí„°ë§ ë° ìœ ì§€ë³´ìˆ˜

### 9.1 íƒœê·¸ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

```python
# scripts/monitor_tag_quality.py
async def check_tag_quality():
    """íƒœê·¸ í’ˆì§ˆ ê²€ì¦"""
    
    # 1. ë‚®ì€ ì‹ ë¢°ë„ íƒœê·¸
    low_confidence = await db.execute(
        select(UnifiedTag)
        .where(UnifiedTag.confidence < 0.5)
        .order_by(UnifiedTag.created_at.desc())
        .limit(100)
    )
    
    print(f"Low confidence tags: {len(low_confidence.all())}")
    
    # 2. ì¤‘ë³µ íƒœê·¸ (ê°™ì€ ë¬¸ì„œì— ê°™ì€ íƒœê·¸ ì—¬ëŸ¬ ê°œ)
    duplicates = await db.execute("""
        SELECT document_type, document_id, tag_type, tag_value, COUNT(*) as cnt
        FROM unified_tags
        GROUP BY document_type, document_id, tag_type, tag_value
        HAVING COUNT(*) > 1
    """)
    
    print(f"Duplicate tags: {len(duplicates.all())}")
    
    # 3. ê³ ì•„ íƒœê·¸ (ë¬¸ì„œ ì‚­ì œë¨)
    orphans = await db.execute("""
        SELECT ut.* FROM unified_tags ut
        LEFT JOIN news_articles na ON ut.document_id = na.id AND ut.document_type = 'news_article'
        WHERE ut.document_type = 'news_article' AND na.id IS NULL
    """)
    
    print(f"Orphan tags: {len(orphans.all())}")
```

### 9.2 ìë™ ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸

```python
# scripts/cleanup_tags.py
async def cleanup_tags():
    """ì˜¤ë˜ëœ íƒœê·¸ ì •ë¦¬"""
    
    # 1. 30ì¼ ì´ìƒ ëœ ë‰´ìŠ¤ íƒœê·¸ ì‚­ì œ
    await db.execute("""
        DELETE FROM unified_tags
        WHERE document_type = 'news_article'
          AND created_at < NOW() - INTERVAL '30 days'
    """)
    
    # 2. ê³ ì•„ íƒœê·¸ ì‚­ì œ
    await db.execute("""
        DELETE FROM unified_tags ut
        USING (
            SELECT ut.id FROM unified_tags ut
            LEFT JOIN news_articles na ON ut.document_id = na.id AND ut.document_type = 'news_article'
            WHERE ut.document_type = 'news_article' AND na.id IS NULL
        ) orphans
        WHERE ut.id = orphans.id
    """)
    
    # 3. VACUUM
    await db.execute("VACUUM ANALYZE unified_tags")
```

---

## 10. ë‹¤ìŒ ë‹¨ê³„

### ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥

1. **DB ìŠ¤í‚¤ë§ˆ ìƒì„±**
   ```bash
   alembic revision --autogenerate -m "Add unified tagging system"
   alembic upgrade head
   ```

2. **AutoTagger êµ¬í˜„**
   ```bash
   # backend/ai/tag_generator.py ì‘ì„±
   pytest tests/test_tag_generator.py
   ```

3. **Backfill ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰**
   ```bash
   # ê¸°ì¡´ ë°ì´í„° íƒœê·¸ ìƒì„±
   python scripts/backfill_tags.py --limit 100
   ```

### 1ì£¼ì¼ ë‚´

4. **ì¦ë¶„ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸**
5. **API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„**
6. **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**

### 1ê°œì›” ë‚´

7. **í”„ë¡ íŠ¸ì—”ë“œ íƒœê·¸ ê²€ìƒ‰ UI**
8. **íƒœê·¸ ì¶”ì²œ ì‹œìŠ¤í…œ** (ML ê¸°ë°˜)
9. **ìë™ íƒœê·¸ ì •ì œ** (ì‚¬ìš©ì í”¼ë“œë°±)

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [PostgreSQL Full Text Search](https://www.postgresql.org/docs/current/textsearch.html)
- [Tag-based Architecture](https://martinfowler.com/articles/tag-based-system.html)
- [Incremental ETL Patterns](https://www.databricks.com/glossary/incremental-etl)

---

**ì‘ì„±ì**: Claude (AI Trading System)  
**ë²„ì „**: 1.0  
**ì˜ˆìƒ êµ¬í˜„ ê¸°ê°„**: 3ì£¼  
**ì˜ˆìƒ ë¹„ìš© ì ˆê°**: 85% ($23.30 â†’ $3.53/ì›”)

**ì´ íƒœê·¸ ì‹œìŠ¤í…œìœ¼ë¡œ ë°ì´í„° ê´€ë¦¬ê°€ í˜ì‹ ì ìœ¼ë¡œ ê°œì„ ë©ë‹ˆë‹¤! ğŸš€**
