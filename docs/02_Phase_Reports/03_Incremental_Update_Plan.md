# ğŸš€ Incremental Update ê¸°ë°˜ ë°ì´í„° ì €ì¥ ì‹¤í–‰ ê³„íš

**í”„ë¡œì íŠ¸**: ai-trading-system  
**ëª©í‘œ**: API í˜¸ì¶œ ë¹„ìš©ì„ ìµœì†Œí™”í•˜ëŠ” ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•  
**ì˜ˆìƒ ë¹„ìš© ì ˆê°**: 86% (ì›” $10.55 â†’ $1.51)  
**êµ¬í˜„ ê¸°ê°„**: 1ì£¼ì¼

---

## ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´

### Before (í˜„ì¬)
```
ë§¤ë²ˆ ì „ì²´ ë°ì´í„° ì¡°íšŒ â†’ API í˜¸ì¶œ 1000íšŒ/ì›” â†’ ë¹„ìš© $10.55/ì›”
```

### After (ëª©í‘œ)
```
1. ì´ˆíšŒ: ì „ì²´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ + DB ì €ì¥
2. ì´í›„: ì €ì¥ ì‹œì  ì´í›„ ì‹ ê·œ ë°ì´í„°ë§Œ ì¡°íšŒ
3. ê²°ê³¼: API í˜¸ì¶œ 30íšŒ/ì›” â†’ ë¹„ìš© $1.51/ì›” (86% ì ˆê°)
```

---

## ğŸ“Š Phaseë³„ êµ¬í˜„ ê³„íš

### Phase 1: SEC íŒŒì¼ ì¦ë¶„ ì €ì¥ (ìš°ì„ ìˆœìœ„ 1)

#### 1.1 ë¬¸ì œ ì •ì˜
- **í˜„ì¬**: SEC 10-Q/10-Kë¥¼ ë§¤ë²ˆ ë‹¤ìš´ë¡œë“œ + Gemini íŒŒì‹±
- **ë¹„ìš©**: ì›” 400íšŒ Ã— $0.0075 = $3.00/ì›”
- **ëª©í‘œ**: ì›” 100íšŒ Ã— $0.0075 = $0.75/ì›” (75% ì ˆê°)

#### 1.2 êµ¬í˜„ ì „ëµ

```sql
-- Step 1: ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ìƒì„±
CREATE TABLE sec_filings (
    accession_number VARCHAR(24) PRIMARY KEY,  -- ê³ ìœ  ID
    ticker VARCHAR(20),
    filing_type VARCHAR(10),
    filing_date DATE,
    local_path TEXT,
    file_hash VARCHAR(64),  -- SHA-256 (ì¤‘ë³µ ë°©ì§€)
    download_status VARCHAR(20),
    parse_status VARCHAR(20),
    downloaded_at TIMESTAMPTZ,
    parsed_at TIMESTAMPTZ
);

CREATE INDEX idx_ticker_date ON sec_filings(ticker, filing_date DESC);
```

```python
# Step 2: ì¦ë¶„ ë‹¤ìš´ë¡œë“œ ë¡œì§
async def download_sec_filing_incremental(ticker: str):
    """
    1. DBì—ì„œ ìµœì‹  filing_date ì¡°íšŒ
    2. SEC APIì—ì„œ ìµœì‹  ë‚ ì§œ ì´í›„ ì‹ ê·œ íŒŒì¼ë§Œ ì¡°íšŒ
    3. accession_numberë¡œ ì¤‘ë³µ í™•ì¸
    4. ì‹ ê·œ íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ + ì €ì¥
    """
    
    # 1. ìµœì‹  ë‚ ì§œ í™•ì¸
    last_filing = await db.execute(
        select(func.max(SECFiling.filing_date))
        .where(SECFiling.ticker == ticker)
    )
    last_date = last_filing.scalar() or date.today() - timedelta(days=365*5)
    
    # 2. SEC API í˜¸ì¶œ (ë‚ ì§œ í•„í„°)
    new_filings = await sec_api.get_filings(
        ticker=ticker,
        filing_type=['10-Q', '10-K'],
        after_date=last_date
    )
    
    # 3. ì¤‘ë³µ í•„í„°ë§
    existing_accessions = await db.execute(
        select(SECFiling.accession_number)
        .where(SECFiling.ticker == ticker)
    )
    existing_set = set(existing_accessions.scalars())
    
    new_filings_filtered = [
        f for f in new_filings
        if f['accession'] not in existing_set
    ]
    
    # 4. ì‹ ê·œ íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ
    for filing in new_filings_filtered:
        await download_and_parse(filing)
    
    return len(new_filings_filtered)
```

#### 1.3 ì˜ˆìƒ íš¨ê³¼

| í•­ëª© | Before | After | ì ˆê° |
|------|--------|-------|------|
| API í˜¸ì¶œ | 400íšŒ/ì›” | 100íšŒ/ì›” | 75% |
| ë¹„ìš© | $3.00/ì›” | $0.75/ì›” | $2.25 |
| ì €ì¥ ìš©ëŸ‰ | 0 MB | ~500 MB | - |

---

### Phase 2: Yahoo Finance ì¦ë¶„ ì—…ë°ì´íŠ¸ (ìš°ì„ ìˆœìœ„ 2)

#### 2.1 ë¬¸ì œ ì •ì˜
- **í˜„ì¬**: 5ë…„ ë°ì´í„°ë¥¼ ë§¤ë²ˆ ë‹¤ìš´ë¡œë“œ
- **ì†ë„**: AAPL ì¡°íšŒ ì‹œ 2~5ì´ˆ ì†Œìš”
- **ëª©í‘œ**: 0.1ì´ˆ (DB ì¡°íšŒ) + ì¼ 1íšŒ ì¦ë¶„ ì—…ë°ì´íŠ¸

#### 2.2 êµ¬í˜„ ì „ëµ

```sql
-- Step 1: ì›ë³¸ OHLCV ì €ì¥ (TimescaleDB)
CREATE TABLE stock_prices (
    time TIMESTAMPTZ NOT NULL,
    ticker VARCHAR(20) NOT NULL,
    open DECIMAL(12, 4),
    high DECIMAL(12, 4),
    low DECIMAL(12, 4),
    close DECIMAL(12, 4),
    volume BIGINT,
    adjusted_close DECIMAL(12, 4),
    PRIMARY KEY (time, ticker)
);

SELECT create_hypertable('stock_prices', 'time');

-- Step 2: ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì¶”ì 
CREATE TABLE price_sync_status (
    ticker VARCHAR(20) PRIMARY KEY,
    last_sync_date DATE NOT NULL,
    last_price_date DATE NOT NULL,
    total_rows INTEGER
);
```

```python
# Step 3: ì¦ë¶„ ì—…ë°ì´íŠ¸ ë¡œì§
async def update_stock_prices_incremental(ticker: str):
    """
    1. DBì—ì„œ ìµœì‹  ë‚ ì§œ ì¡°íšŒ
    2. ìµœì‹  ë‚ ì§œ + 1ì¼ ~ ì˜¤ëŠ˜ê¹Œì§€ë§Œ yfinance í˜¸ì¶œ
    3. ì‹ ê·œ ë°ì´í„°ë§Œ DB ì €ì¥
    """
    
    # 1. ìµœì‹  ë‚ ì§œ í™•ì¸
    sync_status = await db.execute(
        select(PriceSyncStatus)
        .where(PriceSyncStatus.ticker == ticker)
    )
    status = sync_status.scalar_one_or_none()
    
    if status:
        start_date = status.last_price_date + timedelta(days=1)
    else:
        start_date = date.today() - timedelta(days=365*5)  # ì´ˆíšŒ: 5ë…„
    
    # 2. ì‹ ê·œ ë°ì´í„°ë§Œ ì¡°íšŒ
    if start_date >= date.today():
        return 0  # ì´ë¯¸ ìµœì‹ 
    
    df = yf.download(ticker, start=start_date, end=date.today())
    
    # 3. DB ì €ì¥
    new_rows = [
        StockPrice(
            time=index.to_pydatetime(),
            ticker=ticker,
            open=row['Open'],
            high=row['High'],
            low=row['Low'],
            close=row['Close'],
            volume=int(row['Volume']),
            adjusted_close=row['Adj Close']
        )
        for index, row in df.iterrows()
    ]
    
    db.add_all(new_rows)
    
    # 4. ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
    if status:
        status.last_sync_date = date.today()
        status.last_price_date = df.index[-1].date()
        status.total_rows += len(new_rows)
    else:
        db.add(PriceSyncStatus(
            ticker=ticker,
            last_sync_date=date.today(),
            last_price_date=df.index[-1].date(),
            total_rows=len(new_rows)
        ))
    
    await db.commit()
    return len(new_rows)
```

#### 2.3 ìŠ¤ì¼€ì¤„ë§

```python
# ì¼ 1íšŒ ìë™ ì—…ë°ì´íŠ¸ (ì¥ ë§ˆê° í›„)
import schedule

async def daily_price_update():
    """ë§¤ì¼ ì˜¤í›„ 5ì‹œ (í•œêµ­ ì‹œê°„) ì‹¤í–‰"""
    tickers = await get_active_tickers()
    
    for ticker in tickers:
        try:
            new_rows = await update_stock_prices_incremental(ticker)
            logger.info(f"{ticker}: {new_rows} new rows added")
        except Exception as e:
            logger.error(f"{ticker} update failed: {e}")
    
schedule.every().day.at("17:00").do(daily_price_update)
```

#### 2.4 ì˜ˆìƒ íš¨ê³¼

| í•­ëª© | Before | After | ê°œì„  |
|------|--------|-------|------|
| ì¡°íšŒ ì†ë„ | 2~5ì´ˆ | 0.1ì´ˆ | 50ë°° ë¹¨ë¼ì§ |
| API í˜¸ì¶œ | 1000íšŒ/ì›” | 30íšŒ/ì›” | 97% ê°ì†Œ |
| ì €ì¥ ìš©ëŸ‰ | 0 MB | ~2 GB | - |

---

### Phase 3: AI ë¶„ì„ ê²°ê³¼ ìºì‹± (ìš°ì„ ìˆœìœ„ 3)

#### 3.1 ë¬¸ì œ ì •ì˜
- **í˜„ì¬**: ê°™ì€ 10-Që¥¼ ì—¬ëŸ¬ ë²ˆ ë¶„ì„ (ë¹„ìš© ë‚­ë¹„)
- **ë¹„ìš©**: ì›” 1000íšŒ Ã— $0.0075 = $7.50/ì›”
- **ëª©í‘œ**: ì›” 100íšŒ Ã— $0.0075 = $0.75/ì›” (90% ì ˆê°)

#### 3.2 êµ¬í˜„ ì „ëµ

```sql
-- Step 1: AI ë¶„ì„ ìºì‹œ í…Œì´ë¸”
CREATE TABLE ai_analysis_cache (
    id SERIAL PRIMARY KEY,
    input_type VARCHAR(50),  -- 'sec_filing' | 'news' | 'stock'
    input_id INTEGER,
    input_hash VARCHAR(64),  -- ì…ë ¥ ë‚´ìš© í•´ì‹œ
    ai_model VARCHAR(50),
    prompt_version INTEGER,  -- í”„ë¡¬í”„íŠ¸ ë²„ì „ ì¶”ì 
    result JSONB,
    cost_usd DECIMAL(10, 6),
    expires_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    
    UNIQUE (input_type, input_id, input_hash, ai_model, prompt_version)
);

CREATE INDEX idx_cache_lookup ON ai_analysis_cache(
    input_type, input_id, ai_model, prompt_version
);
```

```python
# Step 2: ìºì‹œ ìš°ì„  ë¡œì§
async def analyze_with_cache(
    input_type: str,
    input_id: int,
    input_content: str,
    ai_model: str = 'claude-haiku-4',
    prompt_version: int = 1,
    ttl_days: int = None
):
    """
    1. ì…ë ¥ í•´ì‹œ ê³„ì‚°
    2. ìºì‹œ ì¡°íšŒ (input_hash + prompt_version)
    3. ìºì‹œ íˆíŠ¸ â†’ ì¦‰ì‹œ ë°˜í™˜
    4. ìºì‹œ ë¯¸ìŠ¤ â†’ AI ë¶„ì„ â†’ ì €ì¥ â†’ ë°˜í™˜
    """
    
    # 1. ì…ë ¥ í•´ì‹œ
    input_hash = hashlib.sha256(input_content.encode()).hexdigest()
    
    # 2. ìºì‹œ ì¡°íšŒ
    cache = await db.execute(
        select(AIAnalysisCache).where(
            AIAnalysisCache.input_type == input_type,
            AIAnalysisCache.input_id == input_id,
            AIAnalysisCache.input_hash == input_hash,
            AIAnalysisCache.ai_model == ai_model,
            AIAnalysisCache.prompt_version == prompt_version,
            or_(
                AIAnalysisCache.expires_at.is_(None),
                AIAnalysisCache.expires_at > datetime.utcnow()
            )
        )
    )
    
    cached = cache.scalar_one_or_none()
    if cached:
        logger.info(f"Cache HIT: {input_type} #{input_id}")
        return cached.result
    
    # 3. AI ë¶„ì„
    logger.info(f"Cache MISS: {input_type} #{input_id} - calling AI")
    result, cost = await call_ai_model(ai_model, input_content)
    
    # 4. ìºì‹œ ì €ì¥
    new_cache = AIAnalysisCache(
        input_type=input_type,
        input_id=input_id,
        input_hash=input_hash,
        ai_model=ai_model,
        prompt_version=prompt_version,
        result=result,
        cost_usd=cost,
        expires_at=datetime.utcnow() + timedelta(days=ttl_days) if ttl_days else None
    )
    db.add(new_cache)
    await db.commit()
    
    return result
```

#### 3.3 í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬

```python
# í”„ë¡¬í”„íŠ¸ ë³€ê²½ ì‹œ ë²„ì „ ì¦ê°€
PROMPT_VERSIONS = {
    1: "Original 10-point checklist",
    2: "Added risk factors (2024-11-10)",
    3: "Improved Bull/Bear case (2024-11-15)"
}

# ì‚¬ìš© ì˜ˆì‹œ
result = await analyze_with_cache(
    input_type='sec_filing',
    input_id=123,
    input_content=filing_text,
    prompt_version=3,  # ìµœì‹  ë²„ì „
    ttl_days=None  # ë¬´ì œí•œ ìºì‹± (10-QëŠ” ë³€ê²½ ì•ˆ ë¨)
)
```

#### 3.4 ì˜ˆìƒ íš¨ê³¼

| í•­ëª© | Before | After | ì ˆê° |
|------|--------|-------|------|
| AI í˜¸ì¶œ | 1000íšŒ/ì›” | 100íšŒ/ì›” | 90% |
| ë¹„ìš© | $7.50/ì›” | $0.75/ì›” | $6.75 |
| ìºì‹œ íˆíŠ¸ìœ¨ | 0% | 90%+ | - |

---

## ğŸ“… ì‹¤í–‰ ì¼ì •

### Week 1: ê¸°ë°˜ êµ¬ì¶•
- **Day 1**: SEC íŒŒì¼ í…Œì´ë¸” ìƒì„±
- **Day 2**: SEC ì¦ë¶„ ë‹¤ìš´ë¡œë“œ ë¡œì§ êµ¬í˜„
- **Day 3**: Yahoo Finance í…Œì´ë¸” ìƒì„±
- **Day 4**: Yahoo Finance ì¦ë¶„ ì—…ë°ì´íŠ¸ êµ¬í˜„
- **Day 5**: í…ŒìŠ¤íŠ¸ & ê²€ì¦

### Week 2: ìºì‹± & ìµœì í™”
- **Day 6**: AI ë¶„ì„ ìºì‹œ í…Œì´ë¸” ìƒì„±
- **Day 7**: AI ìºì‹œ ë¡œì§ êµ¬í˜„
- **Day 8**: í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬ êµ¬í˜„
- **Day 9**: í†µí•© í…ŒìŠ¤íŠ¸
- **Day 10**: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ & ë¬¸ì„œí™”

---

## ğŸ”§ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: SEC íŒŒì¼ ì €ì¥
- [ ] `sec_filings` í…Œì´ë¸” ìƒì„± (Alembic migration)
- [ ] `sec_filing_extracts` í…Œì´ë¸” ìƒì„±
- [ ] `download_sec_filing_incremental()` í•¨ìˆ˜ êµ¬í˜„
- [ ] accession_number ì¤‘ë³µ ì²´í¬ ë¡œì§
- [ ] íŒŒì¼ í•´ì‹œ (SHA-256) ê³„ì‚°
- [ ] ë¡œì»¬ íŒŒì¼ ì €ì¥ (Synology NAS)
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ (AAPL 5ê°œ íŒŒì¼)

### Phase 2: Yahoo Finance ì¦ë¶„ ì—…ë°ì´íŠ¸
- [ ] `stock_prices` í…Œì´ë¸” ìƒì„± (TimescaleDB hypertable)
- [ ] `price_sync_status` í…Œì´ë¸” ìƒì„±
- [ ] `update_stock_prices_incremental()` í•¨ìˆ˜ êµ¬í˜„
- [ ] ì¼ì¼ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (schedule.py)
- [ ] ì••ì¶• ì •ì±… ì„¤ì • (6ê°œì›” ì´ìƒ ë°ì´í„°)
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ (AAPL 5ë…„ ë°ì´í„°)
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì¡°íšŒ ì†ë„)

### Phase 3: AI ë¶„ì„ ìºì‹±
- [ ] `ai_analysis_cache` í…Œì´ë¸” ìƒì„±
- [ ] `analyze_with_cache()` í•¨ìˆ˜ êµ¬í˜„
- [ ] ì…ë ¥ í•´ì‹œ ê³„ì‚° (SHA-256)
- [ ] í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ
- [ ] TTL ì •ì±… ì„¤ì • (ë‰´ìŠ¤ 7ì¼, SEC ë¬´ì œí•œ)
- [ ] ìºì‹œ ë¬´íš¨í™” ë¡œì§ (í”„ë¡¬í”„íŠ¸ ë³€ê²½ ì‹œ)
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ (ìºì‹œ íˆíŠ¸ìœ¨ ì¸¡ì •)

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ì§€í‘œ

### ì¼ì¼ ì²´í¬
```sql
-- 1. ì˜¤ëŠ˜ ì¶”ê°€ëœ ë°ì´í„° í™•ì¸
SELECT 
    'SEC Filings' as source,
    COUNT(*) as new_rows
FROM sec_filings
WHERE downloaded_at >= CURRENT_DATE

UNION ALL

SELECT 
    'Stock Prices' as source,
    COUNT(*) as new_rows
FROM stock_prices
WHERE time >= CURRENT_DATE;

-- 2. API í˜¸ì¶œ ë¹„ìš© ì¶”ì 
SELECT 
    DATE(created_at) as date,
    SUM(cost_usd) as daily_cost
FROM ai_analysis_cache
WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY DATE(created_at)
ORDER BY date DESC;

-- 3. ìºì‹œ íˆíŠ¸ìœ¨
SELECT 
    input_type,
    COUNT(*) FILTER (WHERE created_at < CURRENT_DATE - INTERVAL '1 hour') as cached,
    COUNT(*) as total,
    ROUND(COUNT(*) FILTER (WHERE created_at < CURRENT_DATE - INTERVAL '1 hour')::numeric / NULLIF(COUNT(*), 0) * 100, 2) as hit_rate
FROM ai_analysis_cache
WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY input_type;
```

### ì£¼ê°„ ë¦¬í¬íŠ¸
```python
# weekly_report.py
async def generate_weekly_report():
    metrics = {
        'sec_filings_added': await count_new_sec_filings(days=7),
        'stock_prices_added': await count_new_stock_prices(days=7),
        'ai_cost': await sum_ai_cost(days=7),
        'cache_hit_rate': await calculate_cache_hit_rate(days=7),
        'api_calls_saved': await count_api_calls_saved(days=7)
    }
    
    report = f"""
    ğŸ“Š ì£¼ê°„ ë¦¬í¬íŠ¸ (Week {week_number})
    
    ğŸ“¥ ë°ì´í„° ìˆ˜ì§‘
    - SEC íŒŒì¼: {metrics['sec_filings_added']}ê°œ ì‹ ê·œ
    - ì£¼ê°€ ë°ì´í„°: {metrics['stock_prices_added']}í–‰ ì¶”ê°€
    
    ğŸ’° ë¹„ìš©
    - AI ë¶„ì„ ë¹„ìš©: ${metrics['ai_cost']:.2f}
    - ì ˆê°ëœ API í˜¸ì¶œ: {metrics['api_calls_saved']}íšŒ
    
    âš¡ ì„±ëŠ¥
    - ìºì‹œ íˆíŠ¸ìœ¨: {metrics['cache_hit_rate']:.1f}%
    """
    
    # Slack/Telegram ì „ì†¡
    await send_notification(report)
```

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

### ê¸°ìˆ  ëª©í‘œ
- [x] SEC íŒŒì¼ ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ 0íšŒ
- [x] Yahoo Finance ì¦ë¶„ ì—…ë°ì´íŠ¸ ì •ìƒ ì‘ë™
- [x] AI ë¶„ì„ ìºì‹œ íˆíŠ¸ìœ¨ > 90%
- [x] ì „ì²´ ì‹œìŠ¤í…œ ì‘ë‹µ ì‹œê°„ < 5ì´ˆ

### ë¹„ìš© ëª©í‘œ
- [x] SEC ë¹„ìš©: $3.00 â†’ $0.75/ì›” (75% ì ˆê°)
- [x] AI ë¹„ìš©: $7.50 â†’ $0.75/ì›” (90% ì ˆê°)
- [x] ì´ ë¹„ìš©: $10.55 â†’ $1.51/ì›” (86% ì ˆê°)

### ì„±ëŠ¥ ëª©í‘œ
- [x] ì£¼ê°€ ì¡°íšŒ ì†ë„: 2~5ì´ˆ â†’ 0.1ì´ˆ (50ë°° ê°œì„ )
- [x] SEC ë¶„ì„ ì‹œê°„: 45ì´ˆ â†’ 5ì´ˆ (9ë°° ê°œì„ , ìºì‹œ ì‹œ)

---

## ğŸš¨ ë¦¬ìŠ¤í¬ & ëŒ€ì‘

### ë¦¬ìŠ¤í¬ 1: ì €ì¥ ê³µê°„ ë¶€ì¡±
**ì¦ìƒ**: Synology NAS ìš©ëŸ‰ ì´ˆê³¼  
**ëŒ€ì‘**:
- ì••ì¶• ì •ì±… ì ìš© (TimescaleDB)
- 6ê°œì›” ì´ìƒ ë°ì´í„° ì‚­ì œ ì •ì±…
- í´ë¼ìš°ë“œ ë°±ì—… (AWS S3 Glacier)

### ë¦¬ìŠ¤í¬ 2: ë°ì´í„° ë¶ˆì¼ì¹˜
**ì¦ìƒ**: DB ë°ì´í„°ì™€ Yahoo Finance ë°ì´í„° ì°¨ì´  
**ëŒ€ì‘**:
- ì¼ì¼ ë°ì´í„° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
- ë¶ˆì¼ì¹˜ ë°œê²¬ ì‹œ ìë™ ì¬ë™ê¸°í™”
- ì•Œë¦¼ ì‹œìŠ¤í…œ (Slack)

### ë¦¬ìŠ¤í¬ 3: API ë ˆì´íŠ¸ ë¦¬ë°‹
**ì¦ìƒ**: Yahoo Finance 429 Too Many Requests  
**ëŒ€ì‘**:
- ì¬ì‹œë„ ë¡œì§ (exponential backoff)
- ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ (100 â†’ 10 ì¢…ëª©)
- í”„ë¡ì‹œ ë¡œí…Œì´ì…˜

---

## ğŸ“š ì°¸ê³  ìë£Œ

### TimescaleDB
- [Hypertable Best Practices](https://docs.timescale.com/use-timescale/latest/hypertables/)
- [Compression](https://docs.timescale.com/use-timescale/latest/compression/)

### SQLAlchemy
- [Async ORM](https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html)
- [Bulk Insert](https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-bulk-insert-statements)

### í•´ì‹±
- [SHA-256 in Python](https://docs.python.org/3/library/hashlib.html)

---

## âœ… ì™„ë£Œ í›„ í™•ì¸ ì‚¬í•­

- [ ] ëª¨ë“  í…Œì´ë¸” ìƒì„± í™•ì¸ (`\dt` in psql)
- [ ] ì¦ë¶„ ì—…ë°ì´íŠ¸ 1íšŒ ì‹¤í–‰ ì„±ê³µ
- [ ] ìºì‹œ íˆíŠ¸ìœ¨ 90% ë‹¬ì„±
- [ ] ì£¼ê°„ ë¦¬í¬íŠ¸ ì •ìƒ ìƒì„±
- [ ] ë¹„ìš© ëª©í‘œ ë‹¬ì„± ($1.51/ì›”)
- [ ] ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± (0.1ì´ˆ ì¡°íšŒ)
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸ (README.md)
- [ ] GitHubì— ì»¤ë°‹ & í‘¸ì‹œ

---

**ì‘ì„±ì**: Claude (AI Trading System)  
**ë²„ì „**: 1.0  
**ì˜ˆìƒ ì™„ë£Œì¼**: 2025-11-29 (1ì£¼ì¼)

**ë‹¤ìŒ ë‹¨ê³„**: ì´ ë¬¸ì„œë¥¼ ë¡œì»¬ì— ì €ì¥ í›„ Phase 1 êµ¬í˜„ ì‹œì‘! ğŸš€
