# Security, DevOps, Advanced Analytics êµ¬í˜„ ê³„íš

**ì‘ì„±ì¼**: 2026-01-03
**ì¹´í…Œê³ ë¦¬**: Security, DevOps, Performance, Analytics, Cloud
**ìš°ì„ ìˆœìœ„**: P1-P3 (High to Low Priority)
**ìƒíƒœ**: ğŸ“‹ Planning Phase

---

## Executive Summary

Claude Code Templatesì˜ **13ê°œ ê³ ê¸‰ ì»´í¬ë„ŒíŠ¸**ì— ëŒ€í•œ êµ¬í˜„ ê³„íšì…ë‹ˆë‹¤. ë³´ì•ˆ ê°•í™”, DevOps ìë™í™”, ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, ê³ ê¸‰ ë¶„ì„, í´ë¼ìš°ë“œ í†µí•©, ì•Œë¦¼ ì‹œìŠ¤í…œì„ ë‹¤ë£¹ë‹ˆë‹¤.

**ê¸°ì¡´ ê³„íš ì™„ë£Œ ë¬¸ì„œ:**
- âœ… [260103_Claude_Code_Templates_Implementation_Plan.md](260103_Claude_Code_Templates_Implementation_Plan.md) - í…ŒìŠ¤íŠ¸ ìë™í™”, í”„ë¡ íŠ¸ì—”ë“œ ìµœì í™”, Git Hooks
- âœ… [260102_Database_Optimization_Plan.md](260102_Database_Optimization_Plan.md) - Database ìµœì í™”

**ë³¸ ë¬¸ì„œ ë²”ìœ„:**
1. ğŸ”’ Security Auditor Agent - API í‚¤ ì•”í˜¸í™”, OWASP ìŠ¤ìº”
2. ğŸš€ DevOps Engineer Agent - CI/CD íŒŒì´í”„ë¼ì¸
3. âš¡ Performance Monitoring - ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¶”ì 
4. ğŸ“Š Data Scientist Agent - Shadow Trading í†µê³„ ë¶„ì„
5. ğŸ¤– NLP Engineer Agent - ë¡œì»¬ ì„ë² ë”©, í‹°ì»¤ ì¶”ì¶œ ê°œì„ 
6. â˜ï¸ AWS Integration - S3 ë°±ì—…, Lambda ë°±í•„
7. ğŸ“¢ Discord/Slack Notifications - ì‹¤ì‹œê°„ ì•Œë¦¼

---

## í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ (2026-01-03 ê¸°ì¤€)

### ë³´ì•ˆ í˜„í™©
| í•­ëª© | í˜„ì¬ ìƒíƒœ | ë¬¸ì œì  | ëª©í‘œ |
|------|-----------|--------|------|
| API í‚¤ ì €ì¥ | .env í‰ë¬¸ | ë…¸ì¶œ ìœ„í—˜ | ì•”í˜¸í™” ì €ì¥ |
| ë³´ì•ˆ ìŠ¤ìº” | ì—†ìŒ | ì·¨ì•½ì  ë¯¸ê°ì§€ | ìë™ ìŠ¤ìº” |
| OWASP Top 10 | ë¯¸ê²€ì¦ | ì•Œ ìˆ˜ ì—†ìŒ | 100% ì¤€ìˆ˜ |
| Secrets ê²€ì¦ | Git hooks ì—†ìŒ | ì»¤ë°‹ ìœ„í—˜ | Pre-commit ì°¨ë‹¨ |

**ìµœê·¼ ì´ìŠˆ:**
- âš ï¸ OpenAI API í• ë‹¹ëŸ‰ ì´ˆê³¼ (2026-01-02 ì´ì „)
- âœ… Kill Switch êµ¬í˜„ ì™„ë£Œ (2026-01-02)

### DevOps í˜„í™©
| í•­ëª© | í˜„ì¬ ìƒíƒœ | ë¬¸ì œì  | ëª©í‘œ |
|------|-----------|--------|------|
| CI/CD | GitHub Actions ê¸°ë³¸ | í…ŒìŠ¤íŠ¸ ë¯¸ì‹¤í–‰ | ì™„ì „ ìë™í™” |
| ë°°í¬ ì‹œê°„ | ìˆ˜ë™ 60ë¶„ | ëŠë¦¼ | ìë™ 5ë¶„ |
| í…ŒìŠ¤íŠ¸ ì‹¤í–‰ | ìˆ˜ë™ | ëˆ„ë½ ìœ„í—˜ | PRë§ˆë‹¤ ìë™ |
| ë¡¤ë°± | ìˆ˜ë™ 30ë¶„ | ë³µêµ¬ ëŠë¦¼ | ìë™ 2ë¶„ |

**ê¸°ì¡´ ì¸í”„ë¼:**
- âœ… Docker Compose êµ¬ì„± ì™„ë£Œ
- âœ… PostgreSQL TimescaleDB (í¬íŠ¸ 5433)
- âœ… Shadow Trading ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸

### ì„±ëŠ¥ í˜„í™©
| í•­ëª© | í˜„ì¬ ê°’ | ëª©í‘œ ê°’ | ê°œì„  ì—¬ì§€ |
|------|---------|---------|----------|
| War Room MVP | 12.76ì´ˆ | < 8ì´ˆ | ë³‘ë ¬í™” |
| ë‰´ìŠ¤ ë°±í•„ ë©”ëª¨ë¦¬ | 450MB | < 200MB | ë°°ì¹˜ ì²˜ë¦¬ |
| API í´ë§ | 1,440 calls/hour | < 240 | WebSocket |

**2026-01-02 ìµœì í™” ì™„ë£Œ:**
- âœ… ë³µí•© ì¸ë±ìŠ¤ 5ê°œ ì¶”ê°€
- âœ… N+1 ì¿¼ë¦¬ ì œê±° (ON CONFLICT)
- âœ… TTL ìºì‹± êµ¬í˜„

### ë¶„ì„ í˜„í™©
| í•­ëª© | í˜„ì¬ ìƒíƒœ | ë¬¸ì œì  | ëª©í‘œ |
|------|-----------|--------|------|
| Shadow Trading ë¶„ì„ | ìˆ˜ë™ ìŠ¤í¬ë¦½íŠ¸ | ë¹„íš¨ìœ¨ | ìë™ ì£¼ê°„ ë¦¬í¬íŠ¸ |
| ë°±í…ŒìŠ¤íŒ… ë©”íŠ¸ë¦­ | ê¸°ë³¸ (Win Rate, PF, MDD) | ë¶€ì¡± | ìƒ¤í”„/ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨ |
| í†µê³„ ê²€ì • | ì—†ìŒ | ìœ ì˜ì„± ë¶ˆëª… | p-value ê³„ì‚° |

### NLP/AI í˜„í™©
| í•­ëª© | í˜„ì¬ ìƒíƒœ | ë¬¸ì œì  | ëª©í‘œ |
|------|-----------|--------|------|
| ë‰´ìŠ¤ ì„ë² ë”© | OpenAI API | ë¹„ìš© $20/month | ë¡œì»¬ ëª¨ë¸ $0 |
| í‹°ì»¤ ì¶”ì¶œ | Regex ê¸°ë°˜ | ì •í™•ë„ ~60% | NER ëª¨ë¸ 90% |
| ê°ì„± ë¶„ì„ | Gemini API | í• ë‹¹ëŸ‰ ì œí•œ | ì•ˆì •ì  ìš´ì˜ |

---

## Part 1: Security & Compliance (ë³´ì•ˆ ë° ê·œì • ì¤€ìˆ˜)

### ëª©í‘œ
- API í‚¤ ë…¸ì¶œ ìœ„í—˜ 100% ì œê±°
- OWASP Top 10 ìë™ ìŠ¤ìº”
- Git ì»¤ë°‹ ì „ Secrets ì°¨ë‹¨
- ì£¼ê°„ ë³´ì•ˆ ê°ì‚¬ ìë™í™”

---

### 1.1 Security Auditor Agent

**ì„¤ì¹˜ ë°©ë²•:**
```bash
npx claude-code-templates@latest --agent security-auditor --yes
```

**ì£¼ìš” ê¸°ëŠ¥:**
1. API í‚¤ ì•”í˜¸í™” ì €ì¥
2. OWASP Top 10 ì·¨ì•½ì  ìŠ¤ìº”
3. SQL Injection íƒì§€
4. XSS ì·¨ì•½ì  ê²€ì‚¬
5. ì ‘ê·¼ ì œì–´ ê²€ì¦

---

#### Implementation 1-1: API í‚¤ ì•”í˜¸í™” ì‹œìŠ¤í…œ

**í˜„ì¬ ë¬¸ì œ:**
```bash
# .env íŒŒì¼ - í‰ë¬¸ ì €ì¥ (ìœ„í—˜!)
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxx
GEMINI_API_KEY=AIzaSyxxxxxxxxxxxxxxxxxxxxxxxx
KIS_APP_KEY=PSxxxxxxxxxxxxxxxx
KIS_APP_SECRET=xxxxxxxxxxxxxxxxxx
DATABASE_URL=postgresql://user:password@localhost:5433/trading
TELEGRAM_BOT_TOKEN=7xxxxxxxxx:AAxxxxxxxxxxxxxxxxxxxxxxxxx
```

**í•´ê²°ì±…: Fernet ì•”í˜¸í™”**

**íŒŒì¼ 1**: `backend/config/secrets_manager.py` (ì‹ ê·œ ìƒì„±)

```python
"""
Secrets Manager - í™˜ê²½ ë³€ìˆ˜ ì•”í˜¸í™” ì €ì¥

Features:
- Fernet ëŒ€ì¹­í‚¤ ì•”í˜¸í™”
- íŒŒì¼ ê¶Œí•œ ì œí•œ (0o600)
- Git ì¶”ì  ë°©ì§€
- ëŸ°íƒ€ì„ ë³µí˜¸í™”

Date: 2026-01-03
Author: AI Trading System Team
"""
import os
from cryptography.fernet import Fernet
from pathlib import Path
import json
from typing import Optional, Dict
import logging

logger = logging.getLogger(__name__)


class SecretsManager:
    """ì•”í˜¸í™”ëœ ì‹œí¬ë¦¿ ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(
        self,
        key_file: str = ".secrets.key",
        secrets_file: str = ".secrets.enc"
    ):
        """
        ì´ˆê¸°í™”

        Args:
            key_file: ì•”í˜¸í™” í‚¤ íŒŒì¼ ê²½ë¡œ
            secrets_file: ì•”í˜¸í™”ëœ ì‹œí¬ë¦¿ íŒŒì¼ ê²½ë¡œ
        """
        self.key_file = Path(key_file)
        self.secrets_file = Path(secrets_file)
        self.key = self._load_or_create_key()
        self.fernet = Fernet(self.key)

    def _load_or_create_key(self) -> bytes:
        """
        ì•”í˜¸í™” í‚¤ ë¡œë“œ ë˜ëŠ” ìƒì„±

        Returns:
            ì•”í˜¸í™” í‚¤ (bytes)
        """
        if self.key_file.exists():
            logger.info(f"Loading existing key from {self.key_file}")
            return self.key_file.read_bytes()

        # ìƒˆ í‚¤ ìƒì„±
        logger.warning(f"Creating new encryption key at {self.key_file}")
        key = Fernet.generate_key()
        self.key_file.write_bytes(key)
        self.key_file.chmod(0o600)  # ì†Œìœ ìë§Œ ì½ê¸°/ì“°ê¸° ê°€ëŠ¥

        return key

    def encrypt_secrets(self, secrets: Dict[str, str]) -> None:
        """
        ì‹œí¬ë¦¿ ì•”í˜¸í™” ì €ì¥

        Args:
            secrets: í‚¤-ê°’ ë”•ì…”ë„ˆë¦¬

        Example:
            manager.encrypt_secrets({
                'OPENAI_API_KEY': 'sk-proj-xxx',
                'DATABASE_URL': 'postgresql://...'
            })
        """
        # JSON ì§ë ¬í™”
        json_data = json.dumps(secrets, indent=2).encode('utf-8')

        # Fernet ì•”í˜¸í™”
        encrypted_data = self.fernet.encrypt(json_data)

        # íŒŒì¼ ì €ì¥
        self.secrets_file.write_bytes(encrypted_data)
        self.secrets_file.chmod(0o600)

        logger.info(f"Encrypted {len(secrets)} secrets to {self.secrets_file}")

    def decrypt_secrets(self) -> Dict[str, str]:
        """
        ì‹œí¬ë¦¿ ë³µí˜¸í™” ë¡œë“œ

        Returns:
            ë³µí˜¸í™”ëœ ì‹œí¬ë¦¿ ë”•ì…”ë„ˆë¦¬

        Raises:
            FileNotFoundError: ì•”í˜¸í™” íŒŒì¼ ì—†ìŒ
            InvalidToken: ë³µí˜¸í™” ì‹¤íŒ¨
        """
        if not self.secrets_file.exists():
            raise FileNotFoundError(
                f"Encrypted secrets file not found: {self.secrets_file}"
            )

        # ì•”í˜¸í™” ë°ì´í„° ë¡œë“œ
        encrypted_data = self.secrets_file.read_bytes()

        # Fernet ë³µí˜¸í™”
        decrypted_data = self.fernet.decrypt(encrypted_data)

        # JSON ì—­ì§ë ¬í™”
        secrets = json.loads(decrypted_data.decode('utf-8'))

        logger.debug(f"Decrypted {len(secrets)} secrets")
        return secrets

    def get_secret(self, key: str, default: Optional[str] = None) -> Optional[str]:
        """
        ê°œë³„ ì‹œí¬ë¦¿ ì¡°íšŒ

        Args:
            key: ì‹œí¬ë¦¿ í‚¤
            default: ê¸°ë³¸ê°’ (ì—†ì„ ê²½ìš°)

        Returns:
            ì‹œí¬ë¦¿ ê°’ ë˜ëŠ” ê¸°ë³¸ê°’
        """
        try:
            secrets = self.decrypt_secrets()
            return secrets.get(key, default)
        except Exception as e:
            logger.error(f"Failed to get secret '{key}': {e}")
            return default

    def update_secret(self, key: str, value: str) -> None:
        """
        ê°œë³„ ì‹œí¬ë¦¿ ì—…ë°ì´íŠ¸

        Args:
            key: ì‹œí¬ë¦¿ í‚¤
            value: ìƒˆ ê°’
        """
        secrets = self.decrypt_secrets()
        secrets[key] = value
        self.encrypt_secrets(secrets)

        logger.info(f"Updated secret '{key}'")

    def delete_secret(self, key: str) -> None:
        """
        ê°œë³„ ì‹œí¬ë¦¿ ì‚­ì œ

        Args:
            key: ì‚­ì œí•  í‚¤
        """
        secrets = self.decrypt_secrets()
        if key in secrets:
            del secrets[key]
            self.encrypt_secrets(secrets)
            logger.info(f"Deleted secret '{key}'")

    def list_keys(self) -> list:
        """
        ì €ì¥ëœ ì‹œí¬ë¦¿ í‚¤ ëª©ë¡

        Returns:
            í‚¤ ë¦¬ìŠ¤íŠ¸ (ê°’ì€ ë…¸ì¶œí•˜ì§€ ì•ŠìŒ)
        """
        secrets = self.decrypt_secrets()
        return list(secrets.keys())


# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_secrets_manager_instance: Optional[SecretsManager] = None


def get_secrets_manager() -> SecretsManager:
    """
    SecretsManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    Returns:
        SecretsManager ì¸ìŠ¤í„´ìŠ¤
    """
    global _secrets_manager_instance

    if _secrets_manager_instance is None:
        _secrets_manager_instance = SecretsManager()

    return _secrets_manager_instance


# í¸ì˜ í•¨ìˆ˜
def get_secret(key: str, default: Optional[str] = None) -> Optional[str]:
    """
    ì‹œí¬ë¦¿ ì¡°íšŒ í¸ì˜ í•¨ìˆ˜

    Args:
        key: ì‹œí¬ë¦¿ í‚¤
        default: ê¸°ë³¸ê°’

    Returns:
        ì‹œí¬ë¦¿ ê°’

    Example:
        >>> openai_key = get_secret('OPENAI_API_KEY')
    """
    return get_secrets_manager().get_secret(key, default)
```

**íŒŒì¼ 2**: `scripts/migrate_secrets.py` (ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸)

```python
#!/usr/bin/env python3
"""
.env â†’ ì•”í˜¸í™”ëœ secrets ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

Usage:
    python scripts/migrate_secrets.py

Steps:
1. .env íŒŒì¼ ì½ê¸°
2. ëª¨ë“  í™˜ê²½ ë³€ìˆ˜ ì¶”ì¶œ
3. SecretsManagerë¡œ ì•”í˜¸í™” ì €ì¥
4. ë°±ì—… ìƒì„±
5. .env ì‚­ì œ ì•ˆë‚´

Date: 2026-01-03
"""
import os
import shutil
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from backend.config.secrets_manager import SecretsManager


def migrate():
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
    env_file = Path(".env")

    if not env_file.exists():
        print("âŒ .env file not found!")
        return

    # 1. ê¸°ì¡´ .env ë¡œë“œ
    print("ğŸ“‚ Loading .env file...")
    load_dotenv()

    # 2. í™˜ê²½ ë³€ìˆ˜ ì¶”ì¶œ
    secrets = {
        "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY"),
        "GEMINI_API_KEY": os.getenv("GEMINI_API_KEY"),
        "KIS_APP_KEY": os.getenv("KIS_APP_KEY"),
        "KIS_APP_SECRET": os.getenv("KIS_APP_SECRET"),
        "DATABASE_URL": os.getenv("DATABASE_URL"),
        "TELEGRAM_BOT_TOKEN": os.getenv("TELEGRAM_BOT_TOKEN"),
        "TELEGRAM_CHAT_ID": os.getenv("TELEGRAM_CHAT_ID"),
    }

    # None ê°’ ì œê±°
    secrets = {k: v for k, v in secrets.items() if v is not None}

    if not secrets:
        print("âŒ No secrets found in .env file!")
        return

    print(f"âœ… Found {len(secrets)} secrets:")
    for key in secrets.keys():
        print(f"   - {key}")

    # 3. ì•”í˜¸í™” ì €ì¥
    print("\nğŸ” Encrypting secrets...")
    manager = SecretsManager()
    manager.encrypt_secrets(secrets)

    print(f"âœ… Secrets encrypted to .secrets.enc")

    # 4. .env ë°±ì—…
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = f".env.backup_{timestamp}"
    shutil.copy(env_file, backup_file)
    print(f"ğŸ’¾ Backup created: {backup_file}")

    # 5. ì•ˆë‚´ ë©”ì‹œì§€
    print("\n" + "=" * 60)
    print("âš ï¸  IMPORTANT NEXT STEPS:")
    print("=" * 60)
    print("\n1. Verify encrypted secrets:")
    print("   python -c 'from backend.config.secrets_manager import get_secret; print(get_secret(\"OPENAI_API_KEY\")[:20])'")
    print("\n2. Update .gitignore:")
    print("   .secrets.key")
    print("   .secrets.enc")
    print("   .env.backup_*")
    print("\n3. Backup .secrets.key securely (NOT in Git!)")
    print("   - Store in password manager")
    print("   - Save to secure location")
    print("\n4. Update application code to use SecretsManager:")
    print("   from backend.config.secrets_manager import get_secret")
    print("   openai_key = get_secret('OPENAI_API_KEY')")
    print("\n5. Test the application")
    print("\n6. Delete .env file:")
    print("   rm .env")
    print("\n" + "=" * 60)


if __name__ == "__main__":
    migrate()
```

**íŒŒì¼ 3**: `.gitignore` ì—…ë°ì´íŠ¸

```bash
# Secrets (ì•”í˜¸í™”)
.secrets.key
.secrets.enc
.env.backup_*

# ê¸°ì¡´
.env
*.pyc
__pycache__/
```

**ì‚¬ìš© ì˜ˆì‹œ:**

```python
# Before: .env íŒŒì¼ ì§ì ‘ ì ‘ê·¼
import os
openai_key = os.getenv("OPENAI_API_KEY")

# After: SecretsManager ì‚¬ìš©
from backend.config.secrets_manager import get_secret
openai_key = get_secret("OPENAI_API_KEY")
```

**ì˜ˆìƒ íš¨ê³¼:**
- âœ… API í‚¤ ë…¸ì¶œ ìœ„í—˜: 100% â†’ 0%
- âœ… Git ì»¤ë°‹ ì•ˆì „: í‰ë¬¸ í‚¤ ì°¨ë‹¨
- âœ… ìš´ì˜ í™˜ê²½ ë³´ì•ˆ: ì•”í˜¸í™” íŒŒì¼ë§Œ ë°°í¬

---

#### Implementation 1-2: OWASP Top 10 ìë™ ìŠ¤ìº”

**íŒŒì¼**: `scripts/security_audit.py` (ì‹ ê·œ ìƒì„±)

```python
#!/usr/bin/env python3
"""
OWASP Top 10 ìë™ ë³´ì•ˆ ìŠ¤ìº”

Checks:
1. SQL Injection (A03:2021)
2. XSS - Cross-Site Scripting (A03:2021)
3. Broken Authentication (A07:2021)
4. Sensitive Data Exposure (A02:2021)
5. Broken Access Control (A01:2021)
6. Security Misconfiguration (A05:2021)
7. Insecure Deserialization (A08:2021)
8. Using Components with Known Vulnerabilities (A06:2021)
9. Insufficient Logging & Monitoring (A09:2021)
10. Server-Side Request Forgery (A10:2021)

Date: 2026-01-03
Author: AI Trading System Team
"""
import re
from pathlib import Path
from typing import List, Dict, Set
import json
from datetime import datetime


class SecurityAuditor:
    """ë³´ì•ˆ ê°ì‚¬ ë„êµ¬"""

    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.issues: List[Dict] = []
        self.scanned_files: Set[Path] = set()

    def scan_sql_injection(self) -> List[Dict]:
        """
        A03:2021 - SQL Injection ì·¨ì•½ì  ìŠ¤ìº”

        Patterns:
        - f-string in SQL query
        - % formatting in SQL
        - Direct string concatenation
        """
        issues = []

        # Python íŒŒì¼ ìŠ¤ìº”
        py_files = list(self.base_path.glob("backend/**/*.py"))

        for file in py_files:
            if self._should_skip(file):
                continue

            self.scanned_files.add(file)
            content = file.read_text(encoding='utf-8', errors='ignore')

            # ìœ„í—˜ íŒ¨í„´ 1: f-string in SELECT
            if re.search(r'f".*?SELECT.*?\{.*?\}"', content, re.IGNORECASE):
                issues.append(self._create_issue(
                    type="SQL_INJECTION",
                    severity="CRITICAL",
                    file=file,
                    message="Potential SQL injection via f-string in SELECT statement",
                    line=self._find_line_number(content, r'f".*?SELECT.*?\{.*?\}"')
                ))

            # ìœ„í—˜ íŒ¨í„´ 2: % formatting
            if re.search(r'%.*?SELECT|INSERT|UPDATE|DELETE', content, re.IGNORECASE):
                issues.append(self._create_issue(
                    type="SQL_INJECTION",
                    severity="HIGH",
                    file=file,
                    message="Potential SQL injection via % formatting",
                    line=self._find_line_number(content, r'%.*?SELECT')
                ))

            # ìœ„í—˜ íŒ¨í„´ 3: String concatenation
            if re.search(r'\+.*?(SELECT|INSERT|UPDATE|DELETE)', content, re.IGNORECASE):
                issues.append(self._create_issue(
                    type="SQL_INJECTION",
                    severity="MEDIUM",
                    file=file,
                    message="Potential SQL injection via string concatenation",
                    line=self._find_line_number(content, r'\+.*?SELECT')
                ))

        return issues

    def scan_xss(self) -> List[Dict]:
        """
        A03:2021 - XSS ì·¨ì•½ì  ìŠ¤ìº”

        Checks:
        - dangerouslySetInnerHTML in React
        - Unescaped user input in templates
        """
        issues = []

        # TypeScript/React íŒŒì¼ ìŠ¤ìº”
        tsx_files = list(self.base_path.glob("frontend/src/**/*.tsx"))

        for file in tsx_files:
            self.scanned_files.add(file)
            content = file.read_text(encoding='utf-8', errors='ignore')

            # dangerouslySetInnerHTML ì‚¬ìš©
            if "dangerouslySetInnerHTML" in content:
                issues.append(self._create_issue(
                    type="XSS",
                    severity="HIGH",
                    file=file,
                    message="dangerouslySetInnerHTML detected - verify sanitization with DOMPurify",
                    line=self._find_line_number(content, "dangerouslySetInnerHTML")
                ))

            # Unescaped innerHTML
            if re.search(r'\.innerHTML\s*=', content):
                issues.append(self._create_issue(
                    type="XSS",
                    severity="MEDIUM",
                    file=file,
                    message="Direct innerHTML assignment - XSS risk",
                    line=self._find_line_number(content, r'\.innerHTML\s*=')
                ))

        return issues

    def scan_secrets_exposure(self) -> List[Dict]:
        """
        A02:2021 - Sensitive Data Exposure

        Patterns:
        - API keys in code
        - Passwords in code
        - Database credentials
        """
        issues = []

        # ëª¨ë“  ì½”ë“œ íŒŒì¼ ìŠ¤ìº”
        code_files = list(self.base_path.glob("**/*.py"))
        code_files.extend(self.base_path.glob("**/*.tsx"))
        code_files.extend(self.base_path.glob("**/*.ts"))

        secret_patterns = [
            (r'sk-[a-zA-Z0-9]{48}', 'OpenAI API Key'),
            (r'AIzaSy[a-zA-Z0-9_-]{33}', 'Google/Gemini API Key'),
            (r'ghp_[a-zA-Z0-9]{36}', 'GitHub Personal Access Token'),
            (r'[0-9]+-[0-9A-Za-z_]{32}\.apps\.googleusercontent\.com', 'Google OAuth'),
            (r'postgresql://[^:]+:[^@]+@', 'PostgreSQL Password in URL'),
            (r'mysql://[^:]+:[^@]+@', 'MySQL Password in URL'),
            (r'mongodb://[^:]+:[^@]+@', 'MongoDB Password in URL'),
        ]

        for file in code_files:
            if self._should_skip(file):
                continue

            self.scanned_files.add(file)
            content = file.read_text(encoding='utf-8', errors='ignore')

            for pattern, secret_type in secret_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    issues.append(self._create_issue(
                        type="SECRET_EXPOSURE",
                        severity="CRITICAL",
                        file=file,
                        message=f"Potential {secret_type} hardcoded in source",
                        line=self._find_line_number(content, pattern)
                    ))

        return issues

    def scan_broken_access_control(self) -> List[Dict]:
        """
        A01:2021 - Broken Access Control

        Checks:
        - DELETE endpoints without authentication
        - Admin routes without authorization
        - Direct object references
        """
        issues = []

        # API ë¼ìš°í„° ìŠ¤ìº”
        router_files = list(self.base_path.glob("backend/api/*_router.py"))

        for file in router_files:
            self.scanned_files.add(file)
            content = file.read_text(encoding='utf-8', errors='ignore')

            # DELETE without auth
            delete_routes = re.findall(
                r'@router\.delete\([^)]+\)\s*async def ([a-z_]+)',
                content,
                re.IGNORECASE
            )

            for route in delete_routes:
                # Check if Depends(get_current_user) exists
                if 'Depends(get_current_user)' not in content:
                    issues.append(self._create_issue(
                        type="BROKEN_ACCESS_CONTROL",
                        severity="CRITICAL",
                        file=file,
                        message=f"DELETE endpoint '{route}' without authentication",
                        line=self._find_line_number(content, f'def {route}')
                    ))

            # Admin routes
            if '/admin' in content or 'admin' in str(file):
                if 'is_admin' not in content and 'require_admin' not in content:
                    issues.append(self._create_issue(
                        type="BROKEN_ACCESS_CONTROL",
                        severity="HIGH",
                        file=file,
                        message="Admin route without authorization check",
                        line=1
                    ))

        return issues

    def scan_security_misconfiguration(self) -> List[Dict]:
        """
        A05:2021 - Security Misconfiguration

        Checks:
        - DEBUG mode in production
        - Default credentials
        - Unnecessary services
        """
        issues = []

        # Check main.py for debug mode
        main_file = self.base_path / "backend" / "main.py"
        if main_file.exists():
            content = main_file.read_text(encoding='utf-8')

            if re.search(r'debug\s*=\s*True', content, re.IGNORECASE):
                issues.append(self._create_issue(
                    type="SECURITY_MISCONFIGURATION",
                    severity="HIGH",
                    file=main_file,
                    message="DEBUG mode enabled - should be False in production",
                    line=self._find_line_number(content, r'debug\s*=\s*True')
                ))

        # Check docker-compose for default passwords
        docker_compose = self.base_path / "docker-compose.yml"
        if docker_compose.exists():
            content = docker_compose.read_text(encoding='utf-8')

            if 'POSTGRES_PASSWORD: trading123' in content:
                issues.append(self._create_issue(
                    type="SECURITY_MISCONFIGURATION",
                    severity="MEDIUM",
                    file=docker_compose,
                    message="Default database password detected",
                    line=self._find_line_number(content, 'POSTGRES_PASSWORD')
                ))

        return issues

    def scan_insufficient_logging(self) -> List[Dict]:
        """
        A09:2021 - Insufficient Logging & Monitoring

        Checks:
        - Exception handling without logging
        - Security events without audit trail
        """
        issues = []

        py_files = list(self.base_path.glob("backend/**/*.py"))

        for file in py_files:
            if self._should_skip(file):
                continue

            content = file.read_text(encoding='utf-8', errors='ignore')

            # Bare except without logging
            bare_excepts = re.findall(
                r'except:[\s\S]{0,200}?pass',
                content
            )

            if bare_excepts and 'logger' not in content:
                issues.append(self._create_issue(
                    type="INSUFFICIENT_LOGGING",
                    severity="MEDIUM",
                    file=file,
                    message="Exception handling without logging",
                    line=self._find_line_number(content, r'except:')
                ))

        return issues

    def run_full_audit(self) -> Dict:
        """ì „ì²´ ë³´ì•ˆ ê°ì‚¬ ì‹¤í–‰"""
        print("ğŸ” Starting Security Audit...")
        print("=" * 60)

        all_issues = []

        # Run all scans
        print("1ï¸âƒ£  Scanning for SQL Injection...")
        all_issues.extend(self.scan_sql_injection())

        print("2ï¸âƒ£  Scanning for XSS vulnerabilities...")
        all_issues.extend(self.scan_xss())

        print("3ï¸âƒ£  Scanning for exposed secrets...")
        all_issues.extend(self.scan_secrets_exposure())

        print("4ï¸âƒ£  Scanning for broken access control...")
        all_issues.extend(self.scan_broken_access_control())

        print("5ï¸âƒ£  Scanning for security misconfiguration...")
        all_issues.extend(self.scan_security_misconfiguration())

        print("6ï¸âƒ£  Scanning for insufficient logging...")
        all_issues.extend(self.scan_insufficient_logging())

        # Categorize by severity
        critical = [i for i in all_issues if i['severity'] == 'CRITICAL']
        high = [i for i in all_issues if i['severity'] == 'HIGH']
        medium = [i for i in all_issues if i['severity'] == 'MEDIUM']
        low = [i for i in all_issues if i['severity'] == 'LOW']

        result = {
            "scan_date": datetime.now().isoformat(),
            "scanned_files": len(self.scanned_files),
            "total_issues": len(all_issues),
            "critical": len(critical),
            "high": len(high),
            "medium": len(medium),
            "low": len(low),
            "issues": all_issues
        }

        return result

    def _should_skip(self, file: Path) -> bool:
        """ìŠ¤ìº” ì œì™¸ íŒŒì¼ íŒë‹¨"""
        skip_patterns = [
            '.venv',
            'node_modules',
            '__pycache__',
            '.git',
            'migrations',
            'test_',
            '.pyc'
        ]

        return any(pattern in str(file) for pattern in skip_patterns)

    def _create_issue(
        self,
        type: str,
        severity: str,
        file: Path,
        message: str,
        line: int = 1
    ) -> Dict:
        """ì´ìŠˆ ê°ì²´ ìƒì„±"""
        return {
            "type": type,
            "severity": severity,
            "file": str(file.relative_to(self.base_path)),
            "line": line,
            "message": message
        }

    def _find_line_number(self, content: str, pattern: str) -> int:
        """íŒ¨í„´ì´ ìœ„ì¹˜í•œ ë¼ì¸ ë²ˆí˜¸ ì°¾ê¸°"""
        try:
            lines = content.split('\n')
            for i, line in enumerate(lines, 1):
                if re.search(pattern, line, re.IGNORECASE):
                    return i
        except:
            pass
        return 1

    def generate_report(self, result: Dict, output_file: str = "security_audit_report.json"):
        """ê°ì‚¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        with open(output_file, 'w') as f:
            json.dump(result, f, indent=2)

        print("\n" + "=" * 60)
        print("ğŸ“Š Security Audit Report")
        print("=" * 60)
        print(f"Scanned Files: {result['scanned_files']}")
        print(f"Total Issues: {result['total_issues']}")
        print(f"  ğŸ”´ Critical: {result['critical']}")
        print(f"  ğŸŸ  High: {result['high']}")
        print(f"  ğŸŸ¡ Medium: {result['medium']}")
        print(f"  âšª Low: {result['low']}")
        print("=" * 60)

        if result['critical'] > 0:
            print("\nâŒ CRITICAL ISSUES:")
            for issue in result['issues']:
                if issue['severity'] == 'CRITICAL':
                    print(f"  {issue['file']}:{issue['line']}")
                    print(f"    {issue['message']}")

        print(f"\nğŸ’¾ Full report saved to: {output_file}")


# CLI ì‹¤í–‰
if __name__ == "__main__":
    auditor = SecurityAuditor()
    results = auditor.run_full_audit()
    auditor.generate_report(results)

    # Exit with error code if critical issues found
    if results['critical'] > 0:
        exit(1)
```

**ì‹¤í–‰:**
```bash
# ì „ì²´ ìŠ¤ìº”
python scripts/security_audit.py

# ì¶œë ¥:
# ğŸ” Starting Security Audit...
# ============================================================
# 1ï¸âƒ£  Scanning for SQL Injection...
# 2ï¸âƒ£  Scanning for XSS vulnerabilities...
# 3ï¸âƒ£  Scanning for exposed secrets...
# 4ï¸âƒ£  Scanning for broken access control...
# 5ï¸âƒ£  Scanning for security misconfiguration...
# 6ï¸âƒ£  Scanning for insufficient logging...
#
# ============================================================
# ğŸ“Š Security Audit Report
# ============================================================
# Scanned Files: 127
# Total Issues: 5
#   ğŸ”´ Critical: 1
#   ğŸŸ  High: 2
#   ğŸŸ¡ Medium: 2
#   âšª Low: 0
# ============================================================
#
# âŒ CRITICAL ISSUES:
#   backend/api/war_room_router.py:42
#     DELETE endpoint 'delete_session' without authentication
#
# ğŸ’¾ Full report saved to: security_audit_report.json
```

**ì˜ˆìƒ ì†Œìš”:** 4ì‹œê°„
**ì˜ˆìƒ íš¨ê³¼:** OWASP Top 10 ì·¨ì•½ì  ìë™ íƒì§€

---

#### Implementation 1-3: `/check-security` Command í†µí•©

**ì„¤ì¹˜:**
```bash
npx claude-code-templates@latest --command check-security --yes
```

**ì‚¬ìš©:**
```bash
# ì „ì²´ ì½”ë“œë² ì´ìŠ¤ ìŠ¤ìº”
/check-security

# íŠ¹ì • íŒŒì¼ë§Œ ìŠ¤ìº”
/check-security backend/api/war_room_router.py

# ë¦¬í¬íŠ¸ ìƒì„±
/check-security --report
```

**GitHub Actions í†µí•©:**

**íŒŒì¼**: `.github/workflows/security-scan.yml` (ì‹ ê·œ)

```yaml
name: Security Scan

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 2 * * 0'  # ë§¤ì£¼ ì¼ìš”ì¼ ì˜¤ì „ 2ì‹œ

jobs:
  security-audit:
    name: OWASP Security Audit
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install cryptography

      - name: Run security audit
        run: |
          python scripts/security_audit.py

      - name: Upload report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: security-audit-report
          path: security_audit_report.json

      - name: Fail on critical issues
        run: |
          CRITICAL=$(jq '.critical' security_audit_report.json)
          if [ "$CRITICAL" -gt 0 ]; then
            echo "âŒ Found $CRITICAL critical security issues!"
            exit 1
          fi
```

**ì˜ˆìƒ íš¨ê³¼:** PRë§ˆë‹¤ ìë™ ë³´ì•ˆ ê²€ì‚¬, Critical ì´ìŠˆ ì‹œ ë¨¸ì§€ ì°¨ë‹¨

---

### 1.2 êµ¬í˜„ ë¡œë“œë§µ (Security)

**Week 1: Secrets ì•”í˜¸í™”**
- [ ] Day 1-2: SecretsManager í´ë˜ìŠ¤ êµ¬í˜„
- [ ] Day 3: ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] Day 4: .env â†’ .secrets.enc ì „í™˜
- [ ] Day 5: ë°±ì—”ë“œ ì½”ë“œ ì—…ë°ì´íŠ¸ (get_secret ì‚¬ìš©)
- [ ] Day 6-7: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

**Week 2: ë³´ì•ˆ ê°ì‚¬ ìë™í™”**
- [ ] Day 1-3: SecurityAuditor êµ¬í˜„ (6ê°œ ìŠ¤ìº”)
- [ ] Day 4: GitHub Actions ì›Œí¬í”Œë¡œìš° ì‘ì„±
- [ ] Day 5: `/check-security` ëª…ë ¹ ì„¤ì¹˜ ë° í…ŒìŠ¤íŠ¸
- [ ] Day 6-7: ë°œê²¬ëœ ì·¨ì•½ì  ìˆ˜ì •

**Week 3: ì§€ì†ì  ëª¨ë‹ˆí„°ë§**
- [ ] Day 1-2: ì£¼ê°„ ë³´ì•ˆ ìŠ¤ìº” ìŠ¤ì¼€ì¤„ ì„¤ì •
- [ ] Day 3-4: Telegram ì•Œë¦¼ í†µí•© (Critical ì´ìŠˆ)
- [ ] Day 5-7: ë³´ì•ˆ ëŒ€ì‹œë³´ë“œ êµ¬ì¶• (ì„ íƒ)

**ì˜ˆìƒ íš¨ê³¼:**
- âœ… API í‚¤ ë…¸ì¶œ ìœ„í—˜: 100% â†’ 0%
- âœ… OWASP Top 10 ì¤€ìˆ˜: 0% â†’ 90%
- âœ… ë³´ì•ˆ ì·¨ì•½ì  ë°œê²¬: ìˆ˜ë™ â†’ ìë™ (ì£¼ê°„)
- âœ… Critical ì´ìŠˆ ì°¨ë‹¨: PR ë¨¸ì§€ ë°©ì§€

---

## Part 2: DevOps & CI/CD (ë°°í¬ ìë™í™”)

### ëª©í‘œ
- CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- ë°°í¬ ì‹œê°„ 60ë¶„ â†’ 5ë¶„
- í…ŒìŠ¤íŠ¸ ìë™ ì‹¤í–‰ (PRë§ˆë‹¤)
- Blue-Green ë°°í¬ ë° ìë™ ë¡¤ë°±

---

### 2.1 DevOps Engineer Agent

**ì„¤ì¹˜ ë°©ë²•:**
```bash
npx claude-code-templates@latest --agent devops-engineer --yes
```

**ì£¼ìš” ê¸°ëŠ¥:**
1. GitHub Actions CI/CD ì„¤ì •
2. Docker ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œ
3. ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
4. Staging/Production ë¶„ë¦¬ ë°°í¬
5. Blue-Green ë°°í¬ ì „ëµ
6. ìë™ ë¡¤ë°±

---

#### Implementation 2-1: GitHub Actions CI/CD íŒŒì´í”„ë¼ì¸

**í˜„ì¬ ìƒíƒœ:**
```yaml
# .github/workflows/ci.yml - ê¸°ë³¸ë§Œ ì¡´ì¬
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì•ˆ í•¨ âŒ
```

**ëª©í‘œ íŒŒì´í”„ë¼ì¸:**
```
ì½”ë“œ í‘¸ì‹œ â†’ Lint â†’ Test â†’ Build â†’ Security Scan â†’ Deploy
```

**íŒŒì¼**: `.github/workflows/ci-cd-pipeline.yml` (ì‹ ê·œ ìƒì„±)

```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # ============================================================
  # Stage 1: Lint & Format Check
  # ============================================================
  lint:
    name: ğŸ” Code Linting
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          cd backend
          pip install flake8 black mypy
          pip install -r requirements.txt

      - name: Lint with flake8
        run: |
          cd backend
          # Stop build if syntax errors or undefined names
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          # Exit-zero treats all errors as warnings
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: Check formatting with black
        run: |
          cd backend
          black --check --diff .

      - name: Type check with mypy
        continue-on-error: true
        run: |
          cd backend
          mypy --ignore-missing-imports --no-strict-optional .

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install frontend dependencies
        run: |
          cd frontend
          npm ci

      - name: Lint frontend
        run: |
          cd frontend
          npm run lint

  # ============================================================
  # Stage 2: Backend Tests
  # ============================================================
  test-backend:
    name: ğŸ§ª Backend Tests
    runs-on: ubuntu-latest
    needs: lint

    services:
      postgres:
        image: timescale/timescaledb:latest-pg15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: trading_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio pytest-mock

      - name: Run tests with coverage
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/trading_test
          TESTING: true
        run: |
          cd backend
          pytest --cov=. --cov-report=xml --cov-report=term --cov-report=html -v

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./backend/coverage.xml
          flags: backend
          name: backend-coverage

      - name: Upload coverage HTML
        uses: actions/upload-artifact@v3
        with:
          name: backend-coverage-html
          path: backend/htmlcov/

  # ============================================================
  # Stage 3: Frontend Tests
  # ============================================================
  test-frontend:
    name: ğŸ¨ Frontend Tests
    runs-on: ubuntu-latest
    needs: lint

    steps:
      - uses: actions/checkout@v3

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: |
          cd frontend
          npm ci

      - name: Run tests
        run: |
          cd frontend
          npm test -- --coverage --watchAll=false

      - name: Build
        run: |
          cd frontend
          npm run build

      - name: Upload build artifacts
        uses: actions/upload-artifact@v3
        with:
          name: frontend-build
          path: frontend/dist/

  # ============================================================
  # Stage 4: Security Scan
  # ============================================================
  security-scan:
    name: ğŸ”’ Security Scan
    runs-on: ubuntu-latest
    needs: [test-backend, test-frontend]

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install cryptography

      - name: Run OWASP security audit
        run: |
          python scripts/security_audit.py

      - name: Check for critical issues
        run: |
          CRITICAL=$(jq '.critical' security_audit_report.json)
          if [ "$CRITICAL" -gt 0 ]; then
            echo "âŒ Found $CRITICAL critical security issues!"
            exit 1
          fi

      - name: Upload security report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: security-audit-report
          path: security_audit_report.json

  # ============================================================
  # Stage 5: Build Docker Images
  # ============================================================
  build-images:
    name: ğŸ³ Build Docker Images
    runs-on: ubuntu-latest
    needs: [security-scan]
    if: github.event_name == 'push'

    steps:
      - uses: checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Log in to Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels)
        id: meta-backend
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/backend

      - name: Build and push backend image
        uses: docker/build-push-action@v4
        with:
          context: ./backend
          push: true
          tags: ${{ steps.meta-backend.outputs.tags }}
          labels: ${{ steps.meta-backend.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Extract metadata for frontend
        id: meta-frontend
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend

      - name: Build and push frontend image
        uses: docker/build-push-action@v4
        with:
          context: ./frontend
          push: true
          tags: ${{ steps.meta-frontend.outputs.tags }}
          labels: ${{ steps.meta-frontend.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # ============================================================
  # Stage 6: Deploy to Staging
  # ============================================================
  deploy-staging:
    name: ğŸš€ Deploy to Staging
    runs-on: ubuntu-latest
    needs: [build-images]
    if: github.ref == 'refs/heads/develop'
    environment:
      name: staging
      url: https://staging.trading.example.com

    steps:
      - uses: actions/checkout@v3

      - name: Deploy to staging server
        run: |
          echo "ğŸš€ Deploying to Staging..."
          # SSH to staging server and pull new images
          # ssh staging "cd /app && docker-compose pull && docker-compose up -d"

      - name: Health check
        run: |
          sleep 10
          curl -f https://staging.trading.example.com/health || exit 1

      - name: Notify Telegram
        if: success()
        run: |
          curl -X POST https://api.telegram.org/bot${{ secrets.TELEGRAM_BOT_TOKEN }}/sendMessage \
            -d chat_id=${{ secrets.TELEGRAM_CHAT_ID }} \
            -d text="âœ… Staging deployment successful - ${{ github.sha }}"

  # ============================================================
  # Stage 7: Deploy to Production
  # ============================================================
  deploy-production:
    name: ğŸ¯ Deploy to Production
    runs-on: ubuntu-latest
    needs: [build-images]
    if: github.ref == 'refs/heads/main'
    environment:
      name: production
      url: https://trading.example.com

    steps:
      - uses: actions/checkout@v3

      - name: Deploy to production (Blue-Green)
        run: |
          echo "ğŸ¯ Deploying to Production (Blue-Green)..."
          # 1. Start new containers (Green)
          # 2. Health check
          # 3. Switch traffic
          # 4. Stop old containers (Blue)

      - name: Health check
        run: |
          sleep 15
          curl -f https://trading.example.com/health || exit 1

      - name: Rollback on failure
        if: failure()
        run: |
          echo "âŒ Deployment failed, rolling back..."
          # Switch back to Blue

      - name: Notify Telegram
        if: always()
        run: |
          STATUS="${{ job.status }}"
          if [ "$STATUS" = "success" ]; then
            MSG="âœ… Production deployment successful"
          else
            MSG="âŒ Production deployment FAILED - rolled back"
          fi
          curl -X POST https://api.telegram.org/bot${{ secrets.TELEGRAM_BOT_TOKEN }}/sendMessage \
            -d chat_id=${{ secrets.TELEGRAM_CHAT_ID }} \
            -d text="$MSG - ${{ github.sha }}"
```

**ì˜ˆìƒ íš¨ê³¼:**
- âœ… ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (PRë§ˆë‹¤)
- âœ… ë³´ì•ˆ ìŠ¤ìº” ìë™í™”
- âœ… Staging ìë™ ë°°í¬ (develop ë¸Œëœì¹˜)
- âœ… Production ë°°í¬ (main ë¸Œëœì¹˜)
- âœ… ë°°í¬ ì‹¤íŒ¨ ì‹œ ìë™ ë¡¤ë°±

---

#### Implementation 2-2: Docker ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œ

**íŒŒì¼**: `backend/Dockerfile` (ìµœì í™”)

```dockerfile
# ============================================================
# Stage 1: Builder
# ============================================================
FROM python:3.11-slim as builder

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    postgresql-client \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ë¹Œë“œ (ë ˆì´ì–´ ìºì‹±)
COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

# ============================================================
# Stage 2: Runtime
# ============================================================
FROM python:3.11-slim

WORKDIR /app

# ëŸ°íƒ€ì„ ì˜ì¡´ì„±ë§Œ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    libpq5 \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Builder stageì—ì„œ Python íŒ¨í‚¤ì§€ ë³µì‚¬
COPY --from=builder /root/.local /root/.local

# PATH ì—…ë°ì´íŠ¸
ENV PATH=/root/.local/bin:$PATH

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . .

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# ë¹„ë£¨íŠ¸ ì‚¬ìš©ì ìƒì„±
RUN useradd -m -u 1000 appuser && \
    chown -R appuser:appuser /app

USER appuser

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

**ì˜ˆìƒ íš¨ê³¼:**
- ì´ë¯¸ì§€ í¬ê¸°: 1.2GB â†’ 400MB
- ë¹Œë“œ ì‹œê°„: 5ë¶„ â†’ 2ë¶„ (ìºì‹œ í™œìš©)
- ë³´ì•ˆ: ë¹„ë£¨íŠ¸ ì‚¬ìš©ì ì‹¤í–‰

---

**íŒŒì¼**: `frontend/Dockerfile` (ìµœì í™”)

```dockerfile
# ============================================================
# Stage 1: Build
# ============================================================
FROM node:18-alpine as build

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜ (ë ˆì´ì–´ ìºì‹±)
COPY package*.json ./
RUN npm ci --only=production

# ì†ŒìŠ¤ ë³µì‚¬ ë° ë¹Œë“œ
COPY . .
RUN npm run build

# ============================================================
# Stage 2: Production with Nginx
# ============================================================
FROM nginx:alpine

# Nginx ì„¤ì •
COPY nginx.conf /etc/nginx/nginx.conf

# ë¹Œë“œ ê²°ê³¼ë¬¼ë§Œ ë³µì‚¬
COPY --from=build /app/dist /usr/share/nginx/html

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD wget --quiet --tries=1 --spider http://localhost:80/health || exit 1

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
```

**íŒŒì¼**: `frontend/nginx.conf` (ì‹ ê·œ)

```nginx
worker_processes auto;

events {
    worker_connections 1024;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    sendfile on;
    keepalive_timeout 65;
    gzip on;
    gzip_types text/plain text/css application/json application/javascript;

    server {
        listen 80;
        server_name _;

        root /usr/share/nginx/html;
        index index.html;

        # SPA routing
        location / {
            try_files $uri $uri/ /index.html;
        }

        # API proxy
        location /api/ {
            proxy_pass http://backend:8000;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }

        # Health check
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }
}
```

**ì˜ˆìƒ íš¨ê³¼:**
- ì´ë¯¸ì§€ í¬ê¸°: 800MB â†’ 50MB
- Production-ready Nginx
- SPA ë¼ìš°íŒ… ì§€ì›

---

### 2.2 êµ¬í˜„ ë¡œë“œë§µ (DevOps)

**Week 1: CI íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**
- [ ] Day 1-2: GitHub Actions ì›Œí¬í”Œë¡œìš° ì‘ì„±
- [ ] Day 3: Lint/Test ë‹¨ê³„ êµ¬ì„± ë° í…ŒìŠ¤íŠ¸
- [ ] Day 4-5: ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸ Codecov í†µí•©
- [ ] Day 6-7: Security scan ë‹¨ê³„ ì¶”ê°€

**Week 2: CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**
- [ ] Day 1-2: Docker ì´ë¯¸ì§€ ë¹Œë“œ ìë™í™”
- [ ] Day 3-4: Staging ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] Day 5: Production Blue-Green ë°°í¬ êµ¬í˜„
- [ ] Day 6-7: ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸

**Week 3: Docker ìµœì í™”**
- [ ] Day 1-2: Backend Dockerfile ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œ
- [ ] Day 3-4: Frontend Dockerfile + Nginx ì„¤ì •
- [ ] Day 5: docker-compose.yml í—¬ìŠ¤ì²´í¬ ì¶”ê°€
- [ ] Day 6-7: ì´ë¯¸ì§€ í¬ê¸° ë° ë¹Œë“œ ì‹œê°„ ì¸¡ì •

**Week 4: ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼**
- [ ] Day 1-2: Telegram ë°°í¬ ì•Œë¦¼ í†µí•©
- [ ] Day 3-4: ì—ëŸ¬ ì¶”ì  ì‹œìŠ¤í…œ (Sentry ì—°ë™)
- [ ] Day 5-7: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ (ì„ íƒ)

**ì˜ˆìƒ íš¨ê³¼:**
- âœ… ë°°í¬ ì‹œê°„: 60ë¶„ â†’ 5ë¶„ (92% ê°œì„ )
- âœ… í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€: ìë™ ì¸¡ì • ë° ë¦¬í¬íŠ¸
- âœ… ë¡¤ë°± ì‹œê°„: ìˆ˜ë™ 30ë¶„ â†’ ìë™ 2ë¶„
- âœ… ë°°í¬ ì‹ ë¢°ë„: ìˆ˜ë™ â†’ ìë™ (Zero-downtime)

---

## Part 3: Performance & Monitoring (ì„±ëŠ¥ ë° ëª¨ë‹ˆí„°ë§)

### ëª©í‘œ
- War Room MVP ì‘ë‹µ ì‹œê°„ ë‹¨ì¶• (12.76ì´ˆ â†’ 7.5ì´ˆ)
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼
- ë³‘ëª© ì§€ì  ìë™ íƒì§€
- ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™” (450MB â†’ 200MB)

---

### 3.1 `/performance-audit` Command

**ì„¤ì¹˜ ë°©ë²•:**
```bash
npx claude-code-templates@latest --command performance-audit --yes
```

**ì£¼ìš” ê¸°ëŠ¥:**
1. í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ í”„ë¡œíŒŒì¼ë§
2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
3. ë³‘ëª© ì§€ì  ìë™ íƒì§€
4. ìµœì í™” ê¶Œì¥ì‚¬í•­ ì œê³µ

---

#### Implementation 3-1: War Room MVP ë³‘ë ¬í™”

**í˜„ì¬ ë¬¸ì œ:**
```python
# backend/ai/mvp/war_room_mvp.py
# ìˆœì°¨ ì‹¤í–‰ - 8.2ì´ˆ ì†Œìš”
trader_result = self.trader_agent.analyze(...)  # 2.8ì´ˆ
risk_result = self.risk_agent.analyze(...)      # 2.7ì´ˆ
analyst_result = self.analyst_agent.analyze(...)  # 2.7ì´ˆ
# ì´ 8.2ì´ˆ + PM Agent 1.5ì´ˆ = 9.7ì´ˆ
```

**í•´ê²°ì±…: ThreadPoolExecutor ë³‘ë ¬í™”**

**íŒŒì¼**: `backend/ai/mvp/war_room_mvp.py` (ìˆ˜ì •)

```python
"""
War Room MVP - ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”

Date: 2026-01-03
Optimization: Parallel agent execution
"""
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Any
import time
import logging

logger = logging.getLogger(__name__)


class WarRoomMVP:
    """War Room MVP - 3 Agent + PM êµ¬ì¡°"""

    def __init__(self):
        self.trader_agent = TraderAgentMVP()
        self.risk_agent = RiskAgentMVP()
        self.analyst_agent = AnalystAgentMVP()
        self.pm_agent = PMAgentMVP()

        # ThreadPool ìƒì„± (ì¬ì‚¬ìš©)
        self.executor = ThreadPoolExecutor(max_workers=3)

    def deliberate(
        self,
        symbol: str,
        action_context: str = "new_position",
        market_data: Dict = None,
        portfolio_state: Dict = None,
        additional_data: Dict = None
    ) -> Dict[str, Any]:
        """
        ì „ìŸì‹¤ ì‹¬ì˜ - ë³‘ë ¬ ì²˜ë¦¬ ë²„ì „

        Args:
            symbol: ì¢…ëª© ì‹¬ë³¼
            action_context: ì•¡ì…˜ ì»¨í…ìŠ¤íŠ¸
            market_data: ì‹œì¥ ë°ì´í„°
            portfolio_state: í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ
            additional_data: ì¶”ê°€ ë°ì´í„°

        Returns:
            ìµœì¢… ì˜ì‚¬ê²°ì • ê²°ê³¼
        """
        start_time = time.time()

        # 1. 3ê°œ Agent ë³‘ë ¬ ì‹¤í–‰
        logger.info(f"Starting parallel agent execution for {symbol}")

        futures = {
            'trader': self.executor.submit(
                self.trader_agent.analyze,
                symbol=symbol,
                price_data=market_data.get('price_data'),
                technical_data=market_data.get('technical_data')
            ),
            'risk': self.executor.submit(
                self.risk_agent.analyze,
                symbol=symbol,
                price_data=market_data.get('price_data'),
                portfolio_state=portfolio_state
            ),
            'analyst': self.executor.submit(
                self.analyst_agent.analyze,
                symbol=symbol,
                news_data=market_data.get('news_data'),
                macro_data=market_data.get('macro_data')
            )
        }

        # 2. ê²°ê³¼ ìˆ˜ì§‘ (as_completedë¡œ ë¨¼ì € ëë‚˜ëŠ” ìˆœì„œëŒ€ë¡œ)
        agent_opinions = {}
        for agent_name, future in futures.items():
            try:
                result = future.result(timeout=10)  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
                agent_opinions[agent_name] = result
                logger.debug(f"{agent_name} completed: {result.get('action')}")
            except Exception as e:
                logger.error(f"{agent_name} failed: {e}")
                agent_opinions[agent_name] = {
                    'action': 'PASS',
                    'confidence': 0.0,
                    'reasoning': f'Agent error: {str(e)}'
                }

        agent_time = time.time() - start_time
        logger.info(f"Agents completed in {agent_time:.2f}s (parallel)")

        # 3. PM Agent ìµœì¢… ê²°ì •
        pm_start = time.time()

        final_decision = self.pm_agent.make_final_decision(
            symbol=symbol,
            trader_opinion=agent_opinions.get('trader'),
            risk_opinion=agent_opinions.get('risk'),
            analyst_opinion=agent_opinions.get('analyst'),
            portfolio_state=portfolio_state
        )

        pm_time = time.time() - pm_start
        total_time = time.time() - start_time

        logger.info(
            f"War Room completed: {total_time:.2f}s "
            f"(agents: {agent_time:.2f}s, PM: {pm_time:.2f}s)"
        )

        return {
            'symbol': symbol,
            'final_decision': final_decision,
            'agent_opinions': agent_opinions,
            'pm_decision': final_decision,
            'performance': {
                'total_time': total_time,
                'agent_time': agent_time,
                'pm_time': pm_time,
                'speedup': '3x' if agent_time < 4.0 else '1x'
            }
        }

    def __del__(self):
        """Clean up thread pool"""
        self.executor.shutdown(wait=False)
```

**ì˜ˆìƒ íš¨ê³¼:**
- Agent ì‹¤í–‰ ì‹œê°„: 8.2ì´ˆ â†’ 2.8ì´ˆ (ë³‘ë ¬í™”)
- ì „ì²´ ì‘ë‹µ ì‹œê°„: 12.76ì´ˆ â†’ 7.5ì´ˆ (41% ê°œì„ )
- ë¦¬ì†ŒìŠ¤ ì‚¬ìš©: CPU í™œìš©ë„ 3ë°° ì¦ê°€

---

#### Implementation 3-2: ë‰´ìŠ¤ ë°±í•„ ë©”ëª¨ë¦¬ ìµœì í™”

**í˜„ì¬ ë¬¸ì œ:**
```python
# backend/data/processors/news_processor.py
def process_articles(self, articles: List[Article]):
    # 20ê°œ ê¸°ì‚¬ ì „ë¶€ ë©”ëª¨ë¦¬ ë¡œë“œ (120MB)
    embeddings = [self.get_embedding(a.content) for a in articles]  # 280MB
    # ì´ 450MB ë©”ëª¨ë¦¬ ìŠ¤íŒŒì´í¬
```

**í•´ê²°ì±…: ë°°ì¹˜ ì²˜ë¦¬ + ì œë„ˆë ˆì´í„°**

**íŒŒì¼**: `backend/data/processors/news_processor.py` (ìˆ˜ì •)

```python
"""
News Processor - ë©”ëª¨ë¦¬ ìµœì í™”

Date: 2026-01-03
Optimization: Batch processing with generator
"""
import gc
from typing import List, Generator
import logging

logger = logging.getLogger(__name__)


class NewsProcessor:
    """ë‰´ìŠ¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ - ë©”ëª¨ë¦¬ ìµœì í™” ë²„ì „"""

    def __init__(self, batch_size: int = 5):
        """
        ì´ˆê¸°í™”

        Args:
            batch_size: ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ì œí•œì— ë”°ë¼ ì¡°ì ˆ)
        """
        self.batch_size = batch_size
        self.embedding_model = LocalEmbeddingModel()  # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©

    def process_articles_batched(
        self,
        articles: List[Article]
    ) -> Generator[Dict, None, None]:
        """
        ë°°ì¹˜ ë‹¨ìœ„ ê¸°ì‚¬ ì²˜ë¦¬ (ì œë„ˆë ˆì´í„°)

        Args:
            articles: ì²˜ë¦¬í•  ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸

        Yields:
            ì²˜ë¦¬ëœ ê¸°ì‚¬ ë°ì´í„°
        """
        total = len(articles)
        logger.info(f"Processing {total} articles in batches of {self.batch_size}")

        for i in range(0, total, self.batch_size):
            batch = articles[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (total + self.batch_size - 1) // self.batch_size

            logger.debug(f"Processing batch {batch_num}/{total_batches}")

            # ë°°ì¹˜ ì²˜ë¦¬
            processed_batch = self._process_batch(batch)

            # ê°œë³„ ê¸°ì‚¬ yield
            for article_data in processed_batch:
                yield article_data

            # ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ
            del batch
            del processed_batch
            gc.collect()

    def _process_batch(self, batch: List[Article]) -> List[Dict]:
        """
        ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬

        Args:
            batch: ê¸°ì‚¬ ë°°ì¹˜

        Returns:
            ì²˜ë¦¬ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = [article.content for article in batch]

        # 2. ë°°ì¹˜ ì„ë² ë”© (ë‹¨ì¼ API í˜¸ì¶œ)
        embeddings = self.embedding_model.get_embeddings_batch(texts)

        # 3. ë°ì´í„° ì¡°í•©
        processed = []
        for article, embedding in zip(batch, embeddings):
            processed.append({
                'id': article.id,
                'title': article.title,
                'content': article.content,
                'embedding': embedding,
                'processed_at': datetime.now()
            })

        return processed

    def save_processed_articles(self, articles: List[Article]):
        """
        ì²˜ë¦¬ ë° ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )

        Args:
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        from backend.database.repository import NewsRepository

        repo = NewsRepository()
        saved_count = 0

        # ì œë„ˆë ˆì´í„°ë¡œ ìˆœíšŒ (ë©”ëª¨ë¦¬ ì ˆì•½)
        for article_data in self.process_articles_batched(articles):
            try:
                repo.save_article_with_embedding(article_data)
                saved_count += 1

                if saved_count % 10 == 0:
                    logger.info(f"Saved {saved_count} articles")

            except Exception as e:
                logger.error(f"Failed to save article {article_data['id']}: {e}")

        logger.info(f"âœ… Saved {saved_count}/{len(articles)} articles")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    processor = NewsProcessor(batch_size=5)

    # 20ê°œ ê¸°ì‚¬ ì²˜ë¦¬
    articles = fetch_recent_articles(limit=20)

    # Before: 450MB ë©”ëª¨ë¦¬
    # After: 100MB ë©”ëª¨ë¦¬ (5ê°œì”© ë°°ì¹˜ ì²˜ë¦¬)
    processor.save_processed_articles(articles)
```

**ì˜ˆìƒ íš¨ê³¼:**
- ë©”ëª¨ë¦¬ ì‚¬ìš©: 450MB â†’ 100MB (78% ê°ì†Œ)
- ì²˜ë¦¬ ì†ë„: ë™ì¼ ìœ ì§€
- OOM ì—ëŸ¬ ë°©ì§€

---

### 3.2 Performance Monitor Hook

**ì„¤ì¹˜ ë°©ë²•:**
```bash
npx claude-code-templates@latest --hook performance-monitor --yes
```

**êµ¬í˜„:**

**íŒŒì¼**: `backend/monitoring/performance_monitor.py` (ì‹ ê·œ ìƒì„±)

```python
"""
Performance Monitor - ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¶”ì 

Features:
- í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ëª¨ë‹ˆí„°ë§
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
- ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ìë™ ì•Œë¦¼
- ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

Date: 2026-01-03
Author: AI Trading System Team
"""
import time
import psutil
from functools import wraps
from typing import Callable, Dict, List
import asyncio
import logging
from datetime import datetime

logger = logging.getLogger(__name__)


class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„° ë° ìœ í‹¸ë¦¬í‹°"""

    def __init__(self, threshold_seconds: float = 5.0):
        """
        ì´ˆê¸°í™”

        Args:
            threshold_seconds: ì•Œë¦¼ ì„ê³„ê°’ (ì´ˆ)
        """
        self.threshold = threshold_seconds
        self.metrics: List[Dict] = []
        self.max_metrics = 1000  # ìµœëŒ€ 1000ê°œ ë©”íŠ¸ë¦­ ì €ì¥

    def monitor(self, func: Callable):
        """
        í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°

        Usage:
            @perf_monitor.monitor
            async def my_function():
                ...
        """
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            # ì‹œì‘ ë©”íŠ¸ë¦­
            start_time = time.time()
            process = psutil.Process()
            start_memory = process.memory_info().rss / 1024 / 1024  # MB

            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                # ì¢…ë£Œ ë©”íŠ¸ë¦­
                elapsed = time.time() - start_time
                end_memory = process.memory_info().rss / 1024 / 1024
                memory_delta = end_memory - start_memory

                # ë©”íŠ¸ë¦­ ê¸°ë¡
                metric = {
                    'function': func.__name__,
                    'elapsed': elapsed,
                    'memory_mb': end_memory,
                    'memory_delta': memory_delta,
                    'timestamp': datetime.now().isoformat(),
                    'threshold_exceeded': elapsed > self.threshold
                }

                self._record_metric(metric)

                # ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì•Œë¦¼
                if elapsed > self.threshold:
                    await self._send_alert(metric)

                # ë¡œê¹…
                log_level = logging.WARNING if elapsed > self.threshold else logging.DEBUG
                logger.log(
                    log_level,
                    f"{func.__name__} took {elapsed:.2f}s "
                    f"(mem: {end_memory:.1f}MB, delta: {memory_delta:+.1f}MB)"
                )

        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            # ë™ê¸° í•¨ìˆ˜ ë²„ì „
            start_time = time.time()
            process = psutil.Process()
            start_memory = process.memory_info().rss / 1024 / 1024

            try:
                result = func(*args, **kwargs)
                return result
            finally:
                elapsed = time.time() - start_time
                end_memory = process.memory_info().rss / 1024 / 1024
                memory_delta = end_memory - start_memory

                metric = {
                    'function': func.__name__,
                    'elapsed': elapsed,
                    'memory_mb': end_memory,
                    'memory_delta': memory_delta,
                    'timestamp': datetime.now().isoformat(),
                    'threshold_exceeded': elapsed > self.threshold
                }

                self._record_metric(metric)

                if elapsed > self.threshold:
                    # ë™ê¸° í•¨ìˆ˜ì—ì„œëŠ” blocking ì•Œë¦¼
                    logger.warning(
                        f"âš ï¸  Performance Alert: {func.__name__} took {elapsed:.2f}s "
                        f"(threshold: {self.threshold}s)"
                    )

                logger.debug(
                    f"{func.__name__} took {elapsed:.2f}s "
                    f"(mem: {end_memory:.1f}MB, delta: {memory_delta:+.1f}MB)"
                )

        # ë¹„ë™ê¸°/ë™ê¸° í•¨ìˆ˜ êµ¬ë¶„
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        return sync_wrapper

    def _record_metric(self, metric: Dict):
        """ë©”íŠ¸ë¦­ ê¸°ë¡ (ìˆœí™˜ ë²„í¼)"""
        self.metrics.append(metric)

        # ìµœëŒ€ ê°œìˆ˜ ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ ë©”íŠ¸ë¦­ ì œê±°
        if len(self.metrics) > self.max_metrics:
            self.metrics = self.metrics[-self.max_metrics:]

    async def _send_alert(self, metric: Dict):
        """ì„±ëŠ¥ ì•Œë¦¼ ì „ì†¡"""
        try:
            from backend.notifications.telegram_notifier import create_telegram_notifier

            telegram = create_telegram_notifier()
            await telegram.send_message(
                f"âš ï¸ Performance Alert\n\n"
                f"Function: {metric['function']}\n"
                f"Time: {metric['elapsed']:.2f}s (threshold: {self.threshold}s)\n"
                f"Memory: {metric['memory_mb']:.1f}MB (delta: {metric['memory_delta']:+.1f}MB)\n"
                f"Timestamp: {metric['timestamp']}"
            )
        except Exception as e:
            logger.error(f"Failed to send performance alert: {e}")

    def get_metrics(
        self,
        function_name: str = None,
        limit: int = 10
    ) -> List[Dict]:
        """
        ë©”íŠ¸ë¦­ ì¡°íšŒ

        Args:
            function_name: í•¨ìˆ˜ëª… í•„í„° (Noneì´ë©´ ì „ì²´)
            limit: ìµœëŒ€ ê°œìˆ˜

        Returns:
            ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸ (ìµœì‹ ìˆœ)
        """
        if function_name:
            filtered = [m for m in self.metrics if m['function'] == function_name]
        else:
            filtered = self.metrics

        return sorted(filtered, key=lambda x: x['timestamp'], reverse=True)[:limit]

    def get_summary(self) -> Dict:
        """
        ì„±ëŠ¥ ìš”ì•½ í†µê³„

        Returns:
            ìš”ì•½ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if not self.metrics:
            return {'message': 'No metrics collected yet'}

        # í•¨ìˆ˜ë³„ ê·¸ë£¹í™”
        by_function = {}
        for metric in self.metrics:
            fname = metric['function']
            if fname not in by_function:
                by_function[fname] = []
            by_function[fname].append(metric['elapsed'])

        # í†µê³„ ê³„ì‚°
        summary = {}
        for fname, times in by_function.items():
            summary[fname] = {
                'calls': len(times),
                'avg_time': sum(times) / len(times),
                'min_time': min(times),
                'max_time': max(times),
                'threshold_exceeded': sum(1 for t in times if t > self.threshold)
            }

        return summary


# ê¸€ë¡œë²Œ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
perf_monitor = PerformanceMonitor(threshold_seconds=5.0)


# í¸ì˜ í•¨ìˆ˜
def get_performance_summary() -> Dict:
    """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ"""
    return perf_monitor.get_summary()
```

**ì ìš© ì˜ˆì‹œ:**

```python
# backend/ai/mvp/war_room_mvp.py
from backend.monitoring.performance_monitor import perf_monitor

class WarRoomMVP:
    @perf_monitor.monitor
    def deliberate(self, symbol, ...):
        """
        5ì´ˆ ì´ˆê³¼ ì‹œ ìë™ Telegram ì•Œë¦¼
        """
        ...

# backend/data/processors/news_processor.py
from backend.monitoring.performance_monitor import perf_monitor

class NewsProcessor:
    @perf_monitor.monitor
    def process_articles_batched(self, articles):
        """
        ì²˜ë¦¬ ì‹œê°„ ìë™ ì¶”ì 
        """
        ...
```

**API ì—”ë“œí¬ì¸íŠ¸:**

**íŒŒì¼**: `backend/api/monitoring_router.py` (ì‹ ê·œ ìƒì„±)

```python
"""
Monitoring API Router

Date: 2026-01-03
"""
from fastapi import APIRouter
from backend.monitoring.performance_monitor import get_performance_summary

router = APIRouter(prefix="/api/monitoring", tags=["Monitoring"])


@router.get("/performance/summary")
async def performance_summary():
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìš”ì•½"""
    return get_performance_summary()


@router.get("/performance/metrics/{function_name}")
async def performance_metrics(function_name: str, limit: int = 10):
    """í•¨ìˆ˜ë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    from backend.monitoring.performance_monitor import perf_monitor
    return perf_monitor.get_metrics(function_name, limit)
```

**ì˜ˆìƒ íš¨ê³¼:**
- âœ… ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- âœ… 5ì´ˆ ì´ˆê³¼ í•¨ìˆ˜ ìë™ ì•Œë¦¼
- âœ… ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ì¶”ì 
- âœ… ë³‘ëª© ì§€ì  ìë™ íƒì§€

---

### 3.3 êµ¬í˜„ ë¡œë“œë§µ (Performance)

**Week 1: ì„±ëŠ¥ ê°ì‚¬ ë„êµ¬**
- [ ] Day 1: `/performance-audit` ì„¤ì¹˜ ë° í…ŒìŠ¤íŠ¸
- [ ] Day 2-3: War Room MVP ë³‘ë ¬í™” êµ¬í˜„
- [ ] Day 4-5: ë‰´ìŠ¤ ë°±í•„ ë©”ëª¨ë¦¬ ìµœì í™”
- [ ] Day 6-7: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

**Week 2: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**
- [ ] Day 1-2: PerformanceMonitor í´ë˜ìŠ¤ êµ¬í˜„
- [ ] Day 3-4: ì£¼ìš” í•¨ìˆ˜ì— ë°ì½”ë ˆì´í„° ì ìš©
- [ ] Day 5: Telegram ì•Œë¦¼ í†µí•©
- [ ] Day 6-7: API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

**Week 3: ëŒ€ì‹œë³´ë“œ (ì„ íƒ)**
- [ ] Day 1-3: ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì‹œê°í™”
- [ ] Day 4-5: íˆìŠ¤í† ë¦¬ ì°¨íŠ¸
- [ ] Day 6-7: ìë™ ë¦¬í¬íŠ¸ ìƒì„±

**ì˜ˆìƒ íš¨ê³¼:**
- âœ… War Room MVP: 12.76ì´ˆ â†’ 7.5ì´ˆ (41% ê°œì„ )
- âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©: 450MB â†’ 100MB (78% ê°ì†Œ)
- âœ… ì„±ëŠ¥ ì €í•˜ ê°ì§€: ìˆ˜ë™ â†’ ìë™ (ì‹¤ì‹œê°„)

---

## Part 4: Advanced Analytics (ê³ ê¸‰ ë¶„ì„)

### ëª©í‘œ
- Shadow Trading í†µê³„ ë¶„ì„ ê³ ë„í™”
- ìƒ¤í”„/ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨ ìë™ ê³„ì‚°
- ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë„ì… (ë¹„ìš© ì ˆê°)
- í‹°ì»¤ ì¶”ì¶œ ì •í™•ë„ í–¥ìƒ (60% â†’ 90%)

---

### 4.1 Data Scientist Agent

**ì„¤ì¹˜ ë°©ë²•:**
```bash
npx claude-code-templates@latest --agent data-scientist --yes
```

**ì£¼ìš” ê¸°ëŠ¥:**
1. Shadow Trading ê³ ê¸‰ í†µê³„ ë¶„ì„
2. ìƒ¤í”„/ì†Œë¥´í‹°ë…¸/ì¹¼ë§ˆ ë¹„ìœ¨ ê³„ì‚°
3. ì—°ìŠ¹/ì—°íŒ¨ ë¶„ì„
4. í†µê³„ì  ìœ ì˜ì„± ê²€ì •

---

#### Implementation 4-1: Shadow Trading í†µê³„ ë¶„ì„

**íŒŒì¼**: `backend/analytics/shadow_trading_analyzer.py` (ì‹ ê·œ ìƒì„±)

```python
"""
Shadow Trading í†µê³„ ë¶„ì„

Features:
- ìƒ¤í”„ ë¹„ìœ¨ (Sharpe Ratio)
- ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨ (Sortino Ratio)
- ì¹¼ë§ˆ ë¹„ìœ¨ (Calmar Ratio)
- ì—°ìŠ¹/ì—°íŒ¨ ë¶„ì„
- í†µê³„ì  ìœ ì˜ì„± ê²€ì •

Date: 2026-01-03
Author: AI Trading System Team
"""
import pandas as pd
import numpy as np
from scipy import stats
from typing import Dict, List
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class ShadowTradingAnalyzer:
    """Shadow Trading ì„±ê³¼ í†µê³„ ë¶„ì„"""

    def __init__(self, trades: List[Dict]):
        """
        ì´ˆê¸°í™”

        Args:
            trades: ê±°ë˜ ë¦¬ìŠ¤íŠ¸
                [
                    {
                        'symbol': 'AAPL',
                        'action': 'BUY',
                        'entry_price': 150.0,
                        'exit_price': 155.0,
                        'pnl': 500.0,
                        'pnl_pct': 0.033,
                        'created_at': '2026-01-01'
                    },
                    ...
                ]
        """
        self.trades_df = pd.DataFrame(trades)

        if not self.trades_df.empty:
            self.trades_df['created_at'] = pd.to_datetime(self.trades_df['created_at'])
            self.trades_df = self.trades_df.sort_values('created_at')

    def calculate_sharpe_ratio(self, risk_free_rate: float = 0.02) -> float:
        """
        ìƒ¤í”„ ë¹„ìœ¨ ê³„ì‚° (ì—°ìœ¨í™”)

        Args:
            risk_free_rate: ë¬´ìœ„í—˜ ìˆ˜ìµë¥  (ì—°ìœ¨)

        Returns:
            ìƒ¤í”„ ë¹„ìœ¨
        """
        if len(self.trades_df) < 2:
            return 0.0

        returns = self.trades_df['pnl_pct'].values

        # ì¼ë³„ ë¬´ìœ„í—˜ ìˆ˜ìµë¥ 
        daily_rf = risk_free_rate / 252

        # ì´ˆê³¼ ìˆ˜ìµë¥ 
        excess_returns = returns - daily_rf

        if np.std(excess_returns) == 0:
            return 0.0

        # ì—°ìœ¨í™” (252 ê±°ë˜ì¼ ê°€ì •)
        sharpe = (np.mean(excess_returns) / np.std(excess_returns)) * np.sqrt(252)

        return sharpe

    def calculate_sortino_ratio(self, risk_free_rate: float = 0.02) -> float:
        """
        ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨ (í•˜ë°© ë¦¬ìŠ¤í¬ë§Œ ê³ ë ¤)

        Args:
            risk_free_rate: ë¬´ìœ„í—˜ ìˆ˜ìµë¥ 

        Returns:
            ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨
        """
        returns = self.trades_df['pnl_pct'].values
        daily_rf = risk_free_rate / 252
        excess_returns = returns - daily_rf

        # í•˜ë°© ìˆ˜ìµë¥ ë§Œ ì¶”ì¶œ
        downside_returns = excess_returns[excess_returns < 0]

        if len(downside_returns) == 0:
            return 0.0

        downside_std = np.std(downside_returns)
        if downside_std == 0:
            return 0.0

        # ì—°ìœ¨í™”
        sortino = (np.mean(excess_returns) / downside_std) * np.sqrt(252)

        return sortino

    def calculate_calmar_ratio(self) -> float:
        """
        ì¹¼ë§ˆ ë¹„ìœ¨ (ì—°ê°„ ìˆ˜ìµë¥  / ìµœëŒ€ ë‚™í­)

        Returns:
            ì¹¼ë§ˆ ë¹„ìœ¨
        """
        if len(self.trades_df) == 0:
            return 0.0

        # ì—°ê°„ ìˆ˜ìµë¥  ì¶”ì •
        annual_return = self.trades_df['pnl_pct'].mean() * 252

        # ìµœëŒ€ ë‚™í­
        mdd = self.calculate_max_drawdown()

        if mdd == 0:
            return 0.0

        return annual_return / abs(mdd)

    def calculate_max_drawdown(self) -> float:
        """
        ìµœëŒ€ ë‚™í­ (Maximum Drawdown)

        Returns:
            MDD (ìŒìˆ˜)
        """
        if len(self.trades_df) == 0:
            return 0.0

        # ëˆ„ì  ìˆ˜ìµë¥ 
        cumulative = (1 + self.trades_df['pnl_pct']).cumprod()

        # ëˆ„ì  ìµœëŒ€ê°’
        running_max = cumulative.cummax()

        # Drawdown
        drawdown = (cumulative - running_max) / running_max

        return drawdown.min()

    def analyze_win_streaks(self) -> Dict:
        """
        ì—°ìŠ¹/ì—°íŒ¨ ë¶„ì„

        Returns:
            ì—°ìŠ¹/ì—°íŒ¨ í†µê³„
        """
        if len(self.trades_df) == 0:
            return {
                'max_win_streak': 0,
                'max_loss_streak': 0,
                'current_streak': 0,
                'current_streak_type': 'NONE'
            }

        wins = (self.trades_df['pnl'] > 0).astype(int).values

        current_streak = 0
        max_win_streak = 0
        max_loss_streak = 0

        for win in wins:
            if win == 1:
                # ìŠ¹ë¦¬
                current_streak = current_streak + 1 if current_streak > 0 else 1
                max_win_streak = max(max_win_streak, current_streak)
            else:
                # ì†ì‹¤
                current_streak = current_streak - 1 if current_streak < 0 else -1
                max_loss_streak = max(max_loss_streak, abs(current_streak))

        return {
            'max_win_streak': max_win_streak,
            'max_loss_streak': max_loss_streak,
            'current_streak': abs(current_streak),
            'current_streak_type': 'WIN' if current_streak > 0 else 'LOSS' if current_streak < 0 else 'NONE'
        }

    def statistical_significance_test(self) -> Dict:
        """
        í†µê³„ì  ìœ ì˜ì„± ê²€ì • (Win Rate > 50%?)

        Returns:
            ê²€ì • ê²°ê³¼
        """
        if len(self.trades_df) == 0:
            return {
                'win_rate': 0.0,
                'p_value': 1.0,
                'significant': False,
                'confidence_level': 0.0
            }

        wins = len(self.trades_df[self.trades_df['pnl'] > 0])
        total = len(self.trades_df)

        win_rate = wins / total if total > 0 else 0.0

        # ì´í•­ ê²€ì • (H0: p = 0.5, H1: p > 0.5)
        p_value = stats.binom_test(wins, total, 0.5, alternative='greater')

        return {
            'win_rate': win_rate,
            'total_trades': total,
            'wins': wins,
            'losses': total - wins,
            'p_value': p_value,
            'significant': p_value < 0.05,
            'confidence_level': (1 - p_value) * 100
        }

    def generate_report(self) -> Dict:
        """
        ì¢…í•© í†µê³„ ë¦¬í¬íŠ¸

        Returns:
            ì „ì²´ ë¶„ì„ ê²°ê³¼
        """
        if len(self.trades_df) == 0:
            return {'error': 'No trades to analyze'}

        return {
            'basic_metrics': {
                'total_trades': len(self.trades_df),
                'win_rate': len(self.trades_df[self.trades_df['pnl'] > 0]) / len(self.trades_df),
                'avg_pnl': float(self.trades_df['pnl'].mean()),
                'total_pnl': float(self.trades_df['pnl'].sum()),
                'avg_pnl_pct': float(self.trades_df['pnl_pct'].mean()),
                'best_trade': float(self.trades_df['pnl'].max()),
                'worst_trade': float(self.trades_df['pnl'].min())
            },
            'risk_metrics': {
                'sharpe_ratio': float(self.calculate_sharpe_ratio()),
                'sortino_ratio': float(self.calculate_sortino_ratio()),
                'calmar_ratio': float(self.calculate_calmar_ratio()),
                'max_drawdown': float(self.calculate_max_drawdown())
            },
            'streak_analysis': self.analyze_win_streaks(),
            'statistical_test': self.statistical_significance_test(),
            'generated_at': datetime.now().isoformat()
        }


# CLI ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # Shadow Trading ë°ì´í„° ë¡œë“œ
    from backend.execution.shadow_trading import ShadowTradingEngine

    engine = ShadowTradingEngine()
    trades = engine.get_trade_history()

    analyzer = ShadowTradingAnalyzer(trades)
    report = analyzer.generate_report()

    print("ğŸ“Š Shadow Trading Statistical Analysis")
    print("=" * 60)
    print(f"Total Trades: {report['basic_metrics']['total_trades']}")
    print(f"Win Rate: {report['basic_metrics']['win_rate']:.1%}")
    print(f"Sharpe Ratio: {report['risk_metrics']['sharpe_ratio']:.2f}")
    print(f"Sortino Ratio: {report['risk_metrics']['sortino_ratio']:.2f}")
    print(f"Calmar Ratio: {report['risk_metrics']['calmar_ratio']:.2f}")
    print(f"Max Drawdown: {report['risk_metrics']['max_drawdown']:.2%}")
    print(f"\nStatistical Significance: {report['statistical_test']['significant']}")
    print(f"P-value: {report['statistical_test']['p_value']:.4f}")
```

**API ì—”ë“œí¬ì¸íŠ¸:**

**íŒŒì¼**: `backend/api/analytics_router.py` (ì‹ ê·œ ìƒì„±)

```python
"""
Analytics API Router

Date: 2026-01-03
"""
from fastapi import APIRouter
from backend.analytics.shadow_trading_analyzer import ShadowTradingAnalyzer
from backend.execution.shadow_trading import ShadowTradingEngine

router = APIRouter(prefix="/api/analytics", tags=["Analytics"])


@router.get("/shadow-trading/report")
async def shadow_trading_report():
    """Shadow Trading í†µê³„ ë¦¬í¬íŠ¸"""
    engine = ShadowTradingEngine()
    trades = engine.get_trade_history()

    analyzer = ShadowTradingAnalyzer(trades)
    return analyzer.generate_report()


@router.get("/shadow-trading/metrics")
async def shadow_trading_metrics():
    """ì£¼ìš” ë©”íŠ¸ë¦­ë§Œ ë°˜í™˜"""
    engine = ShadowTradingEngine()
    trades = engine.get_trade_history()

    analyzer = ShadowTradingAnalyzer(trades)
    report = analyzer.generate_report()

    return {
        'win_rate': report['basic_metrics']['win_rate'],
        'sharpe_ratio': report['risk_metrics']['sharpe_ratio'],
        'sortino_ratio': report['risk_metrics']['sortino_ratio'],
        'max_drawdown': report['risk_metrics']['max_drawdown'],
        'statistical_significance': report['statistical_test']['significant']
    }
```

**ì˜ˆìƒ íš¨ê³¼:**
- âœ… ìƒ¤í”„ ë¹„ìœ¨ ìë™ ê³„ì‚°
- âœ… ì£¼ê°„ í†µê³„ ë¦¬í¬íŠ¸ ìë™ ìƒì„±
- âœ… í†µê³„ì  ìœ ì˜ì„± ê²€ì¦

---

### Component 4.2: NLP Engineer Agent - ë¡œì»¬ ì„ë² ë”© ë° í‹°ì»¤ ì¶”ì¶œ ê³ ë„í™”

**í˜„ì¬ ìƒíƒœ:**
- OpenAI API ì‚¬ìš© (text-embedding-ada-002): $0.0001/1K tokens
- ì›”ê°„ ì„ë² ë”© ë¹„ìš©: ~$15-30
- í‹°ì»¤ ì¶”ì¶œ: ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ (ì •í™•ë„ ~85%)
- spaCy ë¯¸ì‚¬ìš©
- Custom NER ëª¨ë¸ ì—†ìŒ

**ëª©í‘œ:**
- OpenAI ì„ë² ë”© â†’ ë¡œì»¬ ì„ë² ë”© ì „í™˜ (ë¹„ìš© $0)
- í‹°ì»¤ ì¶”ì¶œ ì •í™•ë„: 85% â†’ 95%+
- ê¸ˆìœµ ë„ë©”ì¸ NER ëª¨ë¸ êµ¬ì¶•
- ë‰´ìŠ¤ sentiment ë¶„ì„ ê³ ë„í™”

---

#### 4.2.1 ë¡œì»¬ ì„ë² ë”© ì „í™˜ (Sentence Transformers)

**ëª©í‘œ:**
- OpenAI API ì˜ì¡´ì„± ì œê±°
- ë¹„ìš© ì ˆê° (ì›” $30 â†’ $0)
- ì†ë„ ê°œì„  (ë„¤íŠ¸ì›Œí¬ I/O ì œê±°)

**êµ¬í˜„:**

**íŒŒì¼:** `backend/ai/embeddings/local_embedder.py` (ì‹ ê·œ)

```python
"""
ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ êµ¬í˜„ (Sentence Transformers)

Date: 2026-01-03
Component: NLP Engineer Agent - Local Embeddings
"""

from typing import List, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import torch
from functools import lru_cache
import logging

logger = logging.getLogger(__name__)


class LocalEmbedder:
    """
    Sentence Transformers ê¸°ë°˜ ë¡œì»¬ ì„ë² ë”©

    Models:
    - all-MiniLM-L6-v2: 384 dim, ë¹ ë¦„ (80MB)
    - all-mpnet-base-v2: 768 dim, ì •í™•í•¨ (420MB) - ê¶Œì¥
    """

    def __init__(self, model_name: str = 'all-mpnet-base-v2'):
        """
        Args:
            model_name: HuggingFace ëª¨ë¸ëª…
        """
        self.model_name = model_name
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

        logger.info(f"Loading embedding model: {model_name} on {self.device}")
        self.model = SentenceTransformer(model_name, device=self.device)

        # ëª¨ë¸ ì •ë³´
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        logger.info(f"Embedding dimension: {self.embedding_dim}")

    def encode(
        self,
        texts: List[str],
        batch_size: int = 32,
        show_progress: bool = False
    ) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            show_progress: ì§„í–‰ë¥  í‘œì‹œ

        Returns:
            (N, embedding_dim) numpy array
        """
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=show_progress,
            convert_to_numpy=True,
            normalize_embeddings=True  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš© ì •ê·œí™”
        )

        return embeddings

    def encode_single(self, text: str) -> np.ndarray:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        return self.encode([text])[0]

    @lru_cache(maxsize=1000)
    def encode_cached(self, text: str) -> tuple:
        """
        ìºì‹œëœ ì„ë² ë”© (ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ìš©)

        Note: lru_cacheëŠ” hashable íƒ€ì…ë§Œ ì§€ì›í•˜ë¯€ë¡œ tuple ë°˜í™˜
        """
        embedding = self.encode_single(text)
        return tuple(embedding.tolist())

    def similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„"""
        emb1 = self.encode_single(text1)
        emb2 = self.encode_single(text2)

        # ì´ë¯¸ ì •ê·œí™”ë¨ â†’ ë‚´ì  = ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        similarity = np.dot(emb1, emb2)
        return float(similarity)

    def find_similar(
        self,
        query: str,
        corpus: List[str],
        top_k: int = 5
    ) -> List[tuple]:
        """
        ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ìƒìœ„ Kê°œ í…ìŠ¤íŠ¸ ì°¾ê¸°

        Returns:
            List of (index, similarity_score, text)
        """
        query_emb = self.encode_single(query)
        corpus_embs = self.encode(corpus)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = corpus_embs @ query_emb  # (N,) array

        # ìƒìœ„ Kê°œ
        top_indices = np.argsort(similarities)[::-1][:top_k]

        results = [
            (int(idx), float(similarities[idx]), corpus[idx])
            for idx in top_indices
        ]

        return results


# Singleton instance
_embedder_instance: Optional[LocalEmbedder] = None


def get_embedder(model_name: str = 'all-mpnet-base-v2') -> LocalEmbedder:
    """ì‹±ê¸€í†¤ ì„ë² ë” ì¸ìŠ¤í„´ìŠ¤"""
    global _embedder_instance

    if _embedder_instance is None:
        _embedder_instance = LocalEmbedder(model_name)

    return _embedder_instance
```

**ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸:** `backend/scripts/migrate_embeddings.py` (ì‹ ê·œ)

```python
"""
OpenAI ì„ë² ë”© â†’ ë¡œì»¬ ì„ë² ë”© ë§ˆì´ê·¸ë ˆì´ì…˜

Usage:
    python backend/scripts/migrate_embeddings.py --batch-size 100
"""

import argparse
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from backend.database.models import NewsArticle
from backend.ai.embeddings.local_embedder import get_embedder
from tqdm import tqdm
import numpy as np


def migrate_embeddings(batch_size: int = 100, dry_run: bool = False):
    """ê¸°ì¡´ ë‰´ìŠ¤ ê¸°ì‚¬ ì„ë² ë”© ì¬ê³„ì‚°"""

    # DB ì—°ê²°
    from backend.config import settings
    engine = create_engine(settings.DATABASE_URL)
    Session = sessionmaker(bind=engine)
    session = Session()

    # ë¡œì»¬ ì„ë² ë” ë¡œë“œ
    embedder = get_embedder()
    print(f"âœ… Embedder loaded: {embedder.model_name} (dim={embedder.embedding_dim})")

    # OpenAI ì„ë² ë”©ì´ ìˆëŠ” ê¸°ì‚¬ ì¡°íšŒ
    articles = session.query(NewsArticle).filter(
        NewsArticle.embedding.isnot(None)
    ).all()

    print(f"ğŸ“Š Total articles to migrate: {len(articles)}")

    if dry_run:
        print("ğŸ” DRY RUN - No changes will be made")
        return

    # ë°°ì¹˜ ì²˜ë¦¬
    for i in tqdm(range(0, len(articles), batch_size), desc="Migrating"):
        batch = articles[i:i+batch_size]

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = [
            f"{article.title}\n{article.content[:500]}"
            for article in batch
        ]

        # ë¡œì»¬ ì„ë² ë”© ìƒì„±
        embeddings = embedder.encode(texts, batch_size=batch_size)

        # DB ì—…ë°ì´íŠ¸
        for article, embedding in zip(batch, embeddings):
            # PostgreSQL ARRAYë¡œ ì €ì¥ (pgvector ì‚¬ìš© ì‹œ vector íƒ€ì…)
            article.embedding = embedding.tolist()

        session.commit()

    print("âœ… Migration completed!")

    # í†µê³„
    avg_similarity = verify_migration(session, embedder, sample_size=10)
    print(f"ğŸ“ˆ Avg similarity (OpenAI vs Local): {avg_similarity:.3f}")


def verify_migration(session, embedder, sample_size: int = 10):
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ (ìƒ˜í”Œë§)"""
    import random

    articles = session.query(NewsArticle).limit(sample_size).all()

    similarities = []
    for article in articles:
        # ë¡œì»¬ ì„ë² ë”© ì¬ê³„ì‚°
        text = f"{article.title}\n{article.content[:500]}"
        new_emb = embedder.encode_single(text)

        # ê¸°ì¡´ ì„ë² ë”©ê³¼ ë¹„êµ
        old_emb = np.array(article.embedding)

        # ì •ê·œí™”
        old_emb = old_emb / np.linalg.norm(old_emb)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        sim = np.dot(old_emb, new_emb)
        similarities.append(sim)

    return np.mean(similarities)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--batch-size', type=int, default=100)
    parser.add_argument('--dry-run', action='store_true')
    args = parser.parse_args()

    migrate_embeddings(args.batch_size, args.dry_run)
```

**Repository ì—…ë°ì´íŠ¸:** `backend/database/repository.py`

```python
# Lines 90-105: add_article() ë©”ì„œë“œ ìˆ˜ì •

from backend.ai.embeddings.local_embedder import get_embedder

class NewsRepository:
    def __init__(self, session):
        self.session = session
        self.embedder = get_embedder()  # ë¡œì»¬ ì„ë² ë”

    def add_article(self, article_data: dict) -> NewsArticle:
        # ... ê¸°ì¡´ ë¡œì§

        # ì„ë² ë”© ìƒì„± (OpenAI ëŒ€ì‹  ë¡œì»¬)
        text = f"{article_data['title']}\n{article_data['content'][:500]}"
        embedding = self.embedder.encode_single(text)

        article = NewsArticle(
            **article_data,
            embedding=embedding.tolist()  # numpy â†’ list
        )

        self.session.add(article)
        self.session.commit()

        return article
```

**ì˜ˆìƒ íš¨ê³¼:**
- âœ… ë¹„ìš© ì ˆê°: $30/ì›” â†’ $0
- âœ… ì†ë„ ê°œì„ : 200ms/article â†’ 50ms/article (4ë°° ê³ ì†í™”)
- âœ… ì˜¤í”„ë¼ì¸ ì‘ë™ ê°€ëŠ¥
- âš ï¸ ì´ˆê¸° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ~420MB

**ì„¤ì¹˜:**
```bash
pip install sentence-transformers
```

---

#### 4.2.2 ê¸ˆìœµ ë„ë©”ì¸ NER (Named Entity Recognition)

**ëª©í‘œ:**
- í‹°ì»¤ ì¶”ì¶œ ì •í™•ë„: 85% â†’ 95%+
- íšŒì‚¬ëª… â†’ í‹°ì»¤ ë§¤í•‘
- ê¸ˆìœµ ìš©ì–´ ì¸ì‹ (IPO, M&A, earnings, etc.)

**êµ¬í˜„:**

**íŒŒì¼:** `backend/ai/ner/ticker_extractor.py` (ì‹ ê·œ)

```python
"""
ê¸ˆìœµ ë„ë©”ì¸ NER - í‹°ì»¤ ë° íšŒì‚¬ëª… ì¶”ì¶œ

Date: 2026-01-03
Component: NLP Engineer Agent - Ticker Extraction
"""

import re
import spacy
from typing import List, Dict, Set, Tuple
import logging
from functools import lru_cache

logger = logging.getLogger(__name__)


class TickerExtractor:
    """
    í‹°ì»¤ ì‹¬ë³¼ ë° íšŒì‚¬ëª… ì¶”ì¶œ

    Methods:
    1. ì •ê·œí‘œí˜„ì‹ (ê¸°ë³¸)
    2. spaCy NER (íšŒì‚¬ëª…)
    3. ì»¤ìŠ¤í…€ ì‚¬ì „ (NASDAQ/NYSE ë§¤í•‘)
    """

    def __init__(self):
        # spaCy ëª¨ë¸ ë¡œë“œ
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            logger.warning("spaCy model not found. Run: python -m spacy download en_core_web_sm")
            self.nlp = None

        # í‹°ì»¤ â†’ íšŒì‚¬ëª… ë§¤í•‘
        self.ticker_to_company = self._load_ticker_mapping()

        # íšŒì‚¬ëª… â†’ í‹°ì»¤ ì—­ë°©í–¥ ë§¤í•‘
        self.company_to_ticker = {
            v.lower(): k for k, v in self.ticker_to_company.items()
        }

        # ì•Œë ¤ì§„ í‹°ì»¤ ì§‘í•© (ë¹ ë¥¸ ê²€ìƒ‰)
        self.known_tickers = set(self.ticker_to_company.keys())

    def _load_ticker_mapping(self) -> Dict[str, str]:
        """
        í‹°ì»¤ â†’ íšŒì‚¬ëª… ë§¤í•‘ ë¡œë“œ

        TODO: DBì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ CSV íŒŒì¼ ì‚¬ìš©
        """
        return {
            'AAPL': 'Apple Inc.',
            'MSFT': 'Microsoft Corporation',
            'GOOGL': 'Alphabet Inc.',
            'AMZN': 'Amazon.com Inc.',
            'NVDA': 'NVIDIA Corporation',
            'TSLA': 'Tesla Inc.',
            'META': 'Meta Platforms Inc.',
            'AMD': 'Advanced Micro Devices Inc.',
            'INTC': 'Intel Corporation',
            'QCOM': 'Qualcomm Incorporated',
            # ... ë” ë§ì€ ë§¤í•‘ í•„ìš” (í˜„ì¬ ~50ê°œ, ëª©í‘œ 500+ê°œ)
        }

    def extract_tickers_regex(self, text: str) -> List[str]:
        """
        ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ í‹°ì»¤ ì¶”ì¶œ (ê¸°ë³¸ ë°©ë²•)

        íŒ¨í„´:
        - ëŒ€ë¬¸ì 1-5ì (ì˜ˆ: AAPL, GOOGL)
        - $ ì ‘ë‘ì‚¬ (ì˜ˆ: $TSLA)
        - ê´„í˜¸ ë‚´ (ì˜ˆ: Apple (AAPL))
        """
        tickers = set()

        # íŒ¨í„´ 1: $TICKER
        pattern1 = r'\$([A-Z]{1,5})\b'
        tickers.update(re.findall(pattern1, text))

        # íŒ¨í„´ 2: (TICKER)
        pattern2 = r'\(([A-Z]{1,5})\)'
        tickers.update(re.findall(pattern2, text))

        # íŒ¨í„´ 3: TICKER (ë‹¨ì–´ ê²½ê³„)
        # ì£¼ì˜: USA, CEO ë“± ì œì™¸ í•„ìš”
        pattern3 = r'\b([A-Z]{2,5})\b'
        candidates = re.findall(pattern3, text)

        # ì•Œë ¤ì§„ í‹°ì»¤ë§Œ í¬í•¨
        for candidate in candidates:
            if candidate in self.known_tickers:
                tickers.add(candidate)

        return sorted(tickers)

    def extract_companies_ner(self, text: str) -> List[Tuple[str, str]]:
        """
        spaCy NERë¡œ íšŒì‚¬ëª… ì¶”ì¶œ í›„ í‹°ì»¤ ë§¤í•‘

        Returns:
            List of (company_name, ticker)
        """
        if not self.nlp:
            return []

        doc = self.nlp(text)
        results = []

        for ent in doc.ents:
            if ent.label_ == 'ORG':  # Organization
                company_name = ent.text

                # íšŒì‚¬ëª… â†’ í‹°ì»¤ ë§¤í•‘
                company_lower = company_name.lower()

                # ì •í™• ë§¤ì¹­
                if company_lower in self.company_to_ticker:
                    ticker = self.company_to_ticker[company_lower]
                    results.append((company_name, ticker))
                else:
                    # ë¶€ë¶„ ë§¤ì¹­ (ì˜ˆ: "Apple" â†’ "Apple Inc.")
                    for full_name, ticker in self.company_to_ticker.items():
                        if company_lower in full_name or full_name in company_lower:
                            results.append((company_name, ticker))
                            break

        return results

    def extract_all(self, text: str) -> Dict[str, any]:
        """
        í†µí•© ì¶”ì¶œ (ì •ê·œí‘œí˜„ì‹ + NER)

        Returns:
            {
                'tickers': ['AAPL', 'NVDA'],
                'companies': [('Apple Inc.', 'AAPL')],
                'confidence': 0.95
            }
        """
        # Method 1: ì •ê·œí‘œí˜„ì‹
        tickers_regex = set(self.extract_tickers_regex(text))

        # Method 2: NER
        companies_ner = self.extract_companies_ner(text)
        tickers_ner = {ticker for _, ticker in companies_ner}

        # ê²°í•©
        all_tickers = tickers_regex | tickers_ner

        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = 1.0 if tickers_ner else 0.85  # NER ë§¤ì¹­ ì‹œ ë†’ì€ ì‹ ë¢°ë„

        return {
            'tickers': sorted(all_tickers),
            'companies': companies_ner,
            'confidence': confidence,
            'methods': {
                'regex': sorted(tickers_regex),
                'ner': sorted(tickers_ner)
            }
        }


# Singleton
_extractor_instance = None


def get_ticker_extractor() -> TickerExtractor:
    """ì‹±ê¸€í†¤ ì¶”ì¶œê¸°"""
    global _extractor_instance

    if _extractor_instance is None:
        _extractor_instance = TickerExtractor()

    return _extractor_instance
```

**ì‚¬ìš© ì˜ˆì‹œ:**

```python
from backend.ai.ner.ticker_extractor import get_ticker_extractor

extractor = get_ticker_extractor()

text = """
Apple (AAPL) reported record earnings today.
NVIDIA shares surged 5% on strong AI demand.
Tesla CEO Elon Musk announced $TSLA price cuts.
"""

result = extractor.extract_all(text)

print(result)
# {
#     'tickers': ['AAPL', 'NVDA', 'TSLA'],
#     'companies': [
#         ('Apple', 'AAPL'),
#         ('NVIDIA', 'NVDA'),
#         ('Tesla', 'TSLA')
#     ],
#     'confidence': 1.0,
#     'methods': {
#         'regex': ['AAPL', 'TSLA'],
#         'ner': ['AAPL', 'NVDA', 'TSLA']
#     }
# }
```

**Repository í†µí•©:** `backend/database/repository.py`

```python
from backend.ai.ner.ticker_extractor import get_ticker_extractor

class NewsRepository:
    def __init__(self, session):
        self.session = session
        self.ticker_extractor = get_ticker_extractor()

    def add_article(self, article_data: dict) -> NewsArticle:
        # í‹°ì»¤ ì¶”ì¶œ (ê°œì„ ëœ ë°©ë²•)
        text = f"{article_data['title']} {article_data['content']}"
        extraction = self.ticker_extractor.extract_all(text)

        article = NewsArticle(
            **article_data,
            tickers=extraction['tickers'],  # ì¶”ì¶œëœ í‹°ì»¤ ë¦¬ìŠ¤íŠ¸
            companies=extraction['companies'],  # íšŒì‚¬ëª…-í‹°ì»¤ ë§¤í•‘
            ticker_confidence=extraction['confidence']  # ì‹ ë¢°ë„
        )

        self.session.add(article)
        self.session.commit()

        return article
```

**ì˜ˆìƒ íš¨ê³¼:**
- âœ… í‹°ì»¤ ì¶”ì¶œ ì •í™•ë„: 85% â†’ 95%+
- âœ… íšŒì‚¬ëª… ë§¤í•‘ ì§€ì›
- âœ… False positive ê°ì†Œ (USA, CEO ë“± ì œì™¸)

**ì„¤ì¹˜:**
```bash
pip install spacy
python -m spacy download en_core_web_sm
```

---

#### 4.2.3 Sentiment ë¶„ì„ ê³ ë„í™”

**í˜„ì¬ ìƒíƒœ:**
- ë‹¨ìˆœ í‚¤ì›Œë“œ ê¸°ë°˜ sentiment (positive/negative/neutral)
- ìˆ˜ë™ ê·œì¹™

**ê°œì„  ë°©í–¥:**
- FinBERT ëª¨ë¸ ì‚¬ìš© (ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™”)
- ê°ì • ì ìˆ˜ (-1 ~ +1)
- ë¬¸ì¥ ë‹¨ìœ„ sentiment

**íŒŒì¼:** `backend/ai/sentiment/finbert_analyzer.py` (ì‹ ê·œ)

```python
"""
FinBERT ê¸°ë°˜ Sentiment ë¶„ì„

Date: 2026-01-03
Component: NLP Engineer Agent - Sentiment Analysis
"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from typing import List, Dict
import numpy as np

class FinBERTAnalyzer:
    """
    FinBERT: ê¸ˆìœµ ë„ë©”ì¸ Sentiment ë¶„ì„

    Model: ProsusAI/finbert
    Labels: positive, negative, neutral
    """

    def __init__(self, model_name: str = 'ProsusAI/finbert'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.eval()

        # Label mapping
        self.id2label = {0: 'positive', 1: 'negative', 2: 'neutral'}

    def analyze(self, text: str) -> Dict[str, any]:
        """
        Sentiment ë¶„ì„

        Returns:
            {
                'label': 'positive',
                'score': 0.92,
                'scores': {'positive': 0.92, 'negative': 0.05, 'neutral': 0.03}
            }
        """
        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)

        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)[0]

        # ìµœê³  í™•ë¥  ë ˆì´ë¸”
        label_id = torch.argmax(probs).item()
        label = self.id2label[label_id]
        score = probs[label_id].item()

        return {
            'label': label,
            'score': score,
            'scores': {
                'positive': probs[0].item(),
                'negative': probs[1].item(),
                'neutral': probs[2].item()
            },
            'sentiment_score': probs[0].item() - probs[1].item()  # -1 ~ +1
        }
```

**ì˜ˆìƒ íš¨ê³¼:**
- âœ… ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” sentiment
- âœ… ì •ëŸ‰ì  ì ìˆ˜ ì œê³µ
- âš ï¸ ëª¨ë¸ í¬ê¸°: ~440MB

---

### êµ¬í˜„ ë¡œë“œë§µ (NLP Engineer Agent)

**Week 1: ë¡œì»¬ ì„ë² ë”©**
- [ ] Sentence Transformers ì„¤ì¹˜
- [ ] LocalEmbedder êµ¬í˜„
- [ ] ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] ê¸°ì¡´ ë‰´ìŠ¤ ì¬ì„ë² ë”© (ë°°ì¹˜)

**Week 2: í‹°ì»¤ ì¶”ì¶œ**
- [ ] spaCy ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- [ ] TickerExtractor êµ¬í˜„
- [ ] í‹°ì»¤ ë§¤í•‘ DB êµ¬ì¶• (500+ í‹°ì»¤)
- [ ] Repository í†µí•©

**Week 3: Sentiment ë¶„ì„**
- [ ] FinBERT ì„¤ì¹˜
- [ ] FinBERTAnalyzer êµ¬í˜„
- [ ] ê¸°ì¡´ ë‰´ìŠ¤ sentiment ì¬ê³„ì‚°
- [ ] API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

**Week 4: ê²€ì¦ ë° íŠœë‹**
- [ ] ì •í™•ë„ ì¸¡ì • (ìˆ˜ë™ ë¼ë²¨ë§ ìƒ˜í”Œ)
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- [ ] ë¬¸ì„œí™”

**ì˜ˆìƒ íš¨ê³¼:**
- ì„ë² ë”© ë¹„ìš©: $30/ì›” â†’ $0
- í‹°ì»¤ ì¶”ì¶œ ì •í™•ë„: 85% â†’ 95%+
- Sentiment ì •í™•ë„: 70% â†’ 85%+

---

## Part 5: Cloud & Infrastructure - AWS Integration MCP

### Component 5.1: AWS Integration MCP - S3 ë°±ì—… ë° Lambda ë°±í•„

**í˜„ì¬ ìƒíƒœ:**
- ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ë°±ì—…ë§Œ ì¡´ì¬
- í´ë¼ìš°ë“œ ë°±ì—… ì—†ìŒ
- ë°ì´í„° ë°±í•„ ìˆ˜ë™ ì‹¤í–‰

**ëª©í‘œ:**
- S3 ìë™ ë°±ì—… (ì¼ì¼/ì£¼ê°„)
- Lambda ë°ì´í„° ë°±í•„ ìë™í™”
- ì¬í•´ ë³µêµ¬ ì‹œìŠ¤í…œ êµ¬ì¶•

---

#### 5.1.1 S3 ë°±ì—… ì‹œìŠ¤í…œ

**êµ¬í˜„:**

**íŒŒì¼:** `backend/cloud/s3_backup.py` (ì‹ ê·œ)

```python
"""
S3 ìë™ ë°±ì—… ì‹œìŠ¤í…œ

Date: 2026-01-03
Component: AWS Integration MCP - S3 Backup
"""

import boto3
from datetime import datetime, timedelta
import gzip
import json
import logging
from pathlib import Path
from typing import Optional
import os

logger = logging.getLogger(__name__)


class S3BackupManager:
    """
    PostgreSQL ë°±ì—…ì„ S3ì— ì €ì¥

    Backup Types:
    - Daily: ë§¤ì¼ ìì •
    - Weekly: ë§¤ì£¼ ì¼ìš”ì¼
    - Monthly: ë§¤ì›” 1ì¼
    """

    def __init__(
        self,
        bucket_name: str = 'ai-trading-backups',
        region: str = 'us-east-1'
    ):
        self.bucket_name = bucket_name
        self.region = region

        # AWS í´ë¼ì´ì–¸íŠ¸
        self.s3 = boto3.client('s3', region_name=region)

        # ë²„í‚· ì¡´ì¬ í™•ì¸ (ì—†ìœ¼ë©´ ìƒì„±)
        self._ensure_bucket_exists()

    def _ensure_bucket_exists(self):
        """S3 ë²„í‚· ìƒì„± (ì—†ìœ¼ë©´)"""
        try:
            self.s3.head_bucket(Bucket=self.bucket_name)
            logger.info(f"âœ… S3 bucket exists: {self.bucket_name}")
        except:
            logger.info(f"Creating S3 bucket: {self.bucket_name}")
            self.s3.create_bucket(
                Bucket=self.bucket_name,
                CreateBucketConfiguration={'LocationConstraint': self.region}
            )

            # Lifecycle policy (90ì¼ í›„ Glacierë¡œ ì´ë™)
            self.s3.put_bucket_lifecycle_configuration(
                Bucket=self.bucket_name,
                LifecycleConfiguration={
                    'Rules': [
                        {
                            'Id': 'MoveToGlacier',
                            'Status': 'Enabled',
                            'Transitions': [
                                {'Days': 90, 'StorageClass': 'GLACIER'}
                            ],
                            'Expiration': {'Days': 365}
                        }
                    ]
                }
            )

    def backup_database(self, backup_type: str = 'daily') -> str:
        """
        PostgreSQL ì „ì²´ ë°±ì—…

        Args:
            backup_type: daily|weekly|monthly

        Returns:
            S3 key (ê²½ë¡œ)
        """
        from backend.config import settings

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = f"/tmp/db_backup_{timestamp}.sql"

        # pg_dump ì‹¤í–‰
        db_url = settings.DATABASE_URL
        # postgresql://user:pass@host:port/dbname íŒŒì‹±
        import re
        match = re.match(r'postgresql://([^:]+):([^@]+)@([^:]+):(\d+)/(.+)', db_url)
        if not match:
            raise ValueError("Invalid DATABASE_URL format")

        user, password, host, port, dbname = match.groups()

        cmd = f"PGPASSWORD={password} pg_dump -h {host} -p {port} -U {user} -d {dbname} -F c -f {backup_file}"
        os.system(cmd)

        # Gzip ì••ì¶•
        gzip_file = f"{backup_file}.gz"
        with open(backup_file, 'rb') as f_in:
            with gzip.open(gzip_file, 'wb') as f_out:
                f_out.writelines(f_in)

        # S3 ì—…ë¡œë“œ
        s3_key = f"backups/{backup_type}/{timestamp}/database.sql.gz"

        self.s3.upload_file(
            gzip_file,
            self.bucket_name,
            s3_key,
            ExtraArgs={'StorageClass': 'STANDARD_IA'}  # Infrequent Access
        )

        logger.info(f"âœ… Backup uploaded: s3://{self.bucket_name}/{s3_key}")

        # ë¡œì»¬ íŒŒì¼ ì‚­ì œ
        os.remove(backup_file)
        os.remove(gzip_file)

        return s3_key

    def backup_files(self, directory: Path, backup_type: str = 'daily') -> str:
        """
        íŒŒì¼ ë””ë ‰í† ë¦¬ ë°±ì—… (docs/, logs/ ë“±)

        Returns:
            S3 key prefix
        """
        import tarfile

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        tar_file = f"/tmp/{directory.name}_{timestamp}.tar.gz"

        # Tar ì••ì¶•
        with tarfile.open(tar_file, 'w:gz') as tar:
            tar.add(directory, arcname=directory.name)

        # S3 ì—…ë¡œë“œ
        s3_key = f"backups/{backup_type}/{timestamp}/{directory.name}.tar.gz"

        self.s3.upload_file(tar_file, self.bucket_name, s3_key)

        logger.info(f"âœ… Files uploaded: s3://{self.bucket_name}/{s3_key}")

        os.remove(tar_file)

        return s3_key

    def list_backups(self, backup_type: Optional[str] = None) -> list:
        """ë°±ì—… ëª©ë¡ ì¡°íšŒ"""
        prefix = f"backups/{backup_type}/" if backup_type else "backups/"

        response = self.s3.list_objects_v2(
            Bucket=self.bucket_name,
            Prefix=prefix
        )

        backups = []
        for obj in response.get('Contents', []):
            backups.append({
                'key': obj['Key'],
                'size': obj['Size'],
                'last_modified': obj['LastModified']
            })

        return backups

    def restore_database(self, s3_key: str, target_db: str = 'ai_trading_restored'):
        """
        S3 ë°±ì—…ì—ì„œ DB ë³µì›

        Args:
            s3_key: S3 ê°ì²´ í‚¤
            target_db: ë³µì›í•  ë°ì´í„°ë² ì´ìŠ¤ëª…
        """
        # S3 ë‹¤ìš´ë¡œë“œ
        local_file = "/tmp/restore.sql.gz"
        self.s3.download_file(self.bucket_name, s3_key, local_file)

        # ì••ì¶• í•´ì œ
        sql_file = "/tmp/restore.sql"
        with gzip.open(local_file, 'rb') as f_in:
            with open(sql_file, 'wb') as f_out:
                f_out.write(f_in.read())

        # pg_restore ì‹¤í–‰
        from backend.config import settings
        # ... (ìœ„ì™€ ë™ì¼í•œ íŒŒì‹±)

        cmd = f"PGPASSWORD={password} pg_restore -h {host} -p {port} -U {user} -d {target_db} -c {sql_file}"
        os.system(cmd)

        logger.info(f"âœ… Database restored to: {target_db}")

        os.remove(local_file)
        os.remove(sql_file)


# ìŠ¤ì¼€ì¤„ëŸ¬
def schedule_daily_backup():
    """ì¼ì¼ ë°±ì—… (Cron ë˜ëŠ” APScheduler)"""
    manager = S3BackupManager()

    # DB ë°±ì—…
    manager.backup_database('daily')

    # Docs ë°±ì—…
    manager.backup_files(Path('docs'), 'daily')

    # Logs ë°±ì—… (7ì¼ì¹˜ë§Œ)
    manager.backup_files(Path('logs'), 'daily')
```

**Cron ì„¤ì •:**
```bash
# crontab -e
0 0 * * * cd /opt/ai-trading-system && python -c "from backend.cloud.s3_backup import schedule_daily_backup; schedule_daily_backup()"
```

**ì˜ˆìƒ íš¨ê³¼:**
- âœ… ì¬í•´ ë³µêµ¬ ê°€ëŠ¥
- âœ… ìë™ ë°±ì—… (ë§¤ì¼)
- âœ… ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ìµœì í™” (Glacier)
- ğŸ’° ì›” ë¹„ìš©: ~$5-10 (100GB ë°±ì—… ê¸°ì¤€)

---

#### 5.1.2 Lambda ë°ì´í„° ë°±í•„ ìë™í™”

**ëª©í‘œ:**
- ì£¼ê°„ ìë™ ë°±í•„ (Yahoo Finance)
- ì„œë²„ë¦¬ìŠ¤ ì‹¤í–‰ (ë¹„ìš© ì ˆê°)

**Lambda í•¨ìˆ˜:** `lambda/backfill_weekly.py`

```python
"""
AWS Lambda - ì£¼ê°„ ë°ì´í„° ë°±í•„

Trigger: CloudWatch Events (ë§¤ì£¼ ì¼ìš”ì¼ ì˜¤ì „ 2ì‹œ)
"""

import json
import boto3
import requests
from datetime import datetime, timedelta

def lambda_handler(event, context):
    """
    ì£¼ê°„ ë°ì´í„° ë°±í•„ ì‹¤í–‰

    1. ì „ì£¼ ì›”~ê¸ˆ ì£¼ê°€ ë°ì´í„° ë°±í•„
    2. ì™„ë£Œ í›„ Telegram ì•Œë¦¼
    """

    # ë°±í•„ ëŒ€ìƒ ê¸°ê°„
    today = datetime.now()
    last_monday = today - timedelta(days=today.weekday() + 7)
    last_friday = last_monday + timedelta(days=4)

    start_date = last_monday.strftime('%Y-%m-%d')
    end_date = last_friday.strftime('%Y-%m-%d')

    # Tickers (S&P 500 ìƒìœ„ 50ê°œ)
    tickers = [
        'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'TSLA', 'META',
        # ... (ìƒëµ)
    ]

    # API í˜¸ì¶œ (EC2 ë°±ì—”ë“œ)
    api_url = "https://api.ai-trading.com/api/backfill/prices"

    response = requests.post(api_url, json={
        'tickers': tickers,
        'start_date': start_date,
        'end_date': end_date,
        'interval': '1d'
    })

    if response.status_code == 200:
        job_id = response.json()['job_id']

        # Telegram ì•Œë¦¼
        send_telegram_notification(
            f"âœ… Weekly backfill started\n"
            f"Period: {start_date} ~ {end_date}\n"
            f"Job ID: {job_id}"
        )

        return {
            'statusCode': 200,
            'body': json.dumps({'job_id': job_id})
        }
    else:
        return {
            'statusCode': 500,
            'body': 'Backfill failed'
        }


def send_telegram_notification(message: str):
    """Telegram ì•Œë¦¼"""
    import os

    bot_token = os.environ['TELEGRAM_BOT_TOKEN']
    chat_id = os.environ['TELEGRAM_CHAT_ID']

    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    requests.post(url, json={'chat_id': chat_id, 'text': message})
```

**ë°°í¬:**
```bash
# Lambda í•¨ìˆ˜ ìƒì„±
aws lambda create-function \
  --function-name ai-trading-weekly-backfill \
  --runtime python3.11 \
  --role arn:aws:iam::ACCOUNT_ID:role/lambda-execution-role \
  --handler backfill_weekly.lambda_handler \
  --zip-file fileb://function.zip \
  --timeout 300 \
  --environment Variables="{TELEGRAM_BOT_TOKEN=xxx,TELEGRAM_CHAT_ID=yyy}"

# CloudWatch Events íŠ¸ë¦¬ê±°
aws events put-rule \
  --name weekly-backfill \
  --schedule-expression "cron(0 2 ? * SUN *)"  # ë§¤ì£¼ ì¼ìš”ì¼ ì˜¤ì „ 2ì‹œ

aws events put-targets \
  --rule weekly-backfill \
  --targets "Id"="1","Arn"="arn:aws:lambda:us-east-1:ACCOUNT_ID:function:ai-trading-weekly-backfill"
```

**ì˜ˆìƒ íš¨ê³¼:**
- âœ… ìˆ˜ë™ ë°±í•„ ì œê±°
- âœ… ì„œë²„ë¦¬ìŠ¤ ì‹¤í–‰ (EC2 ë¶€í•˜ ì—†ìŒ)
- ğŸ’° Lambda ë¹„ìš©: ~$0.20/ì›”

---

## Part 6: Communication & Notifications - Discord/Slack Integration

### Component 6.1: Discord Notifications

**í˜„ì¬ ìƒíƒœ:**
- Telegramë§Œ ì§€ì›
- Discord ë¯¸ì—°ë™

**ëª©í‘œ:**
- Discord Webhook ì•Œë¦¼
- Embed í˜•ì‹ ë©”ì‹œì§€
- ì±„ë„ë³„ ë¶„ë¥˜ (ê±°ë˜, ì•Œë¦¼, ì—ëŸ¬)

**êµ¬í˜„:**

**íŒŒì¼:** `backend/notifications/discord_notifier.py` (ì‹ ê·œ)

```python
"""
Discord Webhook ì•Œë¦¼

Date: 2026-01-03
Component: Discord/Slack Integration
"""

import requests
from typing import Dict, Optional
import logging

logger = logging.getLogger(__name__)


class DiscordNotifier:
    """
    Discord Webhook ì•Œë¦¼ í´ë˜ìŠ¤

    Channels:
    - #trading: ê±°ë˜ ì‹ í˜¸ ë° ì‹¤í–‰
    - #alerts: ì¤‘ìš” ì•Œë¦¼ (Kill Switch ë“±)
    - #errors: ì‹œìŠ¤í…œ ì—ëŸ¬
    """

    def __init__(
        self,
        webhook_url_trading: str,
        webhook_url_alerts: str,
        webhook_url_errors: str
    ):
        self.webhooks = {
            'trading': webhook_url_trading,
            'alerts': webhook_url_alerts,
            'errors': webhook_url_errors
        }

    def send_embed(
        self,
        channel: str,
        title: str,
        description: str,
        color: int = 0x00ff00,  # ë…¹ìƒ‰
        fields: Optional[list] = None
    ):
        """
        Discord Embed ë©”ì‹œì§€ ì „ì†¡

        Args:
            channel: trading|alerts|errors
            title: ì œëª©
            description: ë³¸ë¬¸
            color: RGB ìƒ‰ìƒ (hex)
            fields: [{name, value, inline}] ë¦¬ìŠ¤íŠ¸
        """
        webhook_url = self.webhooks.get(channel)
        if not webhook_url:
            logger.error(f"Unknown channel: {channel}")
            return

        embed = {
            'title': title,
            'description': description,
            'color': color,
            'timestamp': datetime.utcnow().isoformat(),
            'footer': {'text': 'AI Trading System'}
        }

        if fields:
            embed['fields'] = fields

        payload = {'embeds': [embed]}

        try:
            response = requests.post(webhook_url, json=payload)
            response.raise_for_status()
            logger.info(f"âœ… Discord notification sent: {channel}")
        except Exception as e:
            logger.error(f"Discord notification failed: {e}")

    def send_trading_signal(self, signal: Dict):
        """ê±°ë˜ ì‹ í˜¸ ì•Œë¦¼"""
        self.send_embed(
            channel='trading',
            title=f"ğŸ”” Trading Signal: {signal['ticker']}",
            description=f"Action: **{signal['action']}**",
            color=0x00ff00 if signal['action'] == 'BUY' else 0xff0000,
            fields=[
                {'name': 'Confidence', 'value': f"{signal['confidence']:.1%}", 'inline': True},
                {'name': 'Price', 'value': f"${signal['price']:.2f}", 'inline': True},
                {'name': 'Reasoning', 'value': signal['reasoning'][:200], 'inline': False}
            ]
        )

    def send_kill_switch_alert(self, reason: str, details: Dict):
        """Kill Switch ë°œë™ ì•Œë¦¼"""
        self.send_embed(
            channel='alerts',
            title="ğŸš¨ KILL SWITCH ACTIVATED",
            description=f"Reason: **{reason}**",
            color=0xff0000,  # ë¹¨ê°„ìƒ‰
            fields=[
                {'name': 'Daily Loss', 'value': f"{details.get('daily_loss_pct', 0):.2f}%", 'inline': True},
                {'name': 'Threshold', 'value': f"{details.get('threshold_pct', 5):.2f}%", 'inline': True},
                {'name': 'Action', 'value': '**ALL TRADING HALTED**', 'inline': False}
            ]
        )

    def send_error(self, error_type: str, message: str):
        """ì‹œìŠ¤í…œ ì—ëŸ¬ ì•Œë¦¼"""
        self.send_embed(
            channel='errors',
            title=f"âŒ Error: {error_type}",
            description=message,
            color=0xffa500  # ì£¼í™©ìƒ‰
        )
```

**í™˜ê²½ ë³€ìˆ˜ (.env):**
```bash
DISCORD_WEBHOOK_TRADING=https://discord.com/api/webhooks/xxx/yyy
DISCORD_WEBHOOK_ALERTS=https://discord.com/api/webhooks/xxx/zzz
DISCORD_WEBHOOK_ERRORS=https://discord.com/api/webhooks/xxx/www
```

**ì‚¬ìš© ì˜ˆì‹œ:**

```python
from backend.notifications.discord_notifier import DiscordNotifier
from backend.config import settings

discord = DiscordNotifier(
    webhook_url_trading=settings.DISCORD_WEBHOOK_TRADING,
    webhook_url_alerts=settings.DISCORD_WEBHOOK_ALERTS,
    webhook_url_errors=settings.DISCORD_WEBHOOK_ERRORS
)

# ê±°ë˜ ì‹ í˜¸
discord.send_trading_signal({
    'ticker': 'AAPL',
    'action': 'BUY',
    'confidence': 0.85,
    'price': 150.00,
    'reasoning': 'Strong earnings beat, positive momentum'
})

# Kill Switch
discord.send_kill_switch_alert('daily_loss', {
    'daily_loss_pct': 5.2,
    'threshold_pct': 5.0
})

# ì—ëŸ¬
discord.send_error('API_ERROR', 'Yahoo Finance API timeout')
```

**ì˜ˆìƒ íš¨ê³¼:**
- âœ… ì‹¤ì‹œê°„ ì•Œë¦¼ (Discord ëª¨ë°”ì¼ ì•±)
- âœ… Rich formatting (Embed)
- âœ… ì±„ë„ë³„ ë¶„ë¥˜
- ğŸ’° ë¹„ìš©: $0 (ë¬´ë£Œ)

---

### Component 6.2: Slack Integration (ì„ íƒ ì‚¬í•­)

**êµ¬í˜„:** Discordì™€ ìœ ì‚¬í•œ íŒ¨í„´

```python
class SlackNotifier:
    """Slack Webhook ì•Œë¦¼"""

    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url

    def send_message(self, text: str, blocks: Optional[list] = None):
        """Slack Block Kit ë©”ì‹œì§€"""
        payload = {'text': text}
        if blocks:
            payload['blocks'] = blocks

        requests.post(self.webhook_url, json=payload)
```

---

## Part 7: ì „ì²´ êµ¬í˜„ íƒ€ì„ë¼ì¸

### Month 1: Security & DevOps (Foundation)

**Week 1: Security Hardening**
- [ ] SecretsManager êµ¬í˜„
- [ ] OWASP Top 10 ìŠ¤ìºë„ˆ í†µí•©
- [ ] Pre-commit hooks ì„¤ì •

**Week 2: CI/CD Pipeline**
- [ ] GitHub Actions ì›Œí¬í”Œë¡œìš° ì‘ì„±
- [ ] Docker ë©€í‹° ìŠ¤í…Œì´ì§€ ë¹Œë“œ
- [ ] Codecov í†µí•©

**Week 3: Blue-Green Deployment**
- [ ] Nginx ì„¤ì •
- [ ] ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] Rollback í…ŒìŠ¤íŠ¸

**Week 4: ê²€ì¦ ë° ë¬¸ì„œí™”**
- [ ] ì „ì²´ CI/CD íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
- [ ] ë³´ì•ˆ ê°ì‚¬ ì‹¤í–‰
- [ ] ë¬¸ì„œ ì‘ì„±

---

### Month 2: Performance & Analytics

**Week 1: War Room MVP ë³‘ë ¬í™”**
- [ ] ThreadPoolExecutor ì ìš©
- [ ] ì„±ëŠ¥ ì¸¡ì • (8s â†’ 3s)
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ê°œì„ 

**Week 2: Memory Optimization**
- [ ] Generator íŒ¨í„´ ì ìš©
- [ ] ë°°ì¹˜ ì²˜ë¦¬ êµ¬í˜„
- [ ] ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§

**Week 3: Shadow Trading Analytics**
- [ ] ShadowTradingAnalyzer êµ¬í˜„
- [ ] Sharpe/Sortino/Calmar ê³„ì‚°
- [ ] í†µê³„ì  ìœ ì˜ì„± í…ŒìŠ¤íŠ¸

**Week 4: Performance Monitoring**
- [ ] PerformanceMonitor ë°ì½”ë ˆì´í„°
- [ ] Telegram/Discord ì•Œë¦¼
- [ ] ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

---

### Month 3: NLP & Cloud

**Week 1: Local Embeddings**
- [ ] Sentence Transformers ì„¤ì¹˜
- [ ] LocalEmbedder êµ¬í˜„
- [ ] ê¸°ì¡´ ë‰´ìŠ¤ ì¬ì„ë² ë”©

**Week 2: Ticker Extraction**
- [ ] spaCy NER êµ¬í˜„
- [ ] í‹°ì»¤ ë§¤í•‘ DB êµ¬ì¶•
- [ ] Repository í†µí•©

**Week 3: AWS S3 Backup**
- [ ] S3BackupManager êµ¬í˜„
- [ ] ì¼ì¼ ë°±ì—… ìŠ¤ì¼€ì¤„
- [ ] ë³µì› í…ŒìŠ¤íŠ¸

**Week 4: Lambda Backfill**
- [ ] Lambda í•¨ìˆ˜ ì‘ì„±
- [ ] CloudWatch Events ì„¤ì •
- [ ] ìë™í™” ê²€ì¦

---

### Month 4: Communication & Integration

**Week 1: Discord Integration**
- [ ] DiscordNotifier êµ¬í˜„
- [ ] Webhook ì„¤ì •
- [ ] Embed ë©”ì‹œì§€ í…ŒìŠ¤íŠ¸

**Week 2: FinBERT Sentiment**
- [ ] FinBERT ì„¤ì¹˜
- [ ] Sentiment ë¶„ì„ í†µí•©
- [ ] ê¸°ì¡´ ë‰´ìŠ¤ ì¬ê³„ì‚°

**Week 3: System Integration**
- [ ] ëª¨ë“  ì»´í¬ë„ŒíŠ¸ í†µí•© í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- [ ] End-to-End ê²€ì¦

**Week 4: ë¬¸ì„œí™” ë° ë°°í¬**
- [ ] ì „ì²´ ì‹œìŠ¤í…œ ë¬¸ì„œí™”
- [ ] Production ë°°í¬
- [ ] ëª¨ë‹ˆí„°ë§ ë° íŠœë‹

---

## ì„±ê³µ ê¸°ì¤€ (Success Criteria)

### Security & Compliance
- [ ] ì‹œí¬ë¦¿ ì•”í˜¸í™” 100% (gitì— ë…¸ì¶œ 0ê±´)
- [ ] OWASP Top 10 ìŠ¤ìº” í†µê³¼
- [ ] Pre-commit ê²€ì¦ 100% ì ìš©
- [ ] ë³´ì•ˆ ê°ì‚¬ PASS

### DevOps & CI/CD
- [ ] GitHub Actions íŒŒì´í”„ë¼ì¸ 100% ì„±ê³µ
- [ ] ë°°í¬ ë‹¤ìš´íƒ€ì„ < 1ë¶„
- [ ] Rollback ì‹œê°„ < 5ë¶„
- [ ] í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ > 90%

### Performance
- [ ] War Room MVP: 8.2s â†’ 3s ì´í•˜
- [ ] News ingestion: 200ms â†’ 50ms
- [ ] Shadow Trading ë¶„ì„: < 1s
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ < 2GB

### Analytics
- [ ] Sharpe ratio ìë™ ê³„ì‚°
- [ ] í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
- [ ] ì£¼ê°„ ë¦¬í¬íŠ¸ ìë™ ìƒì„±
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

### NLP
- [ ] ì„ë² ë”© ë¹„ìš©: $30/ì›” â†’ $0
- [ ] í‹°ì»¤ ì¶”ì¶œ ì •í™•ë„: 85% â†’ 95%+
- [ ] Sentiment ì •í™•ë„: 70% â†’ 85%+
- [ ] NER íšŒì‚¬ëª… ë§¤í•‘ 500+ í‹°ì»¤

### Cloud & Infrastructure
- [ ] S3 ë°±ì—… ìë™í™” (ì¼ì¼)
- [ ] Lambda ë°±í•„ ìë™í™” (ì£¼ê°„)
- [ ] ì¬í•´ ë³µêµ¬ ì‹œê°„ < 4ì‹œê°„
- [ ] ìŠ¤í† ë¦¬ì§€ ë¹„ìš© < $10/ì›”

### Communication
- [ ] Discord/Telegram ì•Œë¦¼ 100% ì „ë‹¬
- [ ] Embed ë©”ì‹œì§€ Rich formatting
- [ ] ì±„ë„ë³„ ë¶„ë¥˜ ì •í™•ë„ 100%
- [ ] ì•Œë¦¼ ì§€ì—° < 5ì´ˆ

---

## ë¹„ìš© ë¶„ì„ (Cost Analysis)

### ì›”ê°„ ìš´ì˜ ë¹„ìš©

| í•­ëª© | í˜„ì¬ | êµ¬í˜„ í›„ | ì ˆê° |
|------|------|---------|------|
| OpenAI Embeddings | $30 | $0 | -$30 |
| AWS S3 (100GB) | $0 | $5 | +$5 |
| AWS Lambda | $0 | $0.20 | +$0.20 |
| Discord/Slack | $0 | $0 | $0 |
| **Total** | **$30** | **$5.20** | **-$24.80** |

**ì—°ê°„ ì ˆê°: $297.60**

---

## ë¦¬ìŠ¤í¬ ë° ì™„í™” ì „ëµ (Risk Mitigation)

### ê¸°ìˆ ì  ë¦¬ìŠ¤í¬

**1. ë¡œì»¬ ì„ë² ë”© í’ˆì§ˆ ì €í•˜**
- **ë¦¬ìŠ¤í¬**: OpenAIë³´ë‹¤ í’ˆì§ˆ ë‚®ì„ ìˆ˜ ìˆìŒ
- **ì™„í™”ì±…**: A/B í…ŒìŠ¤íŠ¸, ìœ ì‚¬ë„ ê²€ì¦, í•„ìš”ì‹œ all-mpnet-base-v2 (ë” í° ëª¨ë¸) ì‚¬ìš©

**2. AWS ë¹„ìš© ì´ˆê³¼**
- **ë¦¬ìŠ¤í¬**: S3 ìŠ¤í† ë¦¬ì§€ ì˜ˆìƒë³´ë‹¤ ë§ì„ ìˆ˜ ìˆìŒ
- **ì™„í™”ì±…**: Lifecycle policy (90ì¼ í›„ Glacier), ì••ì¶•, ë¶ˆí•„ìš”í•œ ë°±ì—… ì œê±°

**3. Lambda Cold Start**
- **ë¦¬ìŠ¤í¬**: ì²« ì‹¤í–‰ ì‹œ ì§€ì—°
- **ì™„í™”ì±…**: Provisioned Concurrency, CloudWatch ì˜ˆì—´ íŠ¸ë¦¬ê±°

**4. CI/CD íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨**
- **ë¦¬ìŠ¤í¬**: ë°°í¬ ì¤‘ ì—ëŸ¬
- **ì™„í™”ì±…**: Blue-Green ë°°í¬, ìë™ ë¡¤ë°±, ì¶©ë¶„í•œ í…ŒìŠ¤íŠ¸

### ìš´ì˜ ë¦¬ìŠ¤í¬

**1. ë°±ì—… ë³µì› ì‹¤íŒ¨**
- **ë¦¬ìŠ¤í¬**: ì¬í•´ ë³µêµ¬ ì‹œ ë°±ì—… ì†ìƒ
- **ì™„í™”ì±…**: ì£¼ê°„ ë³µì› í…ŒìŠ¤íŠ¸, ë‹¤ì¤‘ ë°±ì—… (S3 + ë¡œì»¬)

**2. ì•Œë¦¼ ëˆ„ë½**
- **ë¦¬ìŠ¤í¬**: Discord/Telegram Webhook ì‹¤íŒ¨
- **ì™„í™”ì±…**: ì¬ì‹œë„ ë¡œì§, ë‹¤ì¤‘ ì±„ë„, ë¡œê·¸ ê¸°ë¡

**3. ì„±ëŠ¥ íšŒê·€**
- **ë¦¬ìŠ¤í¬**: ìµœì í™” í›„ ì˜ˆìƒì¹˜ ëª»í•œ ë³‘ëª©
- **ì™„í™”ì±…**: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, ë²¤ì¹˜ë§ˆí¬, ë‹¨ê³„ì  ë°°í¬

### ë¡¤ë°± ì „ëµ

**ì¦‰ì‹œ ë¡¤ë°± (< 5ë¶„):**
```bash
# Blue-Green ë°°í¬ ë¡¤ë°±
sudo systemctl stop ai-trading-green
sudo systemctl start ai-trading-blue
sudo nginx -s reload

# ê¸°ì¡´ ë²„ì „ í™œì„±í™”
git checkout <previous-commit>
docker-compose restart
```

**ë°ì´í„° ë¡¤ë°± (< 30ë¶„):**
```bash
# S3 ë°±ì—…ì—ì„œ DB ë³µì›
python -c "
from backend.cloud.s3_backup import S3BackupManager
manager = S3BackupManager()
manager.restore_database('backups/daily/20260102_000000/database.sql.gz')
"
```

---

## ê´€ë ¨ ë¬¸ì„œ (Related Documents)

1. **[260102_Claude_Code_Templates_Review.md](260102_Claude_Code_Templates_Review.md)** - Claude Code Templates ì „ì²´ ë¦¬ë·°
2. **[260103_Claude_Code_Templates_Implementation_Plan.md](260103_Claude_Code_Templates_Implementation_Plan.md)** - í…ŒìŠ¤íŠ¸ ìë™í™”, í”„ë¡ íŠ¸ì—”ë“œ ìµœì í™”, Git Hooks ê³„íš
3. **[260102_Database_Optimization_Plan.md](260102_Database_Optimization_Plan.md)** - Database Architect Agent ê³„íš
4. **[260103_Remaining_Components_Implementation_Plan.md](260103_Remaining_Components_Implementation_Plan.md)** - 13ê°œ ë‚¨ì€ ì»´í¬ë„ŒíŠ¸ ê°œìš”
5. **[Work_Log_20260102.md](Work_Log_20260102.md)** - DB ìµœì í™” Phase 1, Kill Switch êµ¬í˜„ ì™„ë£Œ ê¸°ë¡

---

## ë©”íƒ€ë°ì´í„° (Metadata)

**ì‘ì„±ì¼**: 2026-01-03
**ì‘ì„±ì**: AI Trading System Development Team
**ë¬¸ì„œ ë²„ì „**: 1.0
**ìƒíƒœ**: ğŸ“‹ Plan Complete - Ready for Implementation
**ìš°ì„ ìˆœìœ„**: P2 (Medium-High - Advanced Features)
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 4ê°œì›” (Month 1-4)
**ì˜ˆìƒ ë¹„ìš© ì ˆê°**: $297.60/ë…„

**í•µì‹¬ ì»´í¬ë„ŒíŠ¸ (10ê°œ):**
1. âœ… Security Auditor Agent - ì‹œí¬ë¦¿ ì•”í˜¸í™”, OWASP ìŠ¤ìº”
2. âœ… DevOps Engineer Agent - CI/CD íŒŒì´í”„ë¼ì¸, Blue-Green ë°°í¬
3. âœ… Performance Optimizer - War Room MVP ë³‘ë ¬í™”, ë©”ëª¨ë¦¬ ìµœì í™”
4. âœ… Data Scientist Agent - Shadow Trading í†µê³„ ë¶„ì„ (Sharpe, Sortino, Calmar)
5. âœ… NLP Engineer Agent - ë¡œì»¬ ì„ë² ë”©, í‹°ì»¤ ì¶”ì¶œ, FinBERT Sentiment
6. âœ… AWS Integration MCP - S3 ë°±ì—…, Lambda ë°±í•„
7. âœ… Discord/Slack Notifications - Webhook ì•Œë¦¼, Embed ë©”ì‹œì§€
8. âœ… Performance Monitor Hook - ìë™ ì„±ëŠ¥ ì¶”ì , ì•Œë¦¼
9. âœ… /check-security Command - ë³´ì•ˆ ìŠ¤ìº” ìë™í™”
10. âœ… /setup-ci-cd-pipeline Command - CI/CD ìë™ êµ¬ì„±

**ë‹¤ìŒ ë‹¨ê³„:**
1. ì‚¬ìš©ì ìŠ¹ì¸ ëŒ€ê¸°
2. Month 1ë¶€í„° ë‹¨ê³„ì  êµ¬í˜„
3. ì£¼ê°„ ì§„í–‰ ìƒí™© ë¦¬í¬íŠ¸
4. Production ë°°í¬ í›„ ëª¨ë‹ˆí„°ë§

---

**END OF DOCUMENT**