# Claude Code Templates ë‚¨ì€ ì»´í¬ë„ŒíŠ¸ êµ¬í˜„ ê³„íš

**ì‘ì„±ì¼**: 2026-01-03
**ê¸°ì¤€**: 2026-01-02 ì‘ì—… ì™„ë£Œ í›„
**ìš°ì„ ìˆœìœ„**: P2-P3 (Medium to Low Priority)
**ìƒíƒœ**: ğŸ“‹ Ready for Planning

---

## Executive Summary

Claude Code Templates ì¤‘ ì•„ì§ êµ¬í˜„ ê³„íšì„ ì„¸ìš°ì§€ ì•Šì€ **13ê°œ ì»´í¬ë„ŒíŠ¸**ì— ëŒ€í•œ í†µí•© êµ¬í˜„ ê³„íšì…ë‹ˆë‹¤.

**ì´ë¯¸ ê³„íš ì™„ë£Œëœ ì»´í¬ë„ŒíŠ¸ (ë³„ë„ ë¬¸ì„œ):**
- âœ… `/generate-tests` Command - [260103_Claude_Code_Templates_Implementation_Plan.md](260103_Claude_Code_Templates_Implementation_Plan.md)
- âœ… React Performance Optimizer Agent - ìƒë™
- âœ… Auto Git Hooks - ìƒë™
- âœ… Database Architect Agent - [260102_Database_Optimization_Plan.md](260102_Database_Optimization_Plan.md)

**ë‚¨ì€ ì»´í¬ë„ŒíŠ¸ (ë³¸ ë¬¸ì„œ):**
- **High Priority (6ê°œ)**: Security Auditor, DevOps Engineer, 4ê°œ Commands, 2ê°œ MCPs, 2ê°œ Settings, 2ê°œ Hooks
- **Medium Priority (7ê°œ)**: Data Scientist, NLP Engineer, 2ê°œ MCPs, 2ê°œ Skills

---

## í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ (2026-01-03 ê¸°ì¤€)

### ë³´ì•ˆ í˜„í™©
- âŒ API í‚¤ ê´€ë¦¬: .env íŒŒì¼ì— í‰ë¬¸ ì €ì¥
- âŒ ë³´ì•ˆ ìŠ¤ìº”: ì •ê¸°ì  ê°ì‚¬ ì—†ìŒ
- âŒ OWASP Top 10: ë¯¸ê²€ì¦
- âš ï¸ OpenAI API í• ë‹¹ëŸ‰ ì´ˆê³¼ ë°œìƒ (2026-01-02 ì´ì „)
- âœ… Kill Switch ì‹œìŠ¤í…œ êµ¬í˜„ ì™„ë£Œ (2026-01-02)

### DevOps í˜„í™©
- âŒ CI/CD: GitHub Actions ê¸°ë³¸ë§Œ êµ¬ì„± (í…ŒìŠ¤íŠ¸ ë¯¸ì‹¤í–‰)
- âŒ ìë™ ë°°í¬: ì—†ìŒ
- âŒ ëª¨ë‹ˆí„°ë§: ìˆ˜ë™ í™•ì¸ë§Œ ê°€ëŠ¥
- âœ… Docker Compose: êµ¬ì„± ì™„ë£Œ
- âœ… Shadow Trading ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸ (2026-01-02)

### ë°ì´í„° ë¶„ì„ í˜„í™©
- âš ï¸ Shadow Trading ë¶„ì„: ìˆ˜ë™ ìŠ¤í¬ë¦½íŠ¸ë§Œ ì¡´ì¬
- âŒ ë°±í…ŒìŠ¤íŒ… í†µê³„: ê¸°ë³¸ ë©”íŠ¸ë¦­ë§Œ (Win Rate, PF, MDD)
- âŒ Agent ì„±ê³¼ ë¶„ì„: ìë™í™” ì—†ìŒ
- âœ… War Room MVP: 12.76ì´ˆ ì‘ë‹µ (ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±)

### NLP/AI í˜„í™©
- âš ï¸ ë‰´ìŠ¤ ê°ì„± ë¶„ì„: Gemini API ê¸°ë°˜ (í• ë‹¹ëŸ‰ ì œí•œ)
- âŒ í‹°ì»¤ ì¶”ì¶œ: ì •í™•ë„ 60% ì¶”ì •
- âŒ ë¡œì»¬ ì„ë² ë”©: ë¯¸êµ¬í˜„ (OpenAI Embedding ì˜ì¡´)
- âœ… News Aggregation ì •ìƒ ì‘ë™

---

## Component Group 1: Security & Compliance (ë³´ì•ˆ ë° ê·œì • ì¤€ìˆ˜)

### 1.1 Security Auditor Agent

**ëª©í‘œ**: ìë™í™”ëœ ë³´ì•ˆ ê°ì‚¬ ì‹œìŠ¤í…œ êµ¬ì¶•

**ì„¤ì¹˜ ë°©ë²•**:
```bash
npx claude-code-templates@latest --agent security-auditor --yes
```

**ì ìš© ì „ëµ**:

#### Phase 1A: API í‚¤ ë³´ì•ˆ ê°•í™”

**í˜„ì¬ ë¬¸ì œ**:
```python
# .env íŒŒì¼ (í‰ë¬¸ ì €ì¥)
OPENAI_API_KEY=sk-proj-xxxxx
GEMINI_API_KEY=AIzaSyxxxxx
KIS_APP_KEY=PSxxxxx
KIS_APP_SECRET=xxxxx
DATABASE_URL=postgresql://user:password@localhost:5433/trading
```

**í•´ê²° ë°©ë²•**:

**1. Secrets ì•”í˜¸í™”**

**íŒŒì¼**: `backend/config/secrets_manager.py` (ì‹ ê·œ)

```python
"""
Secrets Manager - í™˜ê²½ ë³€ìˆ˜ ì•”í˜¸í™” ì €ì¥

Date: 2026-01-03
Phase: Security Enhancement
"""
import os
from cryptography.fernet import Fernet
from pathlib import Path
import json

class SecretsManager:
    """ì•”í˜¸í™”ëœ ì‹œí¬ë¦¿ ê´€ë¦¬"""

    def __init__(self, key_file: str = ".secrets.key"):
        self.key_file = Path(key_file)
        self.secrets_file = Path(".secrets.enc")
        self.key = self._load_or_create_key()
        self.fernet = Fernet(self.key)

    def _load_or_create_key(self) -> bytes:
        """ì•”í˜¸í™” í‚¤ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        if self.key_file.exists():
            return self.key_file.read_bytes()

        # ìƒˆ í‚¤ ìƒì„±
        key = Fernet.generate_key()
        self.key_file.write_bytes(key)
        self.key_file.chmod(0o600)  # ì†Œìœ ìë§Œ ì½ê¸° ê°€ëŠ¥
        return key

    def encrypt_secrets(self, secrets: dict) -> None:
        """ì‹œí¬ë¦¿ ì•”í˜¸í™” ì €ì¥"""
        json_data = json.dumps(secrets).encode()
        encrypted = self.fernet.encrypt(json_data)
        self.secrets_file.write_bytes(encrypted)
        self.secrets_file.chmod(0o600)

    def decrypt_secrets(self) -> dict:
        """ì‹œí¬ë¦¿ ë³µí˜¸í™” ë¡œë“œ"""
        if not self.secrets_file.exists():
            raise FileNotFoundError("Encrypted secrets file not found")

        encrypted = self.secrets_file.read_bytes()
        decrypted = self.fernet.decrypt(encrypted)
        return json.loads(decrypted.decode())

    def get_secret(self, key: str, default=None):
        """ê°œë³„ ì‹œí¬ë¦¿ ì¡°íšŒ"""
        secrets = self.decrypt_secrets()
        return secrets.get(key, default)

# ì‚¬ìš© ì˜ˆì‹œ
secrets_manager = SecretsManager()

# ì´ˆê¸° ì•”í˜¸í™” (í•œ ë²ˆë§Œ ì‹¤í–‰)
secrets_manager.encrypt_secrets({
    "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY"),
    "GEMINI_API_KEY": os.getenv("GEMINI_API_KEY"),
    "KIS_APP_KEY": os.getenv("KIS_APP_KEY"),
    "KIS_APP_SECRET": os.getenv("KIS_APP_SECRET"),
    "DATABASE_URL": os.getenv("DATABASE_URL")
})

# ëŸ°íƒ€ì„ ì‚¬ìš©
openai_key = secrets_manager.get_secret("OPENAI_API_KEY")
```

**2. .env íŒŒì¼ ì œê±° ë° ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸**

**íŒŒì¼**: `scripts/migrate_secrets.py` (ì‹ ê·œ)

```python
#!/usr/bin/env python3
"""
.env â†’ ì•”í˜¸í™”ëœ secrets ë§ˆì´ê·¸ë ˆì´ì…˜

Usage:
    python scripts/migrate_secrets.py
"""
from dotenv import load_dotenv
import os
from backend.config.secrets_manager import SecretsManager

def migrate():
    # ê¸°ì¡´ .env ë¡œë“œ
    load_dotenv()

    secrets = {
        "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY"),
        "GEMINI_API_KEY": os.getenv("GEMINI_API_KEY"),
        "KIS_APP_KEY": os.getenv("KIS_APP_KEY"),
        "KIS_APP_SECRET": os.getenv("KIS_APP_SECRET"),
        "DATABASE_URL": os.getenv("DATABASE_URL"),
        "TELEGRAM_BOT_TOKEN": os.getenv("TELEGRAM_BOT_TOKEN"),
        "TELEGRAM_CHAT_ID": os.getenv("TELEGRAM_CHAT_ID")
    }

    # ì•”í˜¸í™” ì €ì¥
    manager = SecretsManager()
    manager.encrypt_secrets(secrets)

    print("âœ… Secrets encrypted successfully")
    print("âš ï¸  Please backup .secrets.key securely")
    print("âš ï¸  Add .secrets.key to .gitignore")
    print("âš ï¸  Remove .env file after verification")

if __name__ == "__main__":
    migrate()
```

**3. .gitignore ì—…ë°ì´íŠ¸**

```bash
# .gitignore
.env
.secrets.key
.secrets.enc
```

**ì˜ˆìƒ íš¨ê³¼**: API í‚¤ ë…¸ì¶œ ìœ„í—˜ 100% ì œê±°

---

#### Phase 1B: OWASP Top 10 ìë™ ìŠ¤ìº”

**íŒŒì¼**: `scripts/security_audit.py` (ì‹ ê·œ)

```python
#!/usr/bin/env python3
"""
OWASP Top 10 ìë™ ë³´ì•ˆ ìŠ¤ìº”

Checks:
1. SQL Injection
2. XSS (Cross-Site Scripting)
3. Broken Authentication
4. Sensitive Data Exposure
5. XML External Entities (XXE)
6. Broken Access Control
7. Security Misconfiguration
8. Insecure Deserialization
9. Using Components with Known Vulnerabilities
10. Insufficient Logging & Monitoring
"""
import re
from pathlib import Path
from typing import List, Dict

class SecurityAuditor:
    """ë³´ì•ˆ ê°ì‚¬ ë„êµ¬"""

    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.issues = []

    def scan_sql_injection(self) -> List[Dict]:
        """SQL Injection ì·¨ì•½ì  ìŠ¤ìº”"""
        issues = []

        # repository.py ìŠ¤ìº”
        repo_files = self.base_path.glob("backend/database/*.py")

        for file in repo_files:
            content = file.read_text()

            # ìœ„í—˜ íŒ¨í„´: f-string ë˜ëŠ” % í¬ë§·íŒ…
            if re.search(r'f".*SELECT.*{.*}"', content):
                issues.append({
                    "type": "SQL_INJECTION",
                    "severity": "HIGH",
                    "file": str(file),
                    "message": "Potential SQL injection via f-string"
                })

            if re.search(r'%.*SELECT', content):
                issues.append({
                    "type": "SQL_INJECTION",
                    "severity": "HIGH",
                    "file": str(file),
                    "message": "Potential SQL injection via % formatting"
                })

        return issues

    def scan_xss(self) -> List[Dict]:
        """XSS ì·¨ì•½ì  ìŠ¤ìº”"""
        issues = []

        # í”„ë¡ íŠ¸ì—”ë“œ íŒŒì¼ ìŠ¤ìº”
        tsx_files = self.base_path.glob("frontend/src/**/*.tsx")

        for file in tsx_files:
            content = file.read_text()

            # dangerouslySetInnerHTML ì‚¬ìš©
            if "dangerouslySetInnerHTML" in content:
                issues.append({
                    "type": "XSS",
                    "severity": "MEDIUM",
                    "file": str(file),
                    "message": "dangerouslySetInnerHTML detected - verify sanitization"
                })

        return issues

    def scan_secrets_exposure(self) -> List[Dict]:
        """ì‹œí¬ë¦¿ ë…¸ì¶œ ìŠ¤ìº”"""
        issues = []

        # ëª¨ë“  Python íŒŒì¼ ìŠ¤ìº”
        py_files = self.base_path.glob("**/*.py")

        secret_patterns = [
            (r'sk-[a-zA-Z0-9]{48}', 'OpenAI API Key'),
            (r'AIzaSy[a-zA-Z0-9_-]{33}', 'Google API Key'),
            (r'ghp_[a-zA-Z0-9]{36}', 'GitHub Token'),
            (r'postgresql://[^:]+:[^@]+@', 'Database Password in URL')
        ]

        for file in py_files:
            if '.venv' in str(file) or 'node_modules' in str(file):
                continue

            content = file.read_text()

            for pattern, secret_type in secret_patterns:
                if re.search(pattern, content):
                    issues.append({
                        "type": "SECRET_EXPOSURE",
                        "severity": "CRITICAL",
                        "file": str(file),
                        "message": f"Potential {secret_type} hardcoded"
                    })

        return issues

    def scan_broken_access_control(self) -> List[Dict]:
        """ì ‘ê·¼ ì œì–´ ì·¨ì•½ì  ìŠ¤ìº”"""
        issues = []

        # API ë¼ìš°í„° ìŠ¤ìº”
        router_files = self.base_path.glob("backend/api/*_router.py")

        for file in router_files:
            content = file.read_text()

            # DELETE ì—”ë“œí¬ì¸íŠ¸ì— ì¸ì¦ ì—†ìŒ
            if re.search(r'@router\.delete\(.*\)\s+async def', content):
                if 'Depends(get_current_user)' not in content:
                    issues.append({
                        "type": "BROKEN_ACCESS_CONTROL",
                        "severity": "HIGH",
                        "file": str(file),
                        "message": "DELETE endpoint without authentication"
                    })

        return issues

    def run_full_audit(self) -> Dict:
        """ì „ì²´ ë³´ì•ˆ ê°ì‚¬ ì‹¤í–‰"""
        all_issues = []

        all_issues.extend(self.scan_sql_injection())
        all_issues.extend(self.scan_xss())
        all_issues.extend(self.scan_secrets_exposure())
        all_issues.extend(self.scan_broken_access_control())

        # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
        critical = [i for i in all_issues if i['severity'] == 'CRITICAL']
        high = [i for i in all_issues if i['severity'] == 'HIGH']
        medium = [i for i in all_issues if i['severity'] == 'MEDIUM']

        return {
            "total_issues": len(all_issues),
            "critical": len(critical),
            "high": len(high),
            "medium": len(medium),
            "issues": all_issues
        }

# ì‚¬ìš©
if __name__ == "__main__":
    auditor = SecurityAuditor()
    results = auditor.run_full_audit()

    print(f"ğŸ” Security Audit Results")
    print(f"Total Issues: {results['total_issues']}")
    print(f"  Critical: {results['critical']}")
    print(f"  High: {results['high']}")
    print(f"  Medium: {results['medium']}")

    if results['critical'] > 0:
        print("\nâŒ CRITICAL ISSUES FOUND:")
        for issue in results['issues']:
            if issue['severity'] == 'CRITICAL':
                print(f"  {issue['file']}: {issue['message']}")
```

**ì˜ˆìƒ ì†Œìš”**: 4ì‹œê°„
**ì˜ˆìƒ íš¨ê³¼**: ë³´ì•ˆ ì·¨ì•½ì  ìë™ ê°ì§€, OWASP Top 10 ì¤€ìˆ˜

---

#### Phase 1C: `/check-security` Command í†µí•©

**ì„¤ì¹˜**:
```bash
npx claude-code-templates@latest --command check-security --yes
```

**ì‚¬ìš©**:
```bash
# ì „ì²´ ì½”ë“œë² ì´ìŠ¤ ìŠ¤ìº”
/check-security

# íŠ¹ì • íŒŒì¼ ìŠ¤ìº”
/check-security backend/api/war_room_router.py
```

**ì˜ˆìƒ íš¨ê³¼**: ì»¤ë°‹ ì „ ìë™ ë³´ì•ˆ ê²€ì‚¬

---

### 1.2 êµ¬í˜„ ë¡œë“œë§µ (ë³´ì•ˆ)

**Week 1: Secrets ì•”í˜¸í™”**
- [ ] SecretsManager í´ë˜ìŠ¤ êµ¬í˜„
- [ ] ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] .env â†’ .secrets.enc ì „í™˜
- [ ] .gitignore ì—…ë°ì´íŠ¸

**Week 2: ë³´ì•ˆ ê°ì‚¬ ìë™í™”**
- [ ] SecurityAuditor êµ¬í˜„
- [ ] OWASP Top 10 ìŠ¤ìº” ë¡œì§
- [ ] GitHub Actions í†µí•©
- [ ] `/check-security` ëª…ë ¹ ì„¤ì¹˜

**Week 3: ì§€ì†ì  ëª¨ë‹ˆí„°ë§**
- [ ] ì£¼ê°„ ë³´ì•ˆ ìŠ¤ìº” ìŠ¤ì¼€ì¤„
- [ ] Telegram ì•Œë¦¼ í†µí•©
- [ ] ë³´ì•ˆ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

**ì˜ˆìƒ íš¨ê³¼**:
- API í‚¤ ë…¸ì¶œ ìœ„í—˜: 100% â†’ 0%
- OWASP Top 10 ì¤€ìˆ˜: 0% â†’ 90%
- ë³´ì•ˆ ì·¨ì•½ì  ë°œê²¬ ì‹œê°„: ìˆ˜ë™ â†’ ìë™ (ì¦‰ì‹œ)

---

## Component Group 2: DevOps & CI/CD (ë°°í¬ ìë™í™”)

### 2.1 DevOps Engineer Agent

**ëª©í‘œ**: CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° ìë™ ë°°í¬

**ì„¤ì¹˜ ë°©ë²•**:
```bash
npx claude-code-templates@latest --agent devops-engineer --yes
```

**ì ìš© ì „ëµ**:

#### Phase 2A: GitHub Actions CI/CD íŒŒì´í”„ë¼ì¸

**í˜„ì¬ ìƒíƒœ**:
```yaml
# .github/workflows/ - ê¸°ë³¸ë§Œ ì¡´ì¬
- ci.yml (í…ŒìŠ¤íŠ¸ ë¯¸ì‹¤í–‰)
```

**ëª©í‘œ íŒŒì´í”„ë¼ì¸**:
```
Push â†’ Lint â†’ Test â†’ Build â†’ Deploy (Staging) â†’ Deploy (Production)
```

**íŒŒì¼**: `.github/workflows/ci-cd.yml` (ì‹ ê·œ)

```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  lint:
    name: Code Linting
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd backend
          pip install flake8 black mypy

      - name: Lint with flake8
        run: |
          cd backend
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics

      - name: Check formatting with black
        run: |
          cd backend
          black --check .

      - name: Type check with mypy
        run: |
          cd backend
          mypy --ignore-missing-imports .

  test-backend:
    name: Backend Tests
    runs-on: ubuntu-latest
    needs: lint

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: trading_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio

      - name: Run tests with coverage
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/trading_test
        run: |
          cd backend
          pytest --cov=. --cov-report=xml --cov-report=term

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./backend/coverage.xml
          flags: backend

  test-frontend:
    name: Frontend Tests
    runs-on: ubuntu-latest
    needs: lint

    steps:
      - uses: actions/checkout@v3

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'

      - name: Install dependencies
        run: |
          cd frontend
          npm ci

      - name: Run tests
        run: |
          cd frontend
          npm test -- --coverage

      - name: Build
        run: |
          cd frontend
          npm run build

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: [test-backend, test-frontend]

    steps:
      - uses: actions/checkout@v3

      - name: Run security audit
        run: |
          python scripts/security_audit.py

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [security-scan]
    if: github.ref == 'refs/heads/develop'

    steps:
      - uses: actions/checkout@v3

      - name: Deploy to Staging
        run: |
          echo "ğŸš€ Deploying to Staging..."
          # Docker Compose ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
          # ssh staging-server "cd /app && docker-compose pull && docker-compose up -d"

      - name: Health Check
        run: |
          sleep 10
          curl -f http://staging.example.com/health || exit 1

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [security-scan]
    if: github.ref == 'refs/heads/main'

    steps:
      - uses: actions/checkout@v3

      - name: Deploy to Production
        run: |
          echo "ğŸš€ Deploying to Production..."
          # Blue-Green ë°°í¬
          # 1. ìƒˆ ì»¨í…Œì´ë„ˆ ì‹œì‘ (Green)
          # 2. Health Check
          # 3. íŠ¸ë˜í”½ ì „í™˜
          # 4. ì´ì „ ì»¨í…Œì´ë„ˆ ì¢…ë£Œ (Blue)

      - name: Rollback on Failure
        if: failure()
        run: |
          echo "âŒ Deployment failed, rolling back..."
          # ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±
```

**ì˜ˆìƒ ì†Œìš”**: 6ì‹œê°„
**ì˜ˆìƒ íš¨ê³¼**: ë°°í¬ ì‹œê°„ 60ë¶„ â†’ 5ë¶„, ìë™í™”ëœ í…ŒìŠ¤íŠ¸/ë°°í¬

---

#### Phase 2B: `/setup-ci-cd-pipeline` Command

**ì„¤ì¹˜**:
```bash
npx claude-code-templates@latest --command setup-ci-cd-pipeline --yes
```

**ì‚¬ìš©**:
```bash
# ìë™ CI/CD ì„¤ì •
/setup-ci-cd-pipeline

# ìƒì„±ë˜ëŠ” íŒŒì¼:
# - .github/workflows/ci-cd.yml
# - .github/workflows/deploy-staging.yml
# - .github/workflows/deploy-production.yml
# - scripts/deploy.sh
```

---

#### Phase 2C: Docker ìµœì í™”

**í˜„ì¬ docker-compose.yml ê°œì„ **

**íŒŒì¼**: `docker-compose.yml`

```yaml
version: '3.8'

services:
  postgres:
    image: timescale/timescaledb:latest-pg15
    container_name: trading-postgres
    environment:
      POSTGRES_USER: ${DB_USER:-trading}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-trading123}
      POSTGRES_DB: ${DB_NAME:-ai_trading}
    ports:
      - "5433:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backend/database/migrations:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U trading"]
      interval: 10s
      timeout: 5s
      retries: 5

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: trading-backend
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://trading:trading123@postgres:5432/ai_trading
    ports:
      - "8001:8000"
    volumes:
      - ./backend:/app
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: trading-frontend
    depends_on:
      - backend
    ports:
      - "3002:3000"
    environment:
      VITE_API_URL: http://localhost:8001
    volumes:
      - ./frontend:/app
      - /app/node_modules
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: trading-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  postgres_data:
  redis_data:
```

**Dockerfile ìµœì í™”** (Backend)

**íŒŒì¼**: `backend/Dockerfile`

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    gcc \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ì„¤ì¹˜ (ë ˆì´ì–´ ìºì‹±)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . .

# í—¬ìŠ¤ì²´í¬ ìŠ¤í¬ë¦½íŠ¸
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# ë¹„ë£¨íŠ¸ ì‚¬ìš©ìë¡œ ì‹¤í–‰
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**ì˜ˆìƒ íš¨ê³¼**: ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹œê°„ 30ì´ˆ â†’ 10ì´ˆ, í—¬ìŠ¤ì²´í¬ ìë™í™”

---

### 2.2 êµ¬í˜„ ë¡œë“œë§µ (DevOps)

**Week 1: CI íŒŒì´í”„ë¼ì¸**
- [ ] GitHub Actions ì›Œí¬í”Œë¡œìš° ì‘ì„±
- [ ] Lint/Test ë‹¨ê³„ êµ¬ì„±
- [ ] ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸ í†µí•©

**Week 2: CD íŒŒì´í”„ë¼ì¸**
- [ ] Staging ë°°í¬ ìë™í™”
- [ ] Production ë°°í¬ (Blue-Green)
- [ ] ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜

**Week 3: Docker ìµœì í™”**
- [ ] Dockerfile ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œ
- [ ] docker-compose í—¬ìŠ¤ì²´í¬
- [ ] Redis ìºì‹± í†µí•©

**Week 4: ëª¨ë‹ˆí„°ë§**
- [ ] ë°°í¬ ì•Œë¦¼ (Telegram)
- [ ] ì—ëŸ¬ ì¶”ì  (Sentry í†µí•©)
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

**ì˜ˆìƒ íš¨ê³¼**:
- ë°°í¬ ì‹œê°„: 60ë¶„ â†’ 5ë¶„
- í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€: ìë™ ì¸¡ì • ë° ë¦¬í¬íŠ¸
- ë¡¤ë°± ì‹œê°„: ìˆ˜ë™ 30ë¶„ â†’ ìë™ 2ë¶„

---

## Component Group 3: Performance & Monitoring (ì„±ëŠ¥ ë° ëª¨ë‹ˆí„°ë§)

### 3.1 `/performance-audit` Command

**ëª©í‘œ**: ì½”ë“œ ì„±ëŠ¥ ìë™ ë¶„ì„ ë° ë³‘ëª© ì§€ì  ì‹ë³„

**ì„¤ì¹˜**:
```bash
npx claude-code-templates@latest --command performance-audit --yes
```

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤**:

#### Scenario 1: War Room MVP ì‘ë‹µ ì‹œê°„ ë¶„ì„

**í˜„ì¬ ìƒíƒœ**: 12.76ì´ˆ (ëª©í‘œ ë‹¬ì„±, ì¶”ê°€ ìµœì í™” ê°€ëŠ¥)

**ì‹¤í–‰**:
```bash
/performance-audit backend/ai/mvp/war_room_mvp.py

# ì¶œë ¥ ì˜ˆìƒ:
# ğŸ” Performance Audit - war_room_mvp.py
#
# Bottlenecks:
# 1. deliberate() - 12.5s
#    - Gemini API calls: 8.2s (3 agents)
#    - Database queries: 0.8s
#    - JSON parsing: 0.3s
#
# Recommendations:
# - Parallelize agent calls (8.2s â†’ 3s)
# - Cache portfolio state (0.2s saved)
# - Use async/await for DB queries
```

**ìµœì í™” ì ìš©**:

**íŒŒì¼**: `backend/ai/mvp/war_room_mvp.py`

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class WarRoomMVP:
    async def deliberate_parallel(self, symbol, ...):
        """ë³‘ë ¬ ì²˜ë¦¬ë¡œ Agent í˜¸ì¶œ ì‹œê°„ ë‹¨ì¶•"""

        # Before: ìˆœì°¨ ì‹¤í–‰ (8.2ì´ˆ)
        # trader_result = self.trader_agent.analyze(...)
        # risk_result = self.risk_agent.analyze(...)
        # analyst_result = self.analyst_agent.analyze(...)

        # After: ë³‘ë ¬ ì‹¤í–‰ (3ì´ˆ)
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [
                executor.submit(self.trader_agent.analyze, ...),
                executor.submit(self.risk_agent.analyze, ...),
                executor.submit(self.analyst_agent.analyze, ...)
            ]

            trader_result, risk_result, analyst_result = [
                f.result() for f in futures
            ]

        # PM Agent ìµœì¢… ê²°ì •
        return self.pm_agent.make_final_decision(...)
```

**ì˜ˆìƒ íš¨ê³¼**: War Room MVP 12.76ì´ˆ â†’ 7.5ì´ˆ (41% ê°œì„ )

---

#### Scenario 2: ë‰´ìŠ¤ ë°±í•„ ë©”ëª¨ë¦¬ ìµœì í™”

**í˜„ì¬ ë¬¸ì œ**: 20ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ìŠ¤íŒŒì´í¬

**ì‹¤í–‰**:
```bash
/performance-audit backend/data/processors/news_processor.py

# ì¶œë ¥:
# ğŸ” Memory Usage
# process_articles() - 450MB peak
#   - Article list: 120MB
#   - Embeddings: 280MB (OpenAI API)
#   - Intermediate data: 50MB
#
# Recommendations:
# - Use generator instead of list
# - Batch embedding API calls
# - Clear intermediate results
```

**ìµœì í™”**:

```python
# Before
def process_articles(self, articles: List[Article]):
    embeddings = [self.get_embedding(a.content) for a in articles]
    # 280MB ë©”ëª¨ë¦¬ ì‚¬ìš©

# After
def process_articles_batched(self, articles: List[Article], batch_size=5):
    for i in range(0, len(articles), batch_size):
        batch = articles[i:i+batch_size]
        embeddings = self.get_embeddings_batch(batch)  # ë°°ì¹˜ API í˜¸ì¶œ
        self.save_embeddings(embeddings)
        # ë©”ëª¨ë¦¬ í•´ì œ
        del embeddings
        gc.collect()
```

**ì˜ˆìƒ íš¨ê³¼**: ë©”ëª¨ë¦¬ ì‚¬ìš© 450MB â†’ 100MB

---

### 3.2 Performance Monitor Hook

**ëª©í‘œ**: ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

**ì„¤ì¹˜**:
```bash
npx claude-code-templates@latest --hook performance-monitor --yes
```

**êµ¬í˜„**:

**íŒŒì¼**: `backend/monitoring/performance_monitor.py` (ì‹ ê·œ)

```python
"""
Performance Monitor - ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¶”ì 

Date: 2026-01-03
Phase: Monitoring
"""
import time
import psutil
from functools import wraps
from typing import Callable
import asyncio

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„° ë° ìœ í‹¸ë¦¬í‹°"""

    def __init__(self, threshold_seconds: float = 1.0):
        self.threshold = threshold_seconds
        self.metrics = []

    def monitor(self, func: Callable):
        """í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                elapsed = time.time() - start_time
                end_memory = psutil.Process().memory_info().rss / 1024 / 1024

                metric = {
                    'function': func.__name__,
                    'elapsed': elapsed,
                    'memory_delta': end_memory - start_memory,
                    'timestamp': time.time()
                }

                self.metrics.append(metric)

                # ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì•Œë¦¼
                if elapsed > self.threshold:
                    await self._send_alert(metric)

        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024

            try:
                result = func(*args, **kwargs)
                return result
            finally:
                elapsed = time.time() - start_time
                end_memory = psutil.Process().memory_info().rss / 1024 / 1024

                metric = {
                    'function': func.__name__,
                    'elapsed': elapsed,
                    'memory_delta': end_memory - start_memory,
                    'timestamp': time.time()
                }

                self.metrics.append(metric)

                if elapsed > self.threshold:
                    # Sync í•¨ìˆ˜ì—ì„œëŠ” blocking ì•Œë¦¼
                    print(f"âš ï¸  Performance Alert: {func.__name__} took {elapsed:.2f}s")

        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        return sync_wrapper

    async def _send_alert(self, metric: dict):
        """ì„±ëŠ¥ ì•Œë¦¼ ì „ì†¡"""
        from backend.notifications.telegram_notifier import create_telegram_notifier

        telegram = create_telegram_notifier()
        await telegram.send_message(
            f"âš ï¸ Performance Alert\n\n"
            f"Function: {metric['function']}\n"
            f"Time: {metric['elapsed']:.2f}s\n"
            f"Memory: +{metric['memory_delta']:.1f}MB"
        )

# ê¸€ë¡œë²Œ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
perf_monitor = PerformanceMonitor(threshold_seconds=5.0)

# ì‚¬ìš© ì˜ˆì‹œ
@perf_monitor.monitor
async def deliberate(self, symbol, ...):
    # War Room MVP deliberation
    ...
```

**ì ìš©**:

```python
# backend/ai/mvp/war_room_mvp.py
from backend.monitoring.performance_monitor import perf_monitor

class WarRoomMVP:
    @perf_monitor.monitor
    async def deliberate(self, symbol, ...):
        # 5ì´ˆ ì´ˆê³¼ ì‹œ ìë™ ì•Œë¦¼
        ...
```

**ì˜ˆìƒ íš¨ê³¼**: ì„±ëŠ¥ ì €í•˜ ì¦‰ì‹œ ê°ì§€, Telegram ì•Œë¦¼

---

### 3.3 êµ¬í˜„ ë¡œë“œë§µ (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§)

**Week 1: ì„±ëŠ¥ ê°ì‚¬ ë„êµ¬**
- [ ] `/performance-audit` ì„¤ì¹˜ ë° í…ŒìŠ¤íŠ¸
- [ ] War Room MVP ë³‘ë ¬í™”
- [ ] ë‰´ìŠ¤ ë°±í•„ ë©”ëª¨ë¦¬ ìµœì í™”

**Week 2: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**
- [ ] PerformanceMonitor êµ¬í˜„
- [ ] ì£¼ìš” í•¨ìˆ˜ì— ë°ì½”ë ˆì´í„° ì ìš©
- [ ] Telegram ì•Œë¦¼ í†µí•©

**Week 3: ëŒ€ì‹œë³´ë“œ**
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì‹œê°í™”
- [ ] íˆìŠ¤í† ë¦¬ ì¶”ì 
- [ ] ìë™ ë¦¬í¬íŠ¸ ìƒì„±

**ì˜ˆìƒ íš¨ê³¼**:
- War Room MVP: 12.76ì´ˆ â†’ 7.5ì´ˆ
- ë©”ëª¨ë¦¬ ì‚¬ìš©: 450MB â†’ 100MB
- ì„±ëŠ¥ ì €í•˜ ê°ì§€: ìˆ˜ë™ â†’ ìë™ (ì‹¤ì‹œê°„)

---

## Component Group 4: Advanced Analytics (ê³ ê¸‰ ë¶„ì„)

### 4.1 Data Scientist Agent

**ëª©í‘œ**: Shadow Trading ì„±ê³¼ ë¶„ì„ ê³ ë„í™”

**ì„¤ì¹˜**:
```bash
npx claude-code-templates@latest --agent data-scientist --yes
```

**ì ìš© ì „ëµ**:

#### Phase 4A: Shadow Trading í†µê³„ ë¶„ì„

**íŒŒì¼**: `backend/analytics/shadow_trading_analyzer.py` (ì‹ ê·œ)

```python
"""
Shadow Trading í†µê³„ ë¶„ì„

Date: 2026-01-03
Phase: Advanced Analytics
"""
import pandas as pd
import numpy as np
from scipy import stats
from typing import Dict, List

class ShadowTradingAnalyzer:
    """Shadow Trading ì„±ê³¼ í†µê³„ ë¶„ì„"""

    def __init__(self, trades: List[Dict]):
        self.trades_df = pd.DataFrame(trades)

    def calculate_sharpe_ratio(self, risk_free_rate: float = 0.02) -> float:
        """ìƒ¤í”„ ë¹„ìœ¨ ê³„ì‚°"""
        if len(self.trades_df) < 2:
            return 0.0

        returns = self.trades_df['pnl_pct'].values
        excess_returns = returns - risk_free_rate / 252  # ì¼ë³„ ë¬´ìœ„í—˜ ìˆ˜ìµë¥ 

        if np.std(excess_returns) == 0:
            return 0.0

        return np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(252)

    def calculate_sortino_ratio(self, risk_free_rate: float = 0.02) -> float:
        """ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨ (í•˜ë°© ë¦¬ìŠ¤í¬ë§Œ ê³ ë ¤)"""
        returns = self.trades_df['pnl_pct'].values
        excess_returns = returns - risk_free_rate / 252

        downside_returns = excess_returns[excess_returns < 0]
        if len(downside_returns) == 0:
            return 0.0

        downside_std = np.std(downside_returns)
        if downside_std == 0:
            return 0.0

        return np.mean(excess_returns) / downside_std * np.sqrt(252)

    def calculate_calmar_ratio(self) -> float:
        """ì¹¼ë§ˆ ë¹„ìœ¨ (ì—°ê°„ ìˆ˜ìµë¥  / MDD)"""
        annual_return = self.trades_df['pnl_pct'].mean() * 252
        mdd = self.calculate_max_drawdown()

        if mdd == 0:
            return 0.0

        return annual_return / abs(mdd)

    def calculate_max_drawdown(self) -> float:
        """ìµœëŒ€ ë‚™í­"""
        cumulative = (1 + self.trades_df['pnl_pct']).cumprod()
        running_max = cumulative.cummax()
        drawdown = (cumulative - running_max) / running_max

        return drawdown.min()

    def analyze_win_streaks(self) -> Dict:
        """ì—°ìŠ¹/ì—°íŒ¨ ë¶„ì„"""
        wins = (self.trades_df['pnl'] > 0).astype(int).values

        # ì—°ìŠ¹ ì¹´ìš´íŠ¸
        current_streak = 0
        max_win_streak = 0
        max_loss_streak = 0

        for win in wins:
            if win == 1:
                current_streak = current_streak + 1 if current_streak > 0 else 1
                max_win_streak = max(max_win_streak, current_streak)
            else:
                current_streak = current_streak - 1 if current_streak < 0 else -1
                max_loss_streak = max(max_loss_streak, abs(current_streak))

        return {
            'max_win_streak': max_win_streak,
            'max_loss_streak': max_loss_streak,
            'current_streak': current_streak
        }

    def statistical_significance_test(self) -> Dict:
        """í†µê³„ì  ìœ ì˜ì„± ê²€ì • (Win Rate > 50%?)"""
        wins = len(self.trades_df[self.trades_df['pnl'] > 0])
        total = len(self.trades_df)

        # ì´í•­ ê²€ì •
        p_value = stats.binom_test(wins, total, 0.5, alternative='greater')

        return {
            'win_rate': wins / total if total > 0 else 0,
            'p_value': p_value,
            'significant': p_value < 0.05,
            'confidence_level': (1 - p_value) * 100
        }

    def generate_report(self) -> Dict:
        """ì¢…í•© í†µê³„ ë¦¬í¬íŠ¸"""
        return {
            'basic_metrics': {
                'total_trades': len(self.trades_df),
                'win_rate': len(self.trades_df[self.trades_df['pnl'] > 0]) / len(self.trades_df),
                'avg_pnl': self.trades_df['pnl'].mean(),
                'total_pnl': self.trades_df['pnl'].sum()
            },
            'risk_metrics': {
                'sharpe_ratio': self.calculate_sharpe_ratio(),
                'sortino_ratio': self.calculate_sortino_ratio(),
                'calmar_ratio': self.calculate_calmar_ratio(),
                'max_drawdown': self.calculate_max_drawdown()
            },
            'streak_analysis': self.analyze_win_streaks(),
            'statistical_test': self.statistical_significance_test()
        }

# ì‚¬ìš©
if __name__ == "__main__":
    # Shadow Trading ë°ì´í„° ë¡œë“œ
    from backend.execution.shadow_trading import ShadowTradingEngine

    engine = ShadowTradingEngine()
    trades = engine.get_trade_history()

    analyzer = ShadowTradingAnalyzer(trades)
    report = analyzer.generate_report()

    print("ğŸ“Š Shadow Trading Statistical Analysis")
    print(f"Sharpe Ratio: {report['risk_metrics']['sharpe_ratio']:.2f}")
    print(f"Sortino Ratio: {report['risk_metrics']['sortino_ratio']:.2f}")
    print(f"Statistical Significance: {report['statistical_test']['significant']}")
```

**ì˜ˆìƒ íš¨ê³¼**: ìƒ¤í”„ ë¹„ìœ¨, ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨ ë“± ê³ ê¸‰ ë©”íŠ¸ë¦­ ìë™ ê³„ì‚°

---

### 4.2 NLP Engineer Agent

**ëª©í‘œ**: ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ë° í‹°ì»¤ ì¶”ì¶œ ì •í™•ë„ í–¥ìƒ

**ì„¤ì¹˜**:
```bash
npx claude-code-templates@latest --agent nlp-engineer --yes
```

**ì ìš© ì „ëµ**:

#### Phase 4B: ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë„ì…

**í˜„ì¬ ë¬¸ì œ**: OpenAI Embedding API ì˜ì¡´ (ë¹„ìš©, í• ë‹¹ëŸ‰)

**í•´ê²°ì±…**: HuggingFace Sentence Transformers

**íŒŒì¼**: `backend/ml/local_embeddings.py` (ì‹ ê·œ)

```python
"""
ë¡œì»¬ ì„ë² ë”© ëª¨ë¸

Date: 2026-01-03
Phase: NLP Enhancement
"""
from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List

class LocalEmbeddingModel:
    """ë¡œì»¬ ì„ë² ë”© ìƒì„± (OpenAI ëŒ€ì²´)"""

    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        # 384ì°¨ì› ê²½ëŸ‰ ëª¨ë¸
        self.model = SentenceTransformer(model_name)

    def get_embedding(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        embedding = self.model.encode(text, convert_to_numpy=True)
        return embedding.tolist()

    def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """ë°°ì¹˜ ì„ë² ë”© (ë¹ ë¦„)"""
        embeddings = self.model.encode(texts, convert_to_numpy=True)
        return embeddings.tolist()

    def similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„"""
        emb1 = self.model.encode(text1, convert_to_numpy=True)
        emb2 = self.model.encode(text2, convert_to_numpy=True)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        return float(similarity)

# ì‚¬ìš©
embedding_model = LocalEmbeddingModel()

# ë‰´ìŠ¤ ê¸°ì‚¬ ì„ë² ë”©
article_text = "Apple announces new iPhone with AI features"
embedding = embedding_model.get_embedding(article_text)  # 384ì°¨ì› ë²¡í„°

# ìœ ì‚¬ ê¸°ì‚¬ ê²€ìƒ‰
query = "Apple product launch"
similarity = embedding_model.similarity(query, article_text)  # 0.85
```

**ë§ˆì´ê·¸ë ˆì´ì…˜**:

```python
# backend/data/processors/news_processor.py

# Before
from openai import OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def get_embedding(self, text):
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding  # 1536ì°¨ì›

# After
from backend.ml.local_embeddings import LocalEmbeddingModel
embedding_model = LocalEmbeddingModel()

def get_embedding(self, text):
    return embedding_model.get_embedding(text)  # 384ì°¨ì›
```

**ì˜ˆìƒ íš¨ê³¼**:
- ë¹„ìš©: $0.02/1000 articles â†’ $0 (ë¬´ë£Œ)
- ì†ë„: 200ms/article â†’ 50ms/article
- í• ë‹¹ëŸ‰: ì œí•œ ì—†ìŒ

---

#### Phase 4C: í‹°ì»¤ ì¶”ì¶œ ì •í™•ë„ í–¥ìƒ

**í˜„ì¬ ë¬¸ì œ**: Regex ê¸°ë°˜ í‹°ì»¤ ì¶”ì¶œ (ì •í™•ë„ ~60%)

**í•´ê²°ì±…**: Named Entity Recognition (NER) ëª¨ë¸

**íŒŒì¼**: `backend/ml/ticker_extractor.py` (ì‹ ê·œ)

```python
"""
NER ê¸°ë°˜ í‹°ì»¤ ì¶”ì¶œ

Date: 2026-01-03
Phase: NLP Enhancement
"""
import spacy
from typing import List, Set
import re

class TickerExtractor:
    """ê³ ê¸‰ í‹°ì»¤ ì¶”ì¶œ (NER + ê·œì¹™ ê¸°ë°˜)"""

    def __init__(self):
        # spaCy ì˜ì–´ ëª¨ë¸ (ì¡°ì§ëª… ì¸ì‹)
        self.nlp = spacy.load("en_core_web_sm")

        # S&P 500 í‹°ì»¤ ì‚¬ì „ (ìºì‹œ)
        self.known_tickers = self._load_ticker_dict()

        # íšŒì‚¬ëª… â†’ í‹°ì»¤ ë§¤í•‘
        self.company_to_ticker = {
            "Apple": "AAPL",
            "Microsoft": "MSFT",
            "Amazon": "AMZN",
            "Google": "GOOGL",
            "Alphabet": "GOOGL",
            "Tesla": "TSLA",
            "Nvidia": "NVDA",
            "Meta": "META",
            "Facebook": "META",
            # ... (500ê°œ íšŒì‚¬ ë§¤í•‘)
        }

    def extract_tickers(self, text: str) -> Set[str]:
        """í‹°ì»¤ ì¶”ì¶œ (ë‹¤ë‹¨ê³„)"""
        tickers = set()

        # 1. Regex íŒ¨í„´ (ì „í†µì  ë°©ì‹)
        regex_tickers = self._extract_regex(text)
        tickers.update(regex_tickers)

        # 2. NER ê¸°ë°˜ ì¡°ì§ëª… ì¶”ì¶œ
        ner_tickers = self._extract_ner(text)
        tickers.update(ner_tickers)

        # 3. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ì¦
        validated = self._validate_tickers(tickers, text)

        return validated

    def _extract_regex(self, text: str) -> Set[str]:
        """Regex í‹°ì»¤ ì¶”ì¶œ"""
        # $AAPL í˜•ì‹
        pattern1 = r'\$([A-Z]{1,5})\b'
        # (NASDAQ:AAPL) í˜•ì‹
        pattern2 = r'\((?:NYSE|NASDAQ):([A-Z]{1,5})\)'

        matches = set(re.findall(pattern1, text))
        matches.update(re.findall(pattern2, text))

        return {m for m in matches if m in self.known_tickers}

    def _extract_ner(self, text: str) -> Set[str]:
        """NER ê¸°ë°˜ ì¡°ì§ëª… â†’ í‹°ì»¤ ë³€í™˜"""
        doc = self.nlp(text)
        tickers = set()

        for ent in doc.ents:
            if ent.label_ == "ORG":
                # íšŒì‚¬ëª…ì„ í‹°ì»¤ë¡œ ë³€í™˜
                company = ent.text
                if company in self.company_to_ticker:
                    tickers.add(self.company_to_ticker[company])

        return tickers

    def _validate_tickers(self, tickers: Set[str], text: str) -> Set[str]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ì¦"""
        validated = set()

        for ticker in tickers:
            # ì£¼ë³€ ë‹¨ì–´ í™•ì¸ (ê¸ì •ì  ì‹ í˜¸)
            positive_keywords = [
                "earnings", "revenue", "stock", "shares", "analyst",
                "upgrade", "downgrade", "buy", "sell", "target price"
            ]

            if any(kw in text.lower() for kw in positive_keywords):
                validated.add(ticker)
            elif ticker in self.known_tickers:
                # ìœ ëª… í‹°ì»¤ëŠ” ë¬´ì¡°ê±´ í¬í•¨
                validated.add(ticker)

        return validated

    def _load_ticker_dict(self) -> Set[str]:
        """S&P 500 í‹°ì»¤ ì‚¬ì „ ë¡œë“œ"""
        # ì‹¤ì œë¡œëŠ” DB ë˜ëŠ” íŒŒì¼ì—ì„œ ë¡œë“œ
        return {"AAPL", "MSFT", "AMZN", "GOOGL", "TSLA", "NVDA", "META", ...}

# ì‚¬ìš©
extractor = TickerExtractor()

article_text = """
Apple Inc. (NASDAQ:AAPL) reported strong earnings today.
The company's revenue exceeded analyst expectations.
CEO Tim Cook announced new AI features for iPhone.
"""

tickers = extractor.extract_tickers(article_text)
# Output: {'AAPL'}
```

**ì˜ˆìƒ íš¨ê³¼**: í‹°ì»¤ ì¶”ì¶œ ì •í™•ë„ 60% â†’ 90%

---

### 4.3 êµ¬í˜„ ë¡œë“œë§µ (ê³ ê¸‰ ë¶„ì„)

**Week 1: Shadow Trading í†µê³„**
- [ ] ShadowTradingAnalyzer êµ¬í˜„
- [ ] ìƒ¤í”„/ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨ ê³„ì‚°
- [ ] í†µê³„ì  ìœ ì˜ì„± ê²€ì •
- [ ] ì£¼ê°„ ë¦¬í¬íŠ¸ ìë™ ìƒì„±

**Week 2: ë¡œì»¬ ì„ë² ë”©**
- [ ] Sentence Transformers ì„¤ì¹˜
- [ ] LocalEmbeddingModel êµ¬í˜„
- [ ] OpenAI API ë§ˆì´ê·¸ë ˆì´ì…˜
- [ ] ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸

**Week 3: í‹°ì»¤ ì¶”ì¶œ ê°œì„ **
- [ ] spaCy ëª¨ë¸ ì„¤ì¹˜
- [ ] TickerExtractor êµ¬í˜„
- [ ] íšŒì‚¬ëª… â†’ í‹°ì»¤ ë§¤í•‘ DB
- [ ] ì •í™•ë„ ì¸¡ì • (Before/After)

**ì˜ˆìƒ íš¨ê³¼**:
- Shadow Trading ë¶„ì„: ìˆ˜ë™ â†’ ìë™ (ì£¼ê°„)
- ì„ë² ë”© ë¹„ìš©: $20/month â†’ $0
- í‹°ì»¤ ì¶”ì¶œ ì •í™•ë„: 60% â†’ 90%

---

## Component Group 5: Cloud & Infrastructure (í´ë¼ìš°ë“œ í™•ì¥)

### 5.1 AWS Integration MCP

**ëª©í‘œ**: ë°ì´í„° ë°±ì—… ë° ì„œë²„ë¦¬ìŠ¤ ë°±í•„

**ì„¤ì¹˜**:
```bash
npx claude-code-templates@latest --mcp aws-integration --yes
```

**ì ìš© ì „ëµ**:

#### Phase 5A: S3 ë°±ì—… ì‹œìŠ¤í…œ

**íŒŒì¼**: `backend/cloud/s3_backup.py` (ì‹ ê·œ)

```python
"""
S3 ìë™ ë°±ì—…

Date: 2026-01-03
Phase: Cloud Integration
"""
import boto3
from datetime import datetime, timedelta
import gzip
import json
from pathlib import Path

class S3BackupManager:
    """PostgreSQL â†’ S3 ìë™ ë°±ì—…"""

    def __init__(self, bucket_name: str = "ai-trading-backups"):
        self.s3 = boto3.client('s3')
        self.bucket = bucket_name

    def backup_database(self):
        """ì „ì²´ DB ë°±ì—…"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = f"db_backup_{timestamp}.sql.gz"

        # pg_dumpë¡œ ë°±ì—…
        import subprocess

        dump_cmd = [
            "pg_dump",
            "-h", "localhost",
            "-p", "5433",
            "-U", "trading",
            "-d", "ai_trading",
            "-F", "c",  # Custom format
            "-f", f"/tmp/{backup_file}"
        ]

        subprocess.run(dump_cmd, check=True)

        # S3 ì—…ë¡œë“œ
        with open(f"/tmp/{backup_file}", "rb") as f:
            self.s3.upload_fileobj(
                f,
                self.bucket,
                f"database/{backup_file}"
            )

        print(f"âœ… Database backed up to S3: {backup_file}")

    def backup_news_articles(self, days: int = 30):
        """ìµœê·¼ ë‰´ìŠ¤ ê¸°ì‚¬ ë°±ì—… (JSON)"""
        from backend.database.repository import NewsRepository

        repo = NewsRepository()
        articles = repo.get_recent_articles(hours=days*24)

        timestamp = datetime.now().strftime("%Y%m%d")
        backup_file = f"news_{timestamp}.json.gz"

        # JSON ì••ì¶•
        with gzip.open(f"/tmp/{backup_file}", "wt") as f:
            json.dump([a.to_dict() for a in articles], f)

        # S3 ì—…ë¡œë“œ
        with open(f"/tmp/{backup_file}", "rb") as f:
            self.s3.upload_fileobj(
                f,
                self.bucket,
                f"news/{backup_file}"
            )

    def schedule_backups(self):
        """ë°±ì—… ìŠ¤ì¼€ì¤„ (Cron)"""
        # ë§¤ì¼ ìì • DB ë°±ì—…
        # ë§¤ì£¼ ì¼ìš”ì¼ ë‰´ìŠ¤ ë°±ì—…
        pass

# GitHub Actions ì›Œí¬í”Œë¡œìš°
# .github/workflows/backup.yml
"""
name: Daily Backup

on:
  schedule:
    - cron: '0 0 * * *'  # ë§¤ì¼ ìì •

jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Backup to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          python backend/cloud/s3_backup.py
"""
```

**ì˜ˆìƒ íš¨ê³¼**: ìë™ ë°±ì—…, ë°ì´í„° ì†ì‹¤ ìœ„í—˜ 0%

---

#### Phase 5B: Lambda ì„œë²„ë¦¬ìŠ¤ ë°±í•„

**ëª©ì **: ì£¼ê°€/ë‰´ìŠ¤ ë°±í•„ì„ Lambdaë¡œ ì˜¤í”„ë¡œë“œ (ë°±ì—”ë“œ ë¶€í•˜ ê°ì†Œ)

**íŒŒì¼**: `lambda/news_backfill/handler.py` (ì‹ ê·œ)

```python
"""
AWS Lambda - ë‰´ìŠ¤ ë°±í•„

Date: 2026-01-03
Phase: Serverless
"""
import json
import requests
from datetime import datetime, timedelta

def lambda_handler(event, context):
    """Lambda í•¸ë“¤ëŸ¬ - ë‰´ìŠ¤ ë°±í•„"""

    # íŒŒë¼ë¯¸í„°
    source = event.get('source', 'reuters')
    days = event.get('days', 7)

    # RSS í¬ë¡¤ë§
    articles = fetch_rss_articles(source, days)

    # ë°±ì—”ë“œ APIë¡œ ì „ì†¡
    backend_url = "https://api.trading.example.com/api/news/bulk"
    response = requests.post(backend_url, json=articles)

    return {
        'statusCode': 200,
        'body': json.dumps({
            'articles_fetched': len(articles),
            'backend_response': response.status_code
        })
    }

def fetch_rss_articles(source, days):
    # RSS íŒŒì‹± ë¡œì§
    pass
```

**ë°°í¬**:
```bash
# Serverless Framework
serverless deploy

# CloudWatch Eventsë¡œ ìŠ¤ì¼€ì¤„
# ë§¤ ì‹œê°„ ë‰´ìŠ¤ ë°±í•„
```

**ì˜ˆìƒ íš¨ê³¼**: ë°±ì—”ë“œ ë¶€í•˜ 30% ê°ì†Œ, ë¹„ìš© ì ˆê°

---

### 5.2 êµ¬í˜„ ë¡œë“œë§µ (í´ë¼ìš°ë“œ)

**Week 1: S3 ë°±ì—…**
- [ ] S3 ë²„í‚· ìƒì„±
- [ ] S3BackupManager êµ¬í˜„
- [ ] GitHub Actions ì›Œí¬í”Œë¡œìš°
- [ ] ë°±ì—… ë³µì› í…ŒìŠ¤íŠ¸

**Week 2: Lambda ë°±í•„**
- [ ] Lambda í•¨ìˆ˜ ì‘ì„±
- [ ] Serverless Framework ì„¤ì •
- [ ] CloudWatch Events ìŠ¤ì¼€ì¤„
- [ ] ë°±ì—”ë“œ í†µí•©

**ì˜ˆìƒ íš¨ê³¼**:
- ë°±ì—… ìë™í™”: 100%
- ë°±ì—”ë“œ ë¶€í•˜: -30%
- ë¹„ìš©: Lambda í”„ë¦¬í‹°ì–´ í™œìš©

---

## Component Group 6: Communication & Notifications (ì•Œë¦¼ ì‹œìŠ¤í…œ)

### 6.1 Discord/Slack Notifications Hook

**ëª©í‘œ**: ì¤‘ìš” ì´ë²¤íŠ¸ ì‹¤ì‹œê°„ ì•Œë¦¼

**ì„¤ì¹˜**:
```bash
npx claude-code-templates@latest --hook discord-notifications --yes
# ë˜ëŠ”
npx claude-code-templates@latest --hook slack-notifications --yes
```

**ì ìš© ì‹œë‚˜ë¦¬ì˜¤**:

#### Scenario 1: Shadow Trading ë§¤ë§¤ ì‹ í˜¸ ì•Œë¦¼

**íŒŒì¼**: `backend/notifications/discord_notifier.py` (ì‹ ê·œ)

```python
"""
Discord ì•Œë¦¼

Date: 2026-01-03
Phase: Notifications
"""
import requests
from typing import Dict

class DiscordNotifier:
    """Discord ì›¹í›… ì•Œë¦¼"""

    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url

    def send_trade_signal(self, signal: Dict):
        """ë§¤ë§¤ ì‹ í˜¸ ì•Œë¦¼"""
        embed = {
            "title": f"ğŸ¯ {signal['action']} Signal - {signal['ticker']}",
            "description": signal['reasoning'],
            "color": 0x00ff00 if signal['action'] == 'BUY' else 0xff0000,
            "fields": [
                {"name": "Confidence", "value": f"{signal['confidence']:.1%}", "inline": True},
                {"name": "Target Price", "value": f"${signal['target_price']:.2f}", "inline": True},
                {"name": "Agent", "value": signal['agent'], "inline": True}
            ],
            "timestamp": signal['created_at']
        }

        requests.post(self.webhook_url, json={"embeds": [embed]})

    def send_deployment_notification(self, status: str, commit: str):
        """ë°°í¬ ì•Œë¦¼"""
        color = 0x00ff00 if status == "success" else 0xff0000

        embed = {
            "title": f"ğŸš€ Deployment {status.upper()}",
            "description": f"Commit: {commit[:7]}",
            "color": color
        }

        requests.post(self.webhook_url, json={"embeds": [embed]})

# ì‚¬ìš©
discord = DiscordNotifier(os.getenv("DISCORD_WEBHOOK_URL"))

# War Room MVPì—ì„œ í˜¸ì¶œ
@router.post("/shadow/execute")
async def execute_shadow_trade(request):
    result = shadow_trading.execute_trade(...)

    # Discord ì•Œë¦¼
    discord.send_trade_signal({
        'action': result['action'],
        'ticker': result['ticker'],
        'confidence': result['confidence'],
        'reasoning': result['reasoning'],
        'agent': 'War Room MVP',
        'created_at': datetime.now().isoformat()
    })

    return result
```

**ì˜ˆìƒ íš¨ê³¼**: ì‹¤ì‹œê°„ ë§¤ë§¤ ì•Œë¦¼, íŒ€ í˜‘ì—… ê°•í™”

---

### 6.2 êµ¬í˜„ ë¡œë“œë§µ (ì•Œë¦¼)

**Week 1: Discord í†µí•©**
- [ ] DiscordNotifier êµ¬í˜„
- [ ] ì›¹í›… ì„¤ì •
- [ ] War Room MVP í†µí•©
- [ ] ë°°í¬ ì•Œë¦¼

**Week 2: Slack í†µí•©** (ì„ íƒ)
- [ ] SlackNotifier êµ¬í˜„
- [ ] ì±„ë„ë³„ ì•Œë¦¼ ë¶„ë¦¬
- [ ] ì¸í„°ë™í‹°ë¸Œ ë²„íŠ¼

**ì˜ˆìƒ íš¨ê³¼**:
- ë§¤ë§¤ ì‹ í˜¸ ì¦‰ì‹œ í™•ì¸
- ë°°í¬ ìƒíƒœ ì‹¤ì‹œê°„ ì¶”ì 
- íŒ€ í˜‘ì—… ê°œì„ 

---

## ì „ì²´ êµ¬í˜„ íƒ€ì„ë¼ì¸

### Month 1: ë³´ì•ˆ & DevOps (High Priority)
- Week 1-2: Security Auditor (Secrets ì•”í˜¸í™”, OWASP ìŠ¤ìº”)
- Week 3-4: DevOps Engineer (CI/CD íŒŒì´í”„ë¼ì¸, Docker ìµœì í™”)

**ì˜ˆìƒ íš¨ê³¼**: ë³´ì•ˆ ì·¨ì•½ì  0ê±´, ë°°í¬ ì‹œê°„ 60ë¶„ â†’ 5ë¶„

### Month 2: ì„±ëŠ¥ & ë¶„ì„ (Medium Priority)
- Week 1-2: Performance Monitoring (ì„±ëŠ¥ ê°ì‚¬, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§)
- Week 3-4: Data Scientist (Shadow Trading í†µê³„, ê³ ê¸‰ ë©”íŠ¸ë¦­)

**ì˜ˆìƒ íš¨ê³¼**: War Room MVP 7.5ì´ˆ, ìƒ¤í”„ ë¹„ìœ¨ ìë™ ê³„ì‚°

### Month 3: NLP & í´ë¼ìš°ë“œ (Medium Priority)
- Week 1-2: NLP Engineer (ë¡œì»¬ ì„ë² ë”©, í‹°ì»¤ ì¶”ì¶œ 90%)
- Week 3-4: AWS Integration (S3 ë°±ì—…, Lambda ë°±í•„)

**ì˜ˆìƒ íš¨ê³¼**: ì„ë² ë”© ë¹„ìš© $0, í‹°ì»¤ ì •í™•ë„ 90%

### Month 4: ì•Œë¦¼ & ê³ ê¸‰ ê¸°ëŠ¥ (Low Priority)
- Week 1-2: Discord/Slack ì•Œë¦¼
- Week 3-4: PDF Processing, Excel Automation

**ì˜ˆìƒ íš¨ê³¼**: ì‹¤ì‹œê°„ ì•Œë¦¼, SEC ë³´ê³ ì„œ ìë™ íŒŒì‹±

---

## ìµœì¢… ì„±ê³µ ê¸°ì¤€

### ë³´ì•ˆ (P1)
- [ ] API í‚¤ ì•”í˜¸í™” 100%
- [ ] OWASP Top 10 ìŠ¤ìº” ìë™í™”
- [ ] Secrets ë…¸ì¶œ 0ê±´
- [ ] ë³´ì•ˆ ê°ì‚¬ ì£¼ê°„ ìë™ ì‹¤í–‰

### DevOps (P1)
- [ ] CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- [ ] í…ŒìŠ¤íŠ¸ ìë™ ì‹¤í–‰ (ì»¤ë²„ë¦¬ì§€ 80%+)
- [ ] ë°°í¬ ì‹œê°„ < 5ë¶„
- [ ] ë¡¤ë°± ì‹œê°„ < 2ë¶„

### ì„±ëŠ¥ (P2)
- [ ] War Room MVP < 8ì´ˆ
- [ ] ì„±ëŠ¥ ì €í•˜ ìë™ ê°ì§€
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš© < 200MB (ë‰´ìŠ¤ ë°±í•„)

### ë¶„ì„ (P2)
- [ ] ìƒ¤í”„ ë¹„ìœ¨ ìë™ ê³„ì‚°
- [ ] Shadow Trading ì£¼ê°„ ë¦¬í¬íŠ¸ ìë™ ìƒì„±
- [ ] í†µê³„ì  ìœ ì˜ì„± ê²€ì •

### NLP (P2)
- [ ] ë¡œì»¬ ì„ë² ë”© (ë¹„ìš© $0)
- [ ] í‹°ì»¤ ì¶”ì¶œ ì •í™•ë„ > 90%

### í´ë¼ìš°ë“œ (P3)
- [ ] S3 ìë™ ë°±ì—… (ì¼ì¼)
- [ ] Lambda ë°±í•„ (ì‹œê°„ë‹¹)

### ì•Œë¦¼ (P3)
- [ ] Discord/Slack ì‹¤ì‹œê°„ ì•Œë¦¼
- [ ] ë°°í¬ ìƒíƒœ ì•Œë¦¼

---

## ê´€ë ¨ ë¬¸ì„œ

**ì´ë¯¸ ê³„íš ì™„ë£Œ:**
- [260103_Claude_Code_Templates_Implementation_Plan.md](260103_Claude_Code_Templates_Implementation_Plan.md) - í…ŒìŠ¤íŠ¸, í”„ë¡ íŠ¸ì—”ë“œ, Git Hooks
- [260102_Database_Optimization_Plan.md](260102_Database_Optimization_Plan.md) - DB ìµœì í™”
- [Work_Log_20260102.md](Work_Log_20260102.md) - 2026-01-02 ì‘ì—… ì™„ë£Œ

**ì°¸ê³  ìë£Œ:**
- [260102_Claude_Code_Templates_Review.md](260102_Claude_Code_Templates_Review.md) - 600+ í…œí”Œë¦¿ ë¶„ì„
- [Shadow_Trading_Week1_Report.md](Shadow_Trading_Week1_Report.md) - Shadow Trading ëª¨ë‹ˆí„°ë§

---

**ì‘ì„±ì¼**: 2026-01-03
**ì‘ì„±ì**: AI Trading System Development Team
**ìš°ì„ ìˆœìœ„**: P2-P3 (Medium to Low Priority)
**ìƒíƒœ**: ğŸ“‹ Ready for Implementation
**ë‹¤ìŒ ë¦¬ë·°**: Month 1 ì™„ë£Œ í›„ (ë³´ì•ˆ & DevOps)
**ì´ ì˜ˆìƒ ì†Œìš”**: 4ê°œì›” (16ì£¼)
