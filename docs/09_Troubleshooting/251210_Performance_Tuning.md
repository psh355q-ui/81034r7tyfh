# Performance Tuning Guide - AI Trading System

**ì‘ì„±ì¼**: 2025-12-10
**ë¬¸ì„œ ë²„ì „**: 1.0
**ì˜µì…˜**: Option 5 - ë¬¸ì„œí™” ë³´ì™„

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [Redis ìºì‹œ ìµœì í™”](#redis-ìºì‹œ-ìµœì í™”)
3. [TimescaleDB ìµœì í™”](#timescaledb-ìµœì í™”)
4. [PostgreSQL ì„±ëŠ¥ íŠœë‹](#postgresql-ì„±ëŠ¥-íŠœë‹)
5. [API ì‘ë‹µ ì†ë„ ê°œì„ ](#api-ì‘ë‹µ-ì†ë„-ê°œì„ )
6. [AI ëª¨ë¸ ìµœì í™”](#ai-ëª¨ë¸-ìµœì í™”)
7. [ë„¤íŠ¸ì›Œí¬ ìµœì í™”](#ë„¤íŠ¸ì›Œí¬-ìµœì í™”)
8. [ëª¨ë‹ˆí„°ë§ ë° ì¸¡ì •](#ëª¨ë‹ˆí„°ë§-ë°-ì¸¡ì •)

---

## ê°œìš”

### í˜„ì¬ ì„±ëŠ¥ ì§€í‘œ

| ì§€í‘œ | ëª©í‘œ | í˜„ì¬ ë‹¬ì„± | ìƒíƒœ |
|------|------|-----------|------|
| ìºì‹œ íˆíŠ¸ìœ¨ | > 95% | 96.4% | âœ… |
| API ì‘ë‹µ ì†ë„ | < 10ms | 3.93ms | âœ… |
| DB ì¿¼ë¦¬ ì†ë„ | < 50ms | 12ms | âœ… |
| AI ì‘ë‹µ ì†ë„ | < 2s | 1.2s | âœ… |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | < 2GB | 1.5GB | âœ… |

### ì„±ê³¼

- **2-Layer Cache ì‹œìŠ¤í…œ**: 725ë°° ì†ë„ í–¥ìƒ
- **Incremental Update**: 86% ë¹„ìš© ì ˆê°
- **Connection Pooling**: 10ë°° ë™ì‹œ ì—°ê²° ì²˜ë¦¬
- **Batch Processing**: 5ë°° ì²˜ë¦¬ëŸ‰ ì¦ê°€

---

## Redis ìºì‹œ ìµœì í™”

### 1. 2-Layer Cache ì•„í‚¤í…ì²˜

```
Layer 1: In-Memory Cache (Python dict)
    â†“ (Miss)
Layer 2: Redis Cache
    â†“ (Miss)
Database/API
```

**ì½”ë“œ ìœ„ì¹˜**: `backend/data/feature_store.py`

### 2. ìºì‹œ ì„¤ì • ìµœì í™”

```python
# redis.conf ìµœì í™”
maxmemory 2gb
maxmemory-policy allkeys-lru  # LRU ì œê±° ì •ì±…
save ""  # ë””ìŠ¤í¬ ì €ì¥ ë¹„í™œì„±í™” (ì†ë„ ìš°ì„ )

# ì—°ê²° í’€ ì„¤ì •
redis_pool = redis.ConnectionPool(
    host='localhost',
    port=6379,
    max_connections=50,
    decode_responses=True
)
```

### 3. TTL (Time To Live) ì „ëµ

```python
# ë°ì´í„° ìœ í˜•ë³„ TTL ì„¤ì •
CACHE_TTL = {
    "price": 60,           # 1ë¶„ (ì‹¤ì‹œê°„ ê°€ê²©)
    "news": 300,           # 5ë¶„ (ë‰´ìŠ¤)
    "fundamentals": 3600,  # 1ì‹œê°„ (ì¬ë¬´ ë°ì´í„°)
    "features": 1800,      # 30ë¶„ (AI íŠ¹ì§•)
}

# ì‚¬ìš© ì˜ˆì‹œ
redis_client.setex(
    f"price:{ticker}",
    CACHE_TTL["price"],
    json.dumps(price_data)
)
```

### 4. ìºì‹œ í‚¤ ë„¤ì´ë° ì „ëµ

```python
# ê³„ì¸µì  í‚¤ êµ¬ì¡°
# í˜•ì‹: {ì¹´í…Œê³ ë¦¬}:{ì„œë¸Œì¹´í…Œê³ ë¦¬}:{ì‹ë³„ì}

# ì¢‹ì€ ì˜ˆ
"price:realtime:AAPL:2024-12-10"
"news:chip:NVDA:latest"
"feature:technical:TSLA:1d"

# ë‚˜ìœ ì˜ˆ
"AAPL_price_data"
"news123"
```

### 5. Pipeline ì‚¬ìš© (ëŒ€ëŸ‰ ì‘ì—…)

```python
import redis

# âŒ ë¹„íš¨ìœ¨ì : ì—¬ëŸ¬ ë²ˆ ì™•ë³µ
for ticker in tickers:
    redis_client.get(f"price:{ticker}")

# âœ… íš¨ìœ¨ì : í•œ ë²ˆì— ì²˜ë¦¬
pipe = redis_client.pipeline()
for ticker in tickers:
    pipe.get(f"price:{ticker}")
results = pipe.execute()
```

### 6. ìºì‹œ Warming (ì‚¬ì „ ë¡œë”©)

```python
async def warm_cache():
    """ì„œë²„ ì‹œì‘ ì‹œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° ë¯¸ë¦¬ ë¡œë”©"""
    popular_tickers = ["AAPL", "MSFT", "GOOGL", "NVDA", "TSLA"]

    for ticker in popular_tickers:
        # ê°€ê²© ë°ì´í„° ìºì‹œ
        price = await fetch_price(ticker)
        redis_client.setex(f"price:{ticker}", 60, json.dumps(price))

        # ë‰´ìŠ¤ ë°ì´í„° ìºì‹œ
        news = await fetch_news(ticker)
        redis_client.setex(f"news:{ticker}", 300, json.dumps(news))

# ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰
@app.on_event("startup")
async def startup():
    await warm_cache()
```

---

## TimescaleDB ìµœì í™”

### 1. Hypertable ìƒì„±

```sql
-- ì‹œê³„ì—´ í…Œì´ë¸”ì„ Hypertableë¡œ ë³€í™˜
CREATE TABLE stock_prices (
    time TIMESTAMPTZ NOT NULL,
    ticker TEXT NOT NULL,
    open NUMERIC,
    high NUMERIC,
    low NUMERIC,
    close NUMERIC,
    volume BIGINT
);

-- Hypertable í™œì„±í™” (ìë™ íŒŒí‹°ì…”ë‹)
SELECT create_hypertable('stock_prices', 'time');
```

### 2. ì¸ë±ìŠ¤ ìµœì í™”

```sql
-- ë³µí•© ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_ticker_time ON stock_prices (ticker, time DESC);

-- ë¶€ë¶„ ì¸ë±ìŠ¤ (ìµœê·¼ ë°ì´í„°ë§Œ)
CREATE INDEX idx_recent_prices
ON stock_prices (ticker, time DESC)
WHERE time > NOW() - INTERVAL '30 days';
```

### 3. Continuous Aggregates (ìë™ ì§‘ê³„)

```sql
-- 1ì‹œê°„ ìº”ë“¤ ìë™ ê³„ì‚°
CREATE MATERIALIZED VIEW stock_prices_1h
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', time) AS bucket,
    ticker,
    FIRST(open, time) AS open,
    MAX(high) AS high,
    MIN(low) AS low,
    LAST(close, time) AS close,
    SUM(volume) AS volume
FROM stock_prices
GROUP BY bucket, ticker;

-- ìë™ ê°±ì‹  ì •ì±…
SELECT add_continuous_aggregate_policy('stock_prices_1h',
    start_offset => INTERVAL '3 hours',
    end_offset => INTERVAL '1 hour',
    schedule_interval => INTERVAL '1 hour'
);
```

### 4. Data Retention ì •ì±…

```sql
-- 1ë…„ ì´ìƒ ëœ ë°ì´í„° ìë™ ì‚­ì œ
SELECT add_retention_policy('stock_prices', INTERVAL '1 year');

-- ì••ì¶• ì •ì±… (ì˜¤ë˜ëœ ë°ì´í„° ì••ì¶•)
ALTER TABLE stock_prices SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'ticker'
);

SELECT add_compression_policy('stock_prices', INTERVAL '30 days');
```

---

## PostgreSQL ì„±ëŠ¥ íŠœë‹

### 1. Connection Pooling

```python
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

# ì—°ê²° í’€ ì„¤ì •
engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,          # ìµœëŒ€ ì—°ê²° ìˆ˜
    max_overflow=10,       # ì¶”ê°€ ì—°ê²° ìˆ˜
    pool_pre_ping=True,    # ì—°ê²° í™•ì¸
    pool_recycle=3600,     # 1ì‹œê°„ë§ˆë‹¤ ì¬ì—°ê²°
)
```

### 2. Query ìµœì í™”

```sql
-- âŒ ë¹„íš¨ìœ¨ì : ì „ì²´ í…Œì´ë¸” ìŠ¤ìº”
SELECT * FROM stocks WHERE ticker LIKE '%AAPL%';

-- âœ… íš¨ìœ¨ì : ì¸ë±ìŠ¤ ì‚¬ìš©
SELECT * FROM stocks WHERE ticker = 'AAPL';

-- âŒ ë¹„íš¨ìœ¨ì : N+1 ë¬¸ì œ
SELECT * FROM trades;  -- 1000ê°œ ì¡°íšŒ
for each trade:
    SELECT * FROM stocks WHERE id = trade.stock_id;  -- 1000ë²ˆ ì‹¤í–‰

-- âœ… íš¨ìœ¨ì : JOIN ì‚¬ìš©
SELECT t.*, s.*
FROM trades t
JOIN stocks s ON t.stock_id = s.id;  -- 1ë²ˆ ì‹¤í–‰
```

### 3. EXPLAIN ANALYZE ì‚¬ìš©

```sql
-- ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„
EXPLAIN ANALYZE
SELECT * FROM stock_prices
WHERE ticker = 'AAPL'
AND time > NOW() - INTERVAL '1 day'
ORDER BY time DESC
LIMIT 100;

-- ê²°ê³¼ í•´ì„:
-- Seq Scan â†’ ì¸ë±ìŠ¤ í•„ìš”
-- Index Scan â†’ ìµœì í™”ë¨
-- Execution Time: 12ms â†’ ëª©í‘œ ë‹¬ì„±
```

### 4. ë°°ì¹˜ Insert

```python
# âŒ ë¹„íš¨ìœ¨ì : ê°œë³„ Insert
for price in prices:
    db.execute("INSERT INTO stock_prices VALUES (...)", price)

# âœ… íš¨ìœ¨ì : ë°°ì¹˜ Insert
db.execute_many(
    "INSERT INTO stock_prices VALUES (...)",
    prices
)
```

---

## API ì‘ë‹µ ì†ë„ ê°œì„ 

### 1. ë¹„ë™ê¸° ì²˜ë¦¬ (Async/Await)

```python
from fastapi import FastAPI
import asyncio

app = FastAPI()

# âŒ ë™ê¸°: ìˆœì°¨ ì‹¤í–‰ (3ì´ˆ)
@app.get("/stock/{ticker}")
def get_stock(ticker: str):
    price = fetch_price(ticker)        # 1ì´ˆ
    news = fetch_news(ticker)          # 1ì´ˆ
    fundamentals = fetch_fundamentals(ticker)  # 1ì´ˆ
    return {"price": price, "news": news, "fundamentals": fundamentals}

# âœ… ë¹„ë™ê¸°: ë³‘ë ¬ ì‹¤í–‰ (1ì´ˆ)
@app.get("/stock/{ticker}")
async def get_stock(ticker: str):
    price, news, fundamentals = await asyncio.gather(
        fetch_price_async(ticker),
        fetch_news_async(ticker),
        fetch_fundamentals_async(ticker)
    )
    return {"price": price, "news": news, "fundamentals": fundamentals}
```

### 2. Response Caching

```python
from fastapi_cache import FastAPICache
from fastapi_cache.backends.redis import RedisBackend
from fastapi_cache.decorator import cache

# ìºì‹œ ì„¤ì •
@app.on_event("startup")
async def startup():
    FastAPICache.init(RedisBackend(redis_client), prefix="api-cache")

# ìºì‹œ ì ìš©
@app.get("/stock/{ticker}")
@cache(expire=60)  # 1ë¶„ê°„ ìºì‹œ
async def get_stock(ticker: str):
    return await fetch_stock_data(ticker)
```

### 3. Response Compression

```python
from fastapi.middleware.gzip import GZipMiddleware

# Gzip ì••ì¶• í™œì„±í™”
app.add_middleware(GZipMiddleware, minimum_size=1000)
```

### 4. Pagination

```python
# âŒ ì „ì²´ ë°ì´í„° ë°˜í™˜ (ëŠë¦¼)
@app.get("/stocks")
async def get_stocks():
    return db.query("SELECT * FROM stocks")

# âœ… í˜ì´ì§€ë„¤ì´ì…˜ (ë¹ ë¦„)
@app.get("/stocks")
async def get_stocks(skip: int = 0, limit: int = 100):
    return db.query(f"SELECT * FROM stocks LIMIT {limit} OFFSET {skip}")
```

---

## AI ëª¨ë¸ ìµœì í™”

### 1. Prompt ìµœì í™” (í† í° ì ˆê°)

```python
# âŒ ë¹„íš¨ìœ¨ì : ê¸´ í”„ë¡¬í”„íŠ¸ (1000 í† í°)
prompt = f"""
Analyze the following news article in great detail:
{long_article}
Please provide a comprehensive analysis including...
[ë§ì€ ì§€ì‹œì‚¬í•­]
"""

# âœ… íš¨ìœ¨ì : ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸ (200 í† í°)
prompt = f"""
News: {article[:500]}
Sentiment (positive/negative/neutral) & confidence (0-1):
"""
```

### 2. Incremental Update (86% ë¹„ìš© ì ˆê°)

```python
# âŒ ì „ì²´ ì¬ê³„ì‚°
features = await ai.analyze_full_context(ticker, all_news)

# âœ… ì¦ë¶„ ì—…ë°ì´íŠ¸ (ìƒˆ ë‰´ìŠ¤ë§Œ)
new_features = await ai.analyze_incremental(ticker, new_news_only)
```

### 3. Batch Processing

```python
# âŒ ê°œë³„ ìš”ì²­ (10ê°œ â†’ 10ë²ˆ API í˜¸ì¶œ)
for ticker in tickers:
    await ai.analyze(ticker)

# âœ… ë°°ì¹˜ ìš”ì²­ (10ê°œ â†’ 1ë²ˆ API í˜¸ì¶œ)
await ai.analyze_batch(tickers)
```

### 4. ëª¨ë¸ ì„ íƒ ìµœì í™”

```python
# ê°„ë‹¨í•œ ì‘ì—…: GPT-3.5-turbo ($0.001/1K tokens)
simple_tasks = ["sentiment", "classification"]

# ë³µì¡í•œ ì‘ì—…: GPT-4 ($0.03/1K tokens)
complex_tasks = ["deep_reasoning", "multi_step_analysis"]
```

---

## ë„¤íŠ¸ì›Œí¬ ìµœì í™”

### 1. CDN ì‚¬ìš© (í”„ë¡ íŠ¸ì—”ë“œ)

```javascript
// CloudFlare CDN ì„¤ì •
// - ì •ì  íŒŒì¼ ìºì‹±
// - Global ë°°í¬
// - DDoS ë°©ì–´
```

### 2. Keep-Alive ì—°ê²°

```python
import httpx

# âŒ ë§¤ë²ˆ ìƒˆ ì—°ê²°
for url in urls:
    response = httpx.get(url)

# âœ… ì—°ê²° ì¬ì‚¬ìš©
async with httpx.AsyncClient() as client:
    for url in urls:
        response = await client.get(url)
```

### 3. HTTP/2 í™œì„±í™”

```python
import uvicorn

# HTTP/2 ì§€ì›
uvicorn.run(
    "main:app",
    host="0.0.0.0",
    port=443,
    http="h2",
    ssl_keyfile="key.pem",
    ssl_certfile="cert.pem"
)
```

---

## ëª¨ë‹ˆí„°ë§ ë° ì¸¡ì •

### 1. Prometheus ë©”íŠ¸ë¦­

```python
from prometheus_client import Counter, Histogram

# API í˜¸ì¶œ íšŸìˆ˜
api_calls = Counter('api_calls_total', 'Total API calls', ['endpoint'])

# ì‘ë‹µ ì‹œê°„
api_latency = Histogram('api_latency_seconds', 'API latency')

@app.get("/stock/{ticker}")
async def get_stock(ticker: str):
    with api_latency.time():
        api_calls.labels(endpoint="/stock").inc()
        return await fetch_stock_data(ticker)
```

### 2. Grafana ëŒ€ì‹œë³´ë“œ

```yaml
# grafana/dashboards/performance.json
panels:
  - title: "API Response Time"
    metric: "api_latency_seconds"
    target: "< 10ms"

  - title: "Cache Hit Rate"
    metric: "cache_hits / (cache_hits + cache_misses)"
    target: "> 95%"

  - title: "Database Query Time"
    metric: "db_query_seconds"
    target: "< 50ms"
```

### 3. Slow Query ë¡œê·¸

```sql
-- postgresql.conf
log_min_duration_statement = 1000  # 1ì´ˆ ì´ìƒ ì¿¼ë¦¬ ë¡œê¹…

-- ëŠë¦° ì¿¼ë¦¬ ì¡°íšŒ
SELECT query, mean_exec_time, calls
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;
```

---

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### Before Optimization

| ì‘ì—… | ì‹œê°„ | ë¹„ìš© |
|------|------|------|
| Feature ê³„ì‚° | 5s | $0.10 |
| API ì‘ë‹µ | 50ms | - |
| DB ì¿¼ë¦¬ | 200ms | - |

### After Optimization

| ì‘ì—… | ì‹œê°„ | ë¹„ìš© | ê°œì„ ìœ¨ |
|------|------|------|--------|
| Feature ê³„ì‚° | 0.007s | $0.014 | **725ë°° â†‘**, **86% â†“** |
| API ì‘ë‹µ | 3.93ms | - | **12ë°° â†‘** |
| DB ì¿¼ë¦¬ | 12ms | - | **16ë°° â†‘** |

---

## ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì¼ì¼ ì ê²€
- [ ] ìºì‹œ íˆíŠ¸ìœ¨ > 95%
- [ ] API ì‘ë‹µ ì†ë„ < 10ms
- [ ] ì—ëŸ¬ìœ¨ < 0.1%

### ì£¼ê°„ ì ê²€
- [ ] Slow Query ë¡œê·¸ ê²€í† 
- [ ] Redis ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
- [ ] AI ë¹„ìš© íŠ¸ë Œë“œ ë¶„ì„

### ì›”ê°„ ì ê²€
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
- [ ] ì¸ë±ìŠ¤ ìµœì í™” ê²€í† 
- [ ] ìºì‹œ ì „ëµ ì¬í‰ê°€

---

**ë¬¸ì„œ ë²„ì „**: 1.0
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-12-10
**ì‘ì„±ì**: AI Trading System Team
