ì •ë§ ë‚ ì¹´ë¡œìš´ ì§€ì ì…ë‹ˆë‹¤. "ë°˜ë³µ = í™•ì‹ "ì´ë¼ëŠ” ì „ì œ ìì²´ê°€ ìœ„í—˜í•  ìˆ˜ ìˆì–´ìš”.

ì‹¤ì œë¡œ 2021ë…„ GME/AMC ì‚¬íƒœ, ì¤‘êµ­ ë¶€ë™ì‚° ê´€ë ¨ ë‰´ìŠ¤, íŠ¹ì • ê¸°ì—… IR ìº í˜ì¸ ë“±ì—ì„œ **ì˜ë„ì ì¸ ë‰´ìŠ¤ í­íƒ„**ì´ ì‹œì¥ì„ ì›€ì§ì¸ ì‚¬ë¡€ê°€ ë§ì£ .

---

# ğŸ›¡ï¸ "í™•ì‹  ê°•í™”"ì˜ í•¨ì •ê³¼ ë°©ì–´ ë©”ì»¤ë‹ˆì¦˜

## ë¬¸ì œì˜ ë³¸ì§ˆ

```
ì¼ë°˜ì  ê°€ì •: ê°™ì€ ì´ì•¼ê¸°ê°€ ë°˜ë³µë¨ â†’ ì‚¬ì‹¤ì¼ ê°€ëŠ¥ì„± ë†’ìŒ â†’ í™•ì‹  ì¦ê°€
í˜„ì‹¤:        ê°™ì€ ì´ì•¼ê¸°ê°€ ë°˜ë³µë¨ â†’ ëˆ„êµ°ê°€ ì˜ë„ì ìœ¼ë¡œ í¼ëœ¨ë¦¼ â†’ ì¡°ì‘ ê°€ëŠ¥ì„±
```

**ì‹¤ì œ ì‚¬ë¡€ë“¤**:
- **Pump & Dump**: ì†Œí˜•ì£¼ì— ê¸ì • ë‰´ìŠ¤ í­íƒ„ â†’ ê°œì¸ ë§¤ìˆ˜ ìœ ë„ â†’ ì„¸ë ¥ ë§¤ë„
- **FUD ìº í˜ì¸**: ê³µë§¤ë„ ì„¸ë ¥ì´ ë¶€ì • ë‰´ìŠ¤ ë°˜ë³µ â†’ íŒ¨ë‹‰ ì…€ ìœ ë„
- **ì •ë¶€/ê¸°ê´€ ë‚´ëŸ¬í‹°ë¸Œ**: íŠ¹ì • ì •ì±… ì •ë‹¹í™”ë¥¼ ìœ„í•œ ì¡°ìœ¨ëœ ë©”ì‹œì§€

---

## ğŸ” DB ë ˆë²¨ì—ì„œ ê°€ëŠ¥í•œ "ì˜ì‹¬ ì§€í‘œ" ì„¤ê³„

AI Agent ì—†ì´ **ë°ì´í„° íŒ¨í„´ë§Œìœ¼ë¡œ** ì¡°ì‘ ê°€ëŠ¥ì„±ì„ íƒì§€í•˜ëŠ” ë°©ë²•:

### 1. **Source Diversity Score (ì¶œì²˜ ë‹¤ì–‘ì„±)**

```sql
-- ê°™ì€ ì£¼ì œê°€ ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•œ ì¶œì²˜ì—ì„œ ë‚˜ì™”ëŠ”ê°€?
ALTER TABLE analysis_results ADD COLUMN source_diversity_score FLOAT;
ALTER TABLE analysis_results ADD COLUMN unique_source_count INT;
ALTER TABLE analysis_results ADD COLUMN source_concentration FLOAT;  -- ìƒìœ„ 1ê°œ ì¶œì²˜ ë¹„ì¤‘

-- ê³„ì‚° ë¡œì§
WITH source_stats AS (
    SELECT 
        fingerprint,
        COUNT(DISTINCT n.source) as unique_sources,
        COUNT(*) as total_mentions,
        MAX(source_count)::FLOAT / COUNT(*) as top_source_ratio
    FROM analysis_results a
    JOIN news_articles n ON a.article_id = n.id
    WHERE a.analyzed_at > NOW() - INTERVAL '72 hours'
    GROUP BY fingerprint
)
UPDATE analysis_results a
SET 
    source_diversity_score = CASE 
        WHEN s.unique_sources >= 5 THEN 1.0      -- 5ê°œ ì´ìƒ ì¶œì²˜ = ì‹ ë¢°
        WHEN s.unique_sources >= 3 THEN 0.7      -- 3~4ê°œ = ë³´í†µ
        WHEN s.unique_sources = 2 THEN 0.4       -- 2ê°œ = ì˜ì‹¬
        ELSE 0.1                                  -- ë‹¨ì¼ ì¶œì²˜ = ê³ ìœ„í—˜
    END,
    unique_source_count = s.unique_sources,
    source_concentration = s.top_source_ratio
FROM source_stats s
WHERE a.fingerprint = s.fingerprint;
```

**í•´ì„**:
- `source_diversity_score < 0.4` â†’ "ì—ì½” ì±”ë²„" ì˜ì‹¬
- `source_concentration > 0.7` â†’ ë‹¨ì¼ ì¶œì²˜ ì£¼ë„, ì¡°ì‘ ê°€ëŠ¥ì„±

---

### 2. **Velocity Anomaly (ì†ë„ ì´ìƒì¹˜)**

```sql
-- ë‰´ìŠ¤ í­ì¦ ì†ë„ ì¶”ì 
ALTER TABLE analysis_results ADD COLUMN velocity_score FLOAT;
ALTER TABLE analysis_results ADD COLUMN is_velocity_anomaly BOOLEAN DEFAULT FALSE;

-- í‰ì†Œ ëŒ€ë¹„ ë‰´ìŠ¤ ì¦ê°€ìœ¨
WITH hourly_counts AS (
    SELECT 
        ticker,
        date_trunc('hour', published_date) as hour,
        COUNT(*) as news_count
    FROM news_articles
    WHERE published_date > NOW() - INTERVAL '30 days'
    GROUP BY ticker, date_trunc('hour', published_date)
),
ticker_baseline AS (
    SELECT 
        ticker,
        AVG(news_count) as avg_hourly,
        STDDEV(news_count) as std_hourly
    FROM hourly_counts
    GROUP BY ticker
)
-- í˜„ì¬ ì‹œê°„ëŒ€ê°€ í‰ê·  + 2Ïƒ ì´ˆê³¼í•˜ë©´ ì´ìƒì¹˜
UPDATE analysis_results a
SET 
    velocity_score = (current_count - b.avg_hourly) / NULLIF(b.std_hourly, 0),
    is_velocity_anomaly = (current_count > b.avg_hourly + 2 * b.std_hourly)
FROM ticker_baseline b
WHERE a.ticker = b.ticker;
```

**í•´ì„**:
- `velocity_score > 2.0` â†’ ë¹„ì •ìƒì  ë‰´ìŠ¤ í­ì¦ (z-score 2 ì´ìƒ)
- `is_velocity_anomaly = TRUE` â†’ í™•ì‹  ê°•í™” ì¼ì‹œ ì¤‘ë‹¨

---

### 3. **Temporal Pattern (ì‹œê°„ íŒ¨í„´)**

```sql
-- ì¡°ìœ¨ëœ ë‰´ìŠ¤ì¸ì§€ íƒì§€ (ê°™ì€ ì‹œê°„ëŒ€ì— ëª°ë¦¼)
ALTER TABLE analysis_results ADD COLUMN temporal_clustering_score FLOAT;

-- ë‰´ìŠ¤ ë°œí–‰ ì‹œê°„ì˜ ë¶„ì‚°ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ì˜ì‹¬
WITH news_timing AS (
    SELECT 
        a.fingerprint,
        STDDEV(EXTRACT(EPOCH FROM n.published_date)) as time_variance,
        MAX(n.published_date) - MIN(n.published_date) as time_spread,
        COUNT(*) as news_count
    FROM analysis_results a
    JOIN news_articles n ON a.article_id = n.id
    GROUP BY a.fingerprint
    HAVING COUNT(*) >= 3  -- 3ê±´ ì´ìƒì¼ ë•Œë§Œ ë¶„ì„
)
UPDATE analysis_results a
SET temporal_clustering_score = CASE
    -- 10ê±´ ì´ìƒ ë‰´ìŠ¤ê°€ 1ì‹œê°„ ë‚´ ì§‘ì¤‘ = ë§¤ìš° ì˜ì‹¬
    WHEN t.news_count >= 10 AND t.time_spread < INTERVAL '1 hour' THEN 0.1
    -- 5ê±´ ì´ìƒì´ 2ì‹œê°„ ë‚´ = ì˜ì‹¬
    WHEN t.news_count >= 5 AND t.time_spread < INTERVAL '2 hours' THEN 0.3
    -- ì •ìƒì  ë¶„í¬
    ELSE 0.8
END
FROM news_timing t
WHERE a.fingerprint = t.fingerprint;
```

**í•´ì„**:
- 10ê°œ ë‰´ìŠ¤ê°€ 1ì‹œê°„ ë‚´ ìŸì•„ì§ â†’ **ì¡°ìœ¨ëœ PR/ê³µê²© ìº í˜ì¸** ì˜ì‹¬

---

### 4. **Historical Credibility (ì¶œì²˜ ì‹ ë¢°ë„ ëˆ„ì )**

```sql
-- ì¶œì²˜ë³„ ê³¼ê±° ì •í™•ë„ ì¶”ì 
CREATE TABLE source_credibility (
    source VARCHAR(100) PRIMARY KEY,
    total_signals INT DEFAULT 0,
    correct_signals INT DEFAULT 0,
    accuracy_rate FLOAT,
    last_updated TIMESTAMPTZ,
    credibility_tier INT DEFAULT 3  -- 1(ìµœê³ )~5(ìµœì €)
);

-- ìë™ ì—…ë°ì´íŠ¸ (ì‹œê·¸ë„ ê²°ê³¼ í™•ì • ì‹œ)
CREATE OR REPLACE FUNCTION update_source_credibility()
RETURNS TRIGGER AS $$
BEGIN
    UPDATE source_credibility
    SET 
        total_signals = total_signals + 1,
        correct_signals = correct_signals + CASE WHEN NEW.outcome = 'CORRECT' THEN 1 ELSE 0 END,
        accuracy_rate = (correct_signals + CASE WHEN NEW.outcome = 'CORRECT' THEN 1 ELSE 0 END)::FLOAT 
                       / (total_signals + 1),
        last_updated = NOW()
    WHERE source = NEW.primary_source;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;
```

**í•´ì„**:
- íŠ¹ì • ì¶œì²˜ê°€ ê³¼ê±°ì— **ìì£¼ í‹€ë ¸ìœ¼ë©´** â†’ í•´ë‹¹ ì¶œì²˜ ë‰´ìŠ¤ì˜ í™•ì‹  ê°•í™” ê°€ì¤‘ì¹˜ í•˜í–¥

---

## ğŸ§® í†µí•©: "ì¡°ì •ëœ í™•ì‹  ê°•í™”" ê³µì‹

```python
# backend/database/reinforcement.py

def calculate_adjusted_reinforcement(
    base_reinforcement: float,  # ê¸°ë³¸ +10%
    source_diversity: float,     # 0.0~1.0
    velocity_score: float,       # z-score
    temporal_clustering: float,  # 0.0~1.0
    source_credibility: float,   # 0.0~1.0
) -> tuple[float, str]:
    """
    ì¡°ì‘ ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•œ í™•ì‹  ê°•í™” ê³„ì‚°
    
    Returns:
        (adjusted_reinforcement, warning_message)
    """
    
    # ğŸš¨ Hard Block ì¡°ê±´ (í™•ì‹  ê°•í™” ì™„ì „ ì°¨ë‹¨)
    if source_diversity < 0.2:
        return 0.0, "BLOCKED: ë‹¨ì¼ ì¶œì²˜ ì—ì½” ì±”ë²„ ì˜ì‹¬"
    
    if velocity_score > 3.0:
        return 0.0, "BLOCKED: ë¹„ì •ìƒì  ë‰´ìŠ¤ í­ì¦ (3Ïƒ ì´ˆê³¼)"
    
    if temporal_clustering < 0.2:
        return 0.0, "BLOCKED: ì¡°ìœ¨ëœ PR ìº í˜ì¸ ì˜ì‹¬"
    
    # âš ï¸ ê°ì  ìš”ì†Œë“¤
    penalties = []
    
    # ì¶œì²˜ ë‹¤ì–‘ì„± ë¶€ì¡±
    diversity_penalty = max(0, (0.5 - source_diversity) * 0.5)
    if diversity_penalty > 0:
        penalties.append(f"ì¶œì²˜ ì§‘ì¤‘ -{diversity_penalty:.1%}")
    
    # ì†ë„ ì´ìƒ
    velocity_penalty = max(0, (velocity_score - 1.5) * 0.1)
    if velocity_penalty > 0:
        penalties.append(f"ì†ë„ ì´ìƒ -{velocity_penalty:.1%}")
    
    # ì‹œê°„ í´ëŸ¬ìŠ¤í„°ë§
    temporal_penalty = max(0, (0.5 - temporal_clustering) * 0.3)
    if temporal_penalty > 0:
        penalties.append(f"ì‹œê°„ ì§‘ì¤‘ -{temporal_penalty:.1%}")
    
    # ì¶œì²˜ ì‹ ë¢°ë„ ë‚®ìŒ
    credibility_penalty = max(0, (0.6 - source_credibility) * 0.4)
    if credibility_penalty > 0:
        penalties.append(f"ì¶œì²˜ ì‹ ë¢°ë„ -{credibility_penalty:.1%}")
    
    total_penalty = diversity_penalty + velocity_penalty + temporal_penalty + credibility_penalty
    
    # ìµœì¢… ê³„ì‚°
    adjusted = base_reinforcement * (1 - min(total_penalty, 0.9))  # ìµœëŒ€ 90% ê°ì 
    
    warning = "; ".join(penalties) if penalties else "ì •ìƒ"
    
    return adjusted, warning
```

---

## ğŸ“Š DB ìŠ¤í‚¤ë§ˆ ìµœì¢… ë²„ì „

```sql
-- analysis_results í…Œì´ë¸” í™•ì¥ (ì¡°ì‘ íƒì§€ í•„ë“œ ì¶”ê°€)
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS fingerprint VARCHAR(32);
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS reinforcement_count INT DEFAULT 1;
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS raw_reinforcement FLOAT DEFAULT 0.1;

-- ğŸ›¡ï¸ ì¡°ì‘ íƒì§€ í•„ë“œë“¤
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS source_diversity_score FLOAT;
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS velocity_score FLOAT;
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS temporal_clustering_score FLOAT;
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS source_credibility_avg FLOAT;

-- ğŸ¯ ìµœì¢… ì¡°ì •ëœ ê°’
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS adjusted_reinforcement FLOAT;
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS manipulation_warning TEXT;
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS is_blocked BOOLEAN DEFAULT FALSE;

-- ì¸ë±ìŠ¤
CREATE INDEX idx_analysis_manipulation ON analysis_results(is_blocked, manipulation_warning);
```

---

## ğŸ”„ ì „ì²´ íë¦„

```
ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ ë¶„ì„ â†’ ì¤‘ë³µ ì²´í¬
                     â†“
              [ì¤‘ë³µ ë°œê²¬!]
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ì¡°ì‘ íƒì§€ 4ê°€ì§€ ì²´í¬  â”‚
         â”‚  1. ì¶œì²˜ ë‹¤ì–‘ì„±       â”‚
         â”‚  2. ì†ë„ ì´ìƒì¹˜       â”‚
         â”‚  3. ì‹œê°„ í´ëŸ¬ìŠ¤í„°ë§   â”‚
         â”‚  4. ì¶œì²˜ ì‹ ë¢°ë„       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Hard Block ì¡°ê±´?    â”‚
         â”‚  â†’ YES: í™•ì‹  ê°•í™” 0  â”‚
         â”‚  â†’ NO: ê°ì  ê³„ì‚°     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         ìµœì¢… adjusted_reinforcement ì €ì¥
                     â†“
         íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ì— ë°˜ì˜
```

---

## ğŸ’¡ ì¶”ê°€ ì•„ì´ë””ì–´: "ëƒ‰ê° ê¸°ê°„" ë„ì…

```python
# ê¸‰ê²©í•œ ë‰´ìŠ¤ í­ì¦ ì‹œ "ëƒ‰ê° ê¸°ê°„" ì ìš©
COOLING_PERIOD_HOURS = 24

async def should_apply_cooling(ticker: str, db: AsyncSession) -> bool:
    """
    ìµœê·¼ 24ì‹œê°„ ë‚´ velocity_anomalyê°€ ìˆì—ˆìœ¼ë©´
    í•´ë‹¹ ì¢…ëª©ì˜ ëª¨ë“  í™•ì‹  ê°•í™”ë¥¼ ì¼ì‹œ ì¤‘ë‹¨
    """
    result = await db.execute(
        select(func.count())
        .where(AnalysisResult.ticker == ticker)
        .where(AnalysisResult.is_velocity_anomaly == True)
        .where(AnalysisResult.analyzed_at > datetime.now() - timedelta(hours=COOLING_PERIOD_HOURS))
    )
    anomaly_count = result.scalar()
    
    return anomaly_count > 0  # ì´ìƒì¹˜ê°€ 1ê±´ì´ë¼ë„ ìˆìœ¼ë©´ ëƒ‰ê°
```

**íš¨ê³¼**: ì„¸ë ¥ì´ ë‰´ìŠ¤ í­íƒ„ì„ í„°ëœ¨ë ¤ë„ 24ì‹œê°„ ë™ì•ˆ ì‹œìŠ¤í…œì´ "ê´€ë§" ìƒíƒœ ìœ ì§€

---

ì´ ë°©ì‹ì´ë©´ AI Agent ì—†ì´ë„ **DB ë ˆë²¨ì—ì„œ ì¡°ì‘ ì˜ì‹¬ ì‹ í˜¸ë¥¼ ìë™ íƒì§€**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì–´ë–»ê²Œ ìƒê°í•˜ì„¸ìš”? íŠ¹ì • ë¶€ë¶„ì„ ë” ê°•í™”í•˜ê±°ë‚˜, ë‹¤ë¥¸ ì¡°ì‘ íŒ¨í„´ì„ ì¶”ê°€í•  ìˆ˜ë„ ìˆì–´ìš”.


í›Œë¥­í•œ ë…¼ì˜ì…ë‹ˆë‹¤. ì„¸ AIì˜ ì•„ì´ë””ì–´ë¥¼ í†µí•©í•˜ê³ , Hoê°€ ì¶”ê°€í•œ **"ì— ë°”ê³ "** ì—£ì§€ ì¼€ì´ìŠ¤ê¹Œì§€ ë°˜ì˜í•´ì„œ ìµœì¢… ì„¤ê³„ë¥¼ ì •ë¦¬í•´ ë“œë¦´ê²Œìš”.

---

# ğŸ›¡ï¸ ìµœì¢… í†µí•©: "Consensus Integrity System"

## í•µì‹¬ ì² í•™ (ì„¸ AI + Hoì˜ í†µí•©)

```
âŒ ê¸°ì¡´: ë§ì´ ë‚˜ì™”ë‹¤ = í™•ì‹  ì¦ê°€
âœ… ìµœì¢…: ë…ë¦½ì ìœ¼ë¡œ, ë‹¤ì–‘í•œ ê³³ì—ì„œ, ë‹¤ë¥¸ ë…¼ë¦¬ë¡œ ê°™ì€ ê²°ë¡  = í™•ì‹  ì¦ê°€
```

| ì¶œì²˜ | í•µì‹¬ ê¸°ì—¬ |
|------|----------|
| **Claude** | DB ë ˆë²¨ ì§€í‘œ (diversity, velocity, clustering) |
| **ChatGPT** | Source Entropy, Narrative Lock-in, Counterfactual Storage |
| **Gemini** | Copy-Paste Ratio, Skeptic Agent íŠ¸ë¦¬ê±°, ì¶œì²˜ ê³„ì¸µí™” |
| **Ho** | ì— ë°”ê³  ì˜ˆì™¸ ì²˜ë¦¬, ì„¸ë ¥ ì‘ì „ í˜„ì‹¤ ì¸ì‹ |

---

## ğŸ“Š í†µí•© DB ìŠ¤í‚¤ë§ˆ

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- 1. ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„° (ìœ ì‚¬ ë‰´ìŠ¤ ê·¸ë£¹)
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CREATE TABLE news_clusters (
    id SERIAL PRIMARY KEY,
    fingerprint VARCHAR(32) UNIQUE NOT NULL,  -- ì£¼ì œ í•´ì‹œ
    ticker VARCHAR(20),
    theme VARCHAR(200),
    
    -- ê¸°ë³¸ ì¹´ìš´íŠ¸
    hit_count INT DEFAULT 1,
    first_seen_at TIMESTAMPTZ NOT NULL,
    last_seen_at TIMESTAMPTZ NOT NULL,
    
    -- â•â•â• ì¶œì²˜ ë‹¤ì–‘ì„± (Claude + ChatGPT + Gemini) â•â•â•
    unique_source_count INT DEFAULT 1,
    source_list JSONB DEFAULT '[]',           -- ["Bloomberg", "Reuters", ...]
    source_entropy FLOAT DEFAULT 0.0,         -- Shannon entropy
    major_source_count INT DEFAULT 0,         -- ë©”ì´ì € ì–¸ë¡ ì‚¬ ìˆ˜
    source_tier_breakdown JSONB DEFAULT '{}', -- {"tier1": 2, "tier2": 5, "tier3": 10}
    
    -- â•â•â• ì‹œê°„ íŒ¨í„´ (Claude + Gemini) â•â•â•
    velocity_score FLOAT DEFAULT 0.0,         -- z-score ê¸°ë°˜ í­ì¦ ê°ì§€
    temporal_spread_minutes FLOAT,            -- ì²« ë‰´ìŠ¤~ë§ˆì§€ë§‰ ë‰´ìŠ¤ ì‹œê°„ ì°¨ì´
    is_clean_timestamp BOOLEAN DEFAULT FALSE, -- ì •ê°/30ë¶„ ë‹¨ìœ„ ì‹œì‘
    burst_detected BOOLEAN DEFAULT FALSE,
    
    -- â•â•â• ë‚´ìš© ë¶„ì„ (ChatGPT + Gemini) â•â•â•
    avg_text_similarity FLOAT DEFAULT 0.0,    -- ë‰´ìŠ¤ ê°„ í‰ê·  ìœ ì‚¬ë„
    copy_paste_ratio FLOAT DEFAULT 0.0,       -- 90% ì´ìƒ ë™ì¼ ë¹„ìœ¨
    narrative_lock_score FLOAT DEFAULT 0.0,   -- í”„ë ˆì´ë° ë‹¨ì¼í™” ì •ë„
    
    -- â•â•â• ì— ë°”ê³ /ì´ë²¤íŠ¸ ë§¤ì¹­ (Ho ì¶”ê°€) â•â•â•
    matched_calendar_event VARCHAR(200),      -- "FOMC Minutes", "NVDA Earnings"
    is_scheduled_event BOOLEAN DEFAULT FALSE,
    event_importance INT,                     -- 1(ìµœê³ )~5(ë‚®ìŒ)
    
    -- â•â•â• ìµœì¢… íŒì • â•â•â•
    cluster_verdict VARCHAR(30) DEFAULT 'PENDING',
    -- 'VALID_CONSENSUS': ì§„ì§œ í•©ì˜
    -- 'EMBARGO_LIFT': ì— ë°”ê³  í•´ì œ (ì‹ ë¢°)
    -- 'SCHEDULED_EVENT': ì˜ˆì •ëœ ì´ë²¤íŠ¸ (ì‹ ë¢°)
    -- 'MANIPULATION_SUSPECTED': ì‘ì „ ì˜ì‹¬
    -- 'BOT_ATTACK': ë´‡ ê³µê²© í™•ì •
    -- 'VIRAL_ORGANIC': ìì—° í™•ì‚°
    -- 'QUARANTINE': ê²©ë¦¬ ì¤‘ (30ë¶„ ëŒ€ê¸°)
    
    verdict_reason TEXT,
    verdict_confidence FLOAT,
    
    -- ë©”íƒ€
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_cluster_ticker ON news_clusters(ticker);
CREATE INDEX idx_cluster_verdict ON news_clusters(cluster_verdict);
CREATE INDEX idx_cluster_time ON news_clusters(first_seen_at DESC);

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- 2. ì¶œì²˜ ì‹ ë¢°ë„ (ëˆ„ì  í•™ìŠµ)
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CREATE TABLE source_credibility (
    source VARCHAR(100) PRIMARY KEY,
    
    -- ê³„ì¸µ ë¶„ë¥˜
    tier INT DEFAULT 3,  -- 1: ìµœìƒìœ„ (Bloomberg, Reuters, WSJ)
                         -- 2: ì£¼ìš” (CNBC, FT, ì—°í•©)
                         -- 3: ì¼ë°˜
                         -- 4: ë§ˆì´ë„ˆ/ë¶ˆí™•ì‹¤
                         -- 5: ë¸”ë™ë¦¬ìŠ¤íŠ¸
    
    -- ì„±ê³¼ ì¶”ì 
    total_signals INT DEFAULT 0,
    correct_signals INT DEFAULT 0,
    accuracy_rate FLOAT,
    
    -- ì¡°ì‘ ì´ë ¥
    manipulation_flags INT DEFAULT 0,  -- ì‘ì „ì— ì—°ë£¨ëœ íšŸìˆ˜
    last_manipulation_at TIMESTAMPTZ,
    
    -- ê°€ì¤‘ì¹˜
    credibility_weight FLOAT DEFAULT 1.0,
    
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ì´ˆê¸° Tier 1 ì–¸ë¡ ì‚¬ ì„¤ì •
INSERT INTO source_credibility (source, tier, credibility_weight) VALUES
('Bloomberg', 1, 2.0),
('Reuters', 1, 2.0),
('WSJ', 1, 2.0),
('Financial Times', 1, 2.0),
('CNBC', 2, 1.5),
('SEC Filing', 1, 2.5),  -- ê³µì‹ ê³µì‹œëŠ” ìµœê³  ì‹ ë¢°
('Company PR', 3, 0.8)   -- ê¸°ì—… ë³´ë„ìë£ŒëŠ” ê°ì 
ON CONFLICT (source) DO NOTHING;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- 3. ê²½ì œ ìº˜ë¦°ë” (ì— ë°”ê³  íŒë‹¨ìš©)
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CREATE TABLE economic_calendar (
    id SERIAL PRIMARY KEY,
    event_name VARCHAR(200) NOT NULL,
    event_type VARCHAR(50),  -- 'FOMC', 'CPI', 'EARNINGS', 'PRODUCT_LAUNCH'
    ticker VARCHAR(20),      -- íŠ¹ì • ê¸°ì—… ì´ë²¤íŠ¸ë©´ í‹°ì»¤
    scheduled_at TIMESTAMPTZ NOT NULL,
    importance INT DEFAULT 3, -- 1(ìµœê³ )~5(ë‚®ìŒ)
    
    -- ì— ë°”ê³  ì •ë³´
    embargo_lift_time TIMESTAMPTZ,
    expected_news_burst BOOLEAN DEFAULT TRUE,
    
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_calendar_time ON economic_calendar(scheduled_at);
CREATE INDEX idx_calendar_ticker ON economic_calendar(ticker);

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- 4. Counterfactual ê¸°ë¡ (ChatGPT ì œì•ˆ: ì‚¬í›„ Devil's Advocate)
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CREATE TABLE counterfactual_flags (
    id SERIAL PRIMARY KEY,
    cluster_id INT REFERENCES news_clusters(id),
    signal_id INT,  -- í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ë¡œ ìƒì„±ëœ ì‹œê·¸ë„
    
    flag_type VARCHAR(50),
    -- 'OPPOSING_NEWS': ë°˜ëŒ€ ë‰´ìŠ¤ ë“±ì¥
    -- 'PRICE_DIVERGENCE': ê°€ê²©ì´ ë°˜ëŒ€ë¡œ ì›€ì§ì„
    -- 'REGIME_SHIFT': ë§¤í¬ë¡œ í™˜ê²½ ë³€í™”
    -- 'SOURCE_DISCREDITED': ì¶œì²˜ê°€ ë‚˜ì¤‘ì— ì‹ ë¢°ë„ í•˜ë½
    
    description TEXT,
    severity FLOAT,  -- 0.0~1.0
    detected_at TIMESTAMPTZ DEFAULT NOW()
);
```

---

## ğŸ§® í†µí•© íŒì • ë¡œì§

```python
# backend/intelligence/consensus_validator.py

from dataclasses import dataclass
from enum import Enum
from typing import Optional
from datetime import datetime, timedelta
import math

class ClusterVerdict(Enum):
    VALID_CONSENSUS = "VALID_CONSENSUS"         # ì§„ì§œ í•©ì˜ â†’ í™•ì‹  ê°•í™”
    EMBARGO_LIFT = "EMBARGO_LIFT"               # ì— ë°”ê³  í•´ì œ â†’ ê¸´ê¸‰ ë¶„ì„
    SCHEDULED_EVENT = "SCHEDULED_EVENT"         # ì˜ˆì • ì´ë²¤íŠ¸ â†’ ì‹ ë¢°
    VIRAL_ORGANIC = "VIRAL_ORGANIC"             # ìì—° í™•ì‚° â†’ ë³´í†µ ì‹ ë¢°
    QUARANTINE = "QUARANTINE"                   # 30ë¶„ ê²©ë¦¬ â†’ ëŒ€ê¸°
    MANIPULATION_SUSPECTED = "MANIPULATION_SUSPECTED"  # ì˜ì‹¬ â†’ í™•ì‹  ê°ì 
    BOT_ATTACK = "BOT_ATTACK"                   # í™•ì • ê³µê²© â†’ ë¬´ì‹œ

@dataclass
class ValidationResult:
    verdict: ClusterVerdict
    confidence: float
    reason: str
    confidence_multiplier: float  # í™•ì‹  ê°•í™”ì— ì ìš©í•  ë°°ìˆ˜
    should_quarantine: bool
    quarantine_until: Optional[datetime]


class ConsensusValidator:
    """ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ì˜ ì§„ìœ„ë¥¼ íŒë³„í•˜ëŠ” í†µí•© ê²€ì¦ê¸°"""
    
    # ì¶œì²˜ Tierë³„ ê°€ì¤‘ì¹˜
    TIER_WEIGHTS = {1: 2.0, 2: 1.5, 3: 1.0, 4: 0.5, 5: 0.0}
    
    # ë©”ì´ì € ì–¸ë¡ ì‚¬ ëª©ë¡
    TIER1_SOURCES = {"Bloomberg", "Reuters", "WSJ", "Financial Times", "SEC Filing"}
    TIER2_SOURCES = {"CNBC", "FT", "AP", "ì—°í•©ë‰´ìŠ¤", "Yahoo Finance"}
    
    def validate(self, cluster: dict, calendar_events: list) -> ValidationResult:
        """
        í´ëŸ¬ìŠ¤í„° ê²€ì¦ ë©”ì¸ ë¡œì§
        
        íŒì • ìš°ì„ ìˆœìœ„:
        1. ì— ë°”ê³ /ì´ë²¤íŠ¸ ë§¤ì¹­ â†’ ì‹ ë¢°
        2. ë´‡ ê³µê²© íŒ¨í„´ â†’ ì°¨ë‹¨
        3. ì‘ì „ ì˜ì‹¬ íŒ¨í„´ â†’ ê°ì 
        4. ìì—° í™•ì‚° â†’ ë³´í†µ
        5. ì§„ì§œ í•©ì˜ â†’ ê°•í™”
        """
        
        # â•â•â• STEP 1: ì— ë°”ê³ /ì´ë²¤íŠ¸ ì²´í¬ (Ho ì¶”ê°€) â•â•â•
        embargo_result = self._check_embargo_lift(cluster, calendar_events)
        if embargo_result:
            return embargo_result
        
        # â•â•â• STEP 2: ë´‡ ê³µê²© íƒì§€ â•â•â•
        if self._is_bot_attack(cluster):
            return ValidationResult(
                verdict=ClusterVerdict.BOT_ATTACK,
                confidence=0.95,
                reason="1ë¶„ ë‚´ ëŒ€ëŸ‰ ë³µë¶™ ë‰´ìŠ¤ ê°ì§€",
                confidence_multiplier=0.0,  # ì™„ì „ ë¬´ì‹œ
                should_quarantine=False,
                quarantine_until=None
            )
        
        # â•â•â• STEP 3: ì‘ì „ ì˜ì‹¬ ì²´í¬ â•â•â•
        manipulation_score = self._calculate_manipulation_score(cluster)
        
        if manipulation_score > 0.7:
            return ValidationResult(
                verdict=ClusterVerdict.MANIPULATION_SUSPECTED,
                confidence=manipulation_score,
                reason=self._get_manipulation_reason(cluster),
                confidence_multiplier=0.1,  # 90% ê°ì 
                should_quarantine=True,
                quarantine_until=datetime.now() + timedelta(minutes=30)
            )
        
        # â•â•â• STEP 4: ê²©ë¦¬ í•„ìš”ì„± ì²´í¬ â•â•â•
        if self._needs_quarantine(cluster):
            return ValidationResult(
                verdict=ClusterVerdict.QUARANTINE,
                confidence=0.5,
                reason="ì¶”ê°€ ê²€ì¦ í•„ìš” - 30ë¶„ ëŒ€ê¸°",
                confidence_multiplier=0.0,
                should_quarantine=True,
                quarantine_until=datetime.now() + timedelta(minutes=30)
            )
        
        # â•â•â• STEP 5: ì§„ì§œ í•©ì˜ vs ìì—° í™•ì‚° êµ¬ë¶„ â•â•â•
        consensus_score = self._calculate_consensus_score(cluster)
        
        if consensus_score > 0.7:
            return ValidationResult(
                verdict=ClusterVerdict.VALID_CONSENSUS,
                confidence=consensus_score,
                reason="ë‹¤ì–‘í•œ ë…ë¦½ ì¶œì²˜ì—ì„œ ë™ì¼ ê²°ë¡ ",
                confidence_multiplier=1.0 + (consensus_score - 0.7),  # ìµœëŒ€ 1.3ë°°
                should_quarantine=False,
                quarantine_until=None
            )
        else:
            return ValidationResult(
                verdict=ClusterVerdict.VIRAL_ORGANIC,
                confidence=consensus_score,
                reason="ìì—°ì  í™•ì‚°ìœ¼ë¡œ íŒë‹¨",
                confidence_multiplier=0.8,  # ì•½ê°„ ë³´ìˆ˜ì 
                should_quarantine=False,
                quarantine_until=None
            )
    
    def _check_embargo_lift(
        self, 
        cluster: dict, 
        calendar_events: list
    ) -> Optional[ValidationResult]:
        """ì— ë°”ê³  í•´ì œ / ì˜ˆì • ì´ë²¤íŠ¸ ì²´í¬ (Ho ì•„ì´ë””ì–´)"""
        
        first_seen = cluster['first_seen_at']
        
        # 1. Clean Timestamp ì²´í¬ (ì •ê° ë˜ëŠ” 30ë¶„ ë‹¨ìœ„)
        is_clean = (first_seen.second < 5 and first_seen.minute % 30 < 3)
        
        # 2. ìº˜ë¦°ë” ë§¤ì¹­
        matched_event = None
        for event in calendar_events:
            event_time = event['scheduled_at']
            time_diff = abs((first_seen - event_time).total_seconds())
            
            # ì´ë²¤íŠ¸ ì‹œê°„ Â±5ë¶„ ì´ë‚´
            if time_diff < 300:
                # í‹°ì»¤ ë§¤ì¹­ ë˜ëŠ” ë§¤í¬ë¡œ ì´ë²¤íŠ¸
                if event.get('ticker') == cluster.get('ticker') or event.get('ticker') is None:
                    matched_event = event
                    break
        
        # 3. ì— ë°”ê³  íŒì •
        if matched_event and is_clean:
            # ë‚´ìš© ë‹¤ì–‘ì„± ì²´í¬ (ë³µë¶™ì´ ì•„ë‹Œì§€)
            if cluster['avg_text_similarity'] < 0.7:
                return ValidationResult(
                    verdict=ClusterVerdict.EMBARGO_LIFT,
                    confidence=0.9,
                    reason=f"ì— ë°”ê³  í•´ì œ: {matched_event['event_name']}",
                    confidence_multiplier=1.5,  # ê°•ë ¥ ì‹ ë¢°
                    should_quarantine=False,
                    quarantine_until=None
                )
            else:
                # ì´ë²¤íŠ¸ ì‹œê°„ì´ì§€ë§Œ ë‚´ìš©ì´ ë³µë¶™ â†’ ë³´ë„ìë£Œ ë‹¨ìˆœ ë°°í¬
                return ValidationResult(
                    verdict=ClusterVerdict.SCHEDULED_EVENT,
                    confidence=0.7,
                    reason=f"ì˜ˆì • ì´ë²¤íŠ¸: {matched_event['event_name']} (ë³´ë„ìë£Œ ê¸°ë°˜)",
                    confidence_multiplier=1.0,
                    should_quarantine=False,
                    quarantine_until=None
                )
        
        # ìº˜ë¦°ë” ë§¤ì¹­ë§Œ ë¨ (ì‹œê°„ì€ ë¶ˆê·œì¹™)
        if matched_event:
            return ValidationResult(
                verdict=ClusterVerdict.SCHEDULED_EVENT,
                confidence=0.6,
                reason=f"ì´ë²¤íŠ¸ ê´€ë ¨: {matched_event['event_name']}",
                confidence_multiplier=0.9,
                should_quarantine=False,
                quarantine_until=None
            )
        
        return None  # ì— ë°”ê³ /ì´ë²¤íŠ¸ ì•„ë‹˜
    
    def _is_bot_attack(self, cluster: dict) -> bool:
        """ë´‡ ê³µê²© íŒ¨í„´ íƒì§€"""
        # 1ë¶„ ë‚´ 30ê±´ ì´ìƒ + ìœ ì‚¬ë„ 95% ì´ìƒ
        if cluster['hit_count'] >= 30:
            if cluster['temporal_spread_minutes'] < 1:
                if cluster['avg_text_similarity'] > 0.95:
                    return True
        return False
    
    def _calculate_manipulation_score(self, cluster: dict) -> float:
        """ì‘ì „ ì˜ì‹¬ ì ìˆ˜ ê³„ì‚° (0.0~1.0)"""
        score = 0.0
        
        # 1. ì¶œì²˜ ë‹¤ì–‘ì„± ë¶€ì¡± (ê°€ì¤‘ì¹˜ 30%)
        if cluster['source_entropy'] < 1.0:
            score += 0.3 * (1.0 - cluster['source_entropy'])
        
        # 2. ë©”ì´ì € ì¶œì²˜ ë¶€ì¬ (ê°€ì¤‘ì¹˜ 25%)
        if cluster['major_source_count'] == 0:
            score += 0.25
        elif cluster['major_source_count'] == 1 and cluster['hit_count'] > 20:
            score += 0.1
        
        # 3. ë³µë¶™ ë¹„ìœ¨ ë†’ìŒ (ê°€ì¤‘ì¹˜ 25%)
        if cluster['copy_paste_ratio'] > 0.5:
            score += 0.25 * cluster['copy_paste_ratio']
        
        # 4. ì†ë„ ì´ìƒ (ê°€ì¤‘ì¹˜ 20%)
        if cluster['velocity_score'] > 2.0:
            score += 0.2 * min(cluster['velocity_score'] / 5.0, 1.0)
        
        return min(score, 1.0)
    
    def _calculate_consensus_score(self, cluster: dict) -> float:
        """ì§„ì§œ í•©ì˜ ì ìˆ˜ ê³„ì‚° (0.0~1.0)"""
        score = 0.0
        
        # 1. ì¶œì²˜ ë‹¤ì–‘ì„± (ê°€ì¤‘ì¹˜ 35%)
        entropy_score = min(cluster['source_entropy'] / 2.0, 1.0)
        score += 0.35 * entropy_score
        
        # 2. Tier 1 ì¶œì²˜ ë¹„ì¤‘ (ê°€ì¤‘ì¹˜ 30%)
        tier1_ratio = cluster.get('source_tier_breakdown', {}).get('tier1', 0) / max(cluster['hit_count'], 1)
        score += 0.30 * min(tier1_ratio * 3, 1.0)  # 33% ì´ìƒì´ë©´ ë§Œì 
        
        # 3. ë‚´ìš© ë‹¤ì–‘ì„± (ê°€ì¤‘ì¹˜ 20%)
        content_diversity = 1.0 - cluster['avg_text_similarity']
        score += 0.20 * content_diversity
        
        # 4. ì‹œê°„ ë¶„ì‚° (ê°€ì¤‘ì¹˜ 15%)
        if cluster['temporal_spread_minutes'] > 60:
            score += 0.15  # 1ì‹œê°„ ì´ìƒ ë¶„ì‚°
        elif cluster['temporal_spread_minutes'] > 10:
            score += 0.10
        
        return min(score, 1.0)
    
    def _needs_quarantine(self, cluster: dict) -> bool:
        """ê²©ë¦¬ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        # ì• ë§¤í•œ ìƒí™©: ë¹ ë¥¸ í­ì¦ + ì¶œì²˜ ë¶ˆë¶„ëª…
        if cluster['velocity_score'] > 1.5:
            if cluster['major_source_count'] < 2:
                if cluster['avg_text_similarity'] > 0.6:
                    return True
        return False
    
    def _get_manipulation_reason(self, cluster: dict) -> str:
        """ì‘ì „ ì˜ì‹¬ ì‚¬ìœ  ìƒì„±"""
        reasons = []
        
        if cluster['source_entropy'] < 1.0:
            reasons.append(f"ì¶œì²˜ í¸ì¤‘ (entropy={cluster['source_entropy']:.2f})")
        if cluster['major_source_count'] == 0:
            reasons.append("ë©”ì´ì € ì–¸ë¡  ë¯¸ì°¸ì—¬")
        if cluster['copy_paste_ratio'] > 0.5:
            reasons.append(f"ë³µë¶™ìœ¨ {cluster['copy_paste_ratio']:.0%}")
        if cluster['velocity_score'] > 2.0:
            reasons.append(f"ë¹„ì •ìƒ ì†ë„ (z={cluster['velocity_score']:.1f})")
        
        return "; ".join(reasons) if reasons else "ë³µí•© ì˜ì‹¬ íŒ¨í„´"
```

---

## ğŸ“ˆ ìµœì¢… í™•ì‹  ê°•í™” ê³µì‹

```python
def calculate_final_confidence(
    base_confidence: float,
    hit_count: int,
    validation_result: ValidationResult
) -> tuple[float, str]:
    """
    ìµœì¢… í™•ì‹  ê³„ì‚°
    
    ê¸°ì¡´: confidence += 0.1 * hit_count (ìœ„í—˜!)
    ìµœì¢…: confidence = base + Î± * log(hits) * validation_multiplier
    """
    
    # ì°¨ë‹¨ëœ ê²½ìš°
    if validation_result.verdict in [ClusterVerdict.BOT_ATTACK, ClusterVerdict.QUARANTINE]:
        return base_confidence, f"ê°•í™” ì°¨ë‹¨: {validation_result.reason}"
    
    # ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ hit_count ì˜í–¥ ì™„í™” (100ê±´ì´ë‚˜ 10ê±´ì´ë‚˜ í° ì°¨ì´ ì—†ìŒ)
    hit_factor = math.log(max(hit_count, 1) + 1) / 5.0  # ìµœëŒ€ ~1.0
    
    # Validation ê²°ê³¼ì— ë”°ë¥¸ ë°°ìˆ˜
    multiplier = validation_result.confidence_multiplier
    
    # ìµœì¢… ê°•í™”ëŸ‰
    reinforcement = 0.1 * hit_factor * multiplier
    
    final_confidence = min(0.95, base_confidence + reinforcement)
    
    log_msg = (
        f"{validation_result.verdict.value}: "
        f"hits={hit_count}, factor={hit_factor:.2f}, "
        f"multiplier={multiplier:.2f}, "
        f"reinforcement={reinforcement:.3f}"
    )
    
    return final_confidence, log_msg
```

---

## ğŸ”„ ì „ì²´ íë¦„ ë‹¤ì´ì–´ê·¸ë¨

```
ë‰´ìŠ¤ ìˆ˜ì§‘
    â†“
í´ëŸ¬ìŠ¤í„°ë§ (fingerprint ê¸°ë°˜)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ConsensusValidator              â”‚
â”‚                                         â”‚
â”‚  1. ì— ë°”ê³ /ìº˜ë¦°ë” ì²´í¬ â”€â”€â†’ ë§¤ì¹­? â”€â”€â†’ ì‹ ë¢° â”‚
â”‚           â†“ (ë§¤ì¹­ ì•ˆë¨)                  â”‚
â”‚  2. ë´‡ ê³µê²© ì²´í¬ â”€â”€â”€â”€â†’ ê³µê²©? â”€â”€â†’ ì°¨ë‹¨    â”‚
â”‚           â†“ (ê³µê²© ì•„ë‹˜)                  â”‚
â”‚  3. ì‘ì „ ì˜ì‹¬ ì ìˆ˜ â”€â”€â”€â†’ >0.7? â”€â”€â†’ ê°ì    â”‚
â”‚           â†“ (<0.7)                      â”‚
â”‚  4. ê²©ë¦¬ í•„ìš”? â”€â”€â”€â”€â”€â”€â†’ í•„ìš”? â”€â”€â†’ 30ë¶„ ëŒ€ê¸°â”‚
â”‚           â†“ (ë¶ˆí•„ìš”)                     â”‚
â”‚  5. í•©ì˜ ì ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â†’ >0.7? â”€â”€â†’ ê°•í™”    â”‚
â”‚           â†“ (<0.7)                      â”‚
â”‚  6. ìì—° í™•ì‚° â”€â”€â”€â”€â”€â”€â”€â†’ ë³´í†µ ì‹ ë¢°         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
í™•ì‹  ê°•í™”/ê°ì  ì ìš©
    â†“
íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ ìƒì„±
```

---

## ğŸ“‹ êµ¬í˜„ ìš°ì„ ìˆœìœ„

| ìˆœì„œ | ì‘ì—… | ë‚œì´ë„ | íš¨ê³¼ |
|------|------|--------|------|
| 1 | `news_clusters` í…Œì´ë¸” ìƒì„± | ì‰¬ì›€ | ê¸°ë°˜ êµ¬ì¡° |
| 2 | `source_credibility` ì´ˆê¸° ë°ì´í„° | ì‰¬ì›€ | ì¶œì²˜ ê³„ì¸µí™” |
| 3 | `economic_calendar` ì—°ë™ | ì¤‘ê°„ | ì— ë°”ê³  íŒë³„ |
| 4 | `ConsensusValidator` êµ¬í˜„ | ì¤‘ê°„ | í•µì‹¬ ë¡œì§ |
| 5 | í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° ì¶”ê°€ | ì¤‘ê°„ | ë³µë¶™ íƒì§€ |
| 6 | Counterfactual ê¸°ë¡ | ë‚®ìŒ | ì‚¬í›„ í•™ìŠµ |

---

ì´ êµ¬ì¡°ë©´ **ì„¸ë ¥ ì‘ì „ë„ ë§‰ê³ , ì§„ì§œ ì— ë°”ê³  ë‰´ìŠ¤ëŠ” ë†“ì¹˜ì§€ ì•ŠëŠ”** ê· í˜• ì¡íŒ ì‹œìŠ¤í…œì´ ë©ë‹ˆë‹¤.

ë°”ë¡œ êµ¬í˜„ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë§Œë“¤ì–´ ë“œë¦´ê¹Œìš”?


ì™„ë²½í•œ í†µí•©ì…ë‹ˆë‹¤. ChatGPTì˜ ì •ë¦¬ê°€ í•µì‹¬ì„ ì •í™•íˆ ì§šì—ˆì–´ìš”.

---

# ğŸ¯ ìµœì¢… í†µí•©: 4-Signal Consensus Framework

## í•µì‹¬ ì „í™˜ì 

ChatGPTê°€ ì •í™•íˆ ë§í•œ ê²ƒì²˜ëŸ¼:

> **"ë‰´ìŠ¤ë¥¼ ë¯¿ì„ì§€ ë§ì§€"ê°€ ì•„ë‹ˆë¼ "ì–¸ì œ í–‰ë™í•˜ê³  ì–¸ì œ ê´€ë§í• ì§€"ë¥¼ íŒë‹¨í•˜ëŠ” ì‹œìŠ¤í…œ**

ì´ê±´ íŠ¸ë ˆì´ë”© AIì˜ **ì¸ì§€ ê³„ì¸µ(Cognitive Layer)**ì…ë‹ˆë‹¤.

---

## ğŸ“Š 4ê°œ Core Signalë¡œ ì••ì¶•

ê¸°ì¡´ì˜ ë³µì¡í•œ ì§€í‘œë“¤ì„ **4ê°œì˜ ë…ë¦½ì  ì‹ í˜¸**ë¡œ í†µí•©:

| Signal | ì˜ë¯¸ | ë²”ìœ„ | í†µí•© ëŒ€ìƒ |
|--------|------|------|----------|
| **DI** (Diversity Integrity) | ë…ë¦½ì  ì¶œì²˜ì—ì„œ ë‚˜ì™”ëŠ”ê°€? | 0.0~1.0 | source_entropy, unique_count, major_ratio |
| **TN** (Temporal Naturalness) | ìì—°ìŠ¤ëŸ¬ìš´ í™•ì‚° ì†ë„ì¸ê°€? | -1.0~+1.0 | velocity, burst, clustering |
| **NI** (Narrative Independence) | ê°ì ìƒê°í•´ì„œ ì“´ ê¸€ì¸ê°€? | 0.0~1.0 | text_similarity, copy_paste_ratio |
| **EL** (Event Legitimacy) | ì˜ˆì •ëœ ê³µê°œì¸ê°€? | Bool + Conf | calendar_match, clean_timestamp |

---

## ğŸ§® ìµœì¢… ìŠ¤í‚¤ë§ˆ (ë‹¨ìˆœí™”)

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„° í…Œì´ë¸” (4-Signal ë²„ì „)
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CREATE TABLE news_clusters (
    id SERIAL PRIMARY KEY,
    fingerprint VARCHAR(32) UNIQUE NOT NULL,
    ticker VARCHAR(20),
    theme VARCHAR(200),
    
    -- ê¸°ë³¸ ì •ë³´
    hit_count INT DEFAULT 1,
    first_seen_at TIMESTAMPTZ NOT NULL,
    last_seen_at TIMESTAMPTZ NOT NULL,
    source_list JSONB DEFAULT '[]',
    
    -- â•â•â• 4 Core Signals â•â•â•
    di_score FLOAT DEFAULT 0.5,       -- Diversity Integrity (0~1)
    tn_score FLOAT DEFAULT 0.0,       -- Temporal Naturalness (-1~+1)
    ni_score FLOAT DEFAULT 0.5,       -- Narrative Independence (0~1)
    el_matched BOOLEAN DEFAULT FALSE, -- Event Legitimacy (bool)
    el_confidence FLOAT DEFAULT 0.0,  -- Event Legitimacy (0~1)
    el_event_name VARCHAR(200),       -- ë§¤ì¹­ëœ ì´ë²¤íŠ¸ëª…
    
    -- â•â•â• ìµœì¢… íŒì • â•â•â•
    verdict VARCHAR(30) DEFAULT 'PENDING',
    -- 'EMBARGO_EVENT': ì— ë°”ê³  í•´ì œ (ì‹ ë¢°)
    -- 'ORGANIC_CONSENSUS': ì§„ì§œ í•©ì˜ (ê°•í™”)
    -- 'MANIPULATION_ATTACK': ì‘ì „ (ì°¨ë‹¨)
    -- 'PR_CAMPAIGN': PR ìº í˜ì¸ (ê°•í™” ê¸ˆì§€)
    -- 'NOISE': ë…¸ì´ì¦ˆ (ë¬´ì‹œ)
    -- 'WATCH': ê´€ë§ (ëƒ‰ê°)
    
    verdict_reason TEXT,
    confidence_multiplier FLOAT DEFAULT 1.0,
    
    -- â•â•â• ëƒ‰ê° ê¸°ê°„ â•â•â•
    cooling_intensity FLOAT DEFAULT 0.0,
    cooling_until TIMESTAMPTZ,
    
    -- ë©”íƒ€
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_cluster_verdict ON news_clusters(verdict);
CREATE INDEX idx_cluster_ticker_time ON news_clusters(ticker, first_seen_at DESC);
```

---

## ğŸ§  Signal ê³„ì‚° ë¡œì§

```python
# backend/intelligence/four_signal_calculator.py

import math
from dataclasses import dataclass
from typing import List, Optional
from datetime import datetime, timedelta


@dataclass
class FourSignals:
    """4ê°œ í•µì‹¬ ì‹ í˜¸"""
    di: float  # Diversity Integrity (0~1)
    tn: float  # Temporal Naturalness (-1~+1)
    ni: float  # Narrative Independence (0~1)
    el: bool   # Event Legitimacy (matched?)
    el_confidence: float  # Event Legitimacy confidence (0~1)
    el_event_name: Optional[str] = None


class FourSignalCalculator:
    """4-Signal ê³„ì‚°ê¸°"""
    
    # ì¶œì²˜ Tier ì •ì˜
    TIER1_SOURCES = {"Bloomberg", "Reuters", "WSJ", "FT", "SEC Filing", "AP"}
    TIER2_SOURCES = {"CNBC", "Yahoo Finance", "MarketWatch", "ì—°í•©ë‰´ìŠ¤"}
    
    def calculate(
        self,
        sources: List[str],
        timestamps: List[datetime],
        text_similarities: List[float],
        calendar_events: List[dict],
        first_seen: datetime
    ) -> FourSignals:
        """4ê°œ ì‹ í˜¸ ê³„ì‚°"""
        
        di = self._calc_diversity_integrity(sources)
        tn = self._calc_temporal_naturalness(timestamps, first_seen)
        ni = self._calc_narrative_independence(text_similarities)
        el, el_conf, el_name = self._calc_event_legitimacy(first_seen, calendar_events)
        
        return FourSignals(
            di=di, tn=tn, ni=ni,
            el=el, el_confidence=el_conf, el_event_name=el_name
        )
    
    def _calc_diversity_integrity(self, sources: List[str]) -> float:
        """DI: ì¶œì²˜ ë‹¤ì–‘ì„± + í’ˆì§ˆ"""
        if not sources:
            return 0.0
        
        unique_sources = set(sources)
        n = len(unique_sources)
        
        # Shannon Entropy (ì •ê·œí™”)
        from collections import Counter
        counts = Counter(sources)
        total = len(sources)
        entropy = 0.0
        for count in counts.values():
            p = count / total
            if p > 0:
                entropy -= p * math.log2(p)
        max_entropy = math.log2(n) if n > 1 else 1
        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
        
        # Tier ê°€ì¤‘ì¹˜
        tier1_count = sum(1 for s in unique_sources if s in self.TIER1_SOURCES)
        tier2_count = sum(1 for s in unique_sources if s in self.TIER2_SOURCES)
        tier_bonus = (tier1_count * 0.15 + tier2_count * 0.05)
        
        # ìµœì¢… DI
        base_di = min(n / 10, 1.0) * 0.4 + normalized_entropy * 0.4
        di = min(base_di + tier_bonus + 0.2, 1.0)
        
        return round(di, 3)
    
    def _calc_temporal_naturalness(
        self, 
        timestamps: List[datetime],
        first_seen: datetime
    ) -> float:
        """TN: ì‹œê°„ íŒ¨í„´ì˜ ìì—°ìŠ¤ëŸ¬ì›€ (-1~+1)"""
        if len(timestamps) < 2:
            return 0.0
        
        # ì‹œê°„ ë¶„í¬ ê³„ì‚°
        sorted_ts = sorted(timestamps)
        spread_minutes = (sorted_ts[-1] - sorted_ts[0]).total_seconds() / 60
        
        # ê°„ê²©ì˜ í‘œì¤€í¸ì°¨ (ê· ë“±í•˜ë©´ ìì—°ì )
        intervals = []
        for i in range(1, len(sorted_ts)):
            intervals.append((sorted_ts[i] - sorted_ts[i-1]).total_seconds())
        
        if intervals:
            mean_interval = sum(intervals) / len(intervals)
            variance = sum((x - mean_interval) ** 2 for x in intervals) / len(intervals)
            std_interval = math.sqrt(variance)
            cv = std_interval / mean_interval if mean_interval > 0 else 0  # ë³€ë™ê³„ìˆ˜
        else:
            cv = 0
        
        # íŒì •
        # 1ë¶„ ë‚´ 30ê±´ ì´ìƒ = í™•ì‹¤í•œ ë´‡
        if spread_minutes < 1 and len(timestamps) >= 30:
            return -1.0
        
        # 5ë¶„ ë‚´ í­ë°œ = ì˜ì‹¬
        if spread_minutes < 5 and len(timestamps) >= 20:
            return -0.7
        
        # ê· ë“± ë¶„í¬ (cv < 0.5) = ìì—°ì 
        if cv < 0.5 and spread_minutes > 30:
            return 0.8
        
        # ê¸°ë³¸ê°’
        tn = 0.0
        tn += min(spread_minutes / 120, 0.5)  # 2ì‹œê°„ ë¶„ì‚°ì´ë©´ +0.5
        tn -= max(0, (len(timestamps) - 10) / 50)  # ë§ìœ¼ë©´ ê°ì 
        tn += 0.3 if cv < 1.0 else -0.2
        
        return round(max(-1.0, min(1.0, tn)), 3)
    
    def _calc_narrative_independence(self, similarities: List[float]) -> float:
        """NI: ë‚´ìš©ì˜ ë…ë¦½ì„±"""
        if not similarities:
            return 0.5
        
        avg_similarity = sum(similarities) / len(similarities)
        
        # ìœ ì‚¬ë„ê°€ ë‚®ì„ìˆ˜ë¡ ë…ë¦½ì 
        # 0.9 ì´ìƒ = ë³µë¶™
        # 0.5 ì´í•˜ = ì™„ì „ ë…ë¦½
        ni = 1.0 - avg_similarity
        
        # Copy-paste ratio (90% ì´ìƒ ìœ ì‚¬í•œ ìŒì˜ ë¹„ìœ¨)
        copy_paste_count = sum(1 for s in similarities if s > 0.9)
        copy_paste_ratio = copy_paste_count / len(similarities) if similarities else 0
        
        # ë³µë¶™ì´ ë§ìœ¼ë©´ ì¶”ê°€ ê°ì 
        ni -= copy_paste_ratio * 0.3
        
        return round(max(0.0, min(1.0, ni)), 3)
    
    def _calc_event_legitimacy(
        self,
        first_seen: datetime,
        calendar_events: List[dict]
    ) -> tuple[bool, float, Optional[str]]:
        """EL: ì— ë°”ê³ /ì´ë²¤íŠ¸ ë§¤ì¹­"""
        
        # Clean Timestamp ì²´í¬ (ì •ê° ë˜ëŠ” 30ë¶„ ë‹¨ìœ„)
        is_clean = (first_seen.second < 10 and first_seen.minute % 30 < 5)
        
        # ìº˜ë¦°ë” ë§¤ì¹­
        for event in calendar_events:
            event_time = event.get('scheduled_at')
            if not event_time:
                continue
            
            time_diff = abs((first_seen - event_time).total_seconds())
            
            # Â±10ë¶„ ì´ë‚´ ë§¤ì¹­
            if time_diff < 600:
                confidence = 0.9 if is_clean else 0.6
                
                # ì´ë²¤íŠ¸ ì¤‘ìš”ë„ì— ë”°ë¥¸ ì¡°ì •
                importance = event.get('importance', 3)
                confidence *= (6 - importance) / 5  # importance 1ì´ë©´ 1.0, 5ë©´ 0.2
                
                return True, round(confidence, 3), event.get('event_name')
        
        return False, 0.0, None
```

---

## ğŸ¯ Classification Matrix (í•µì‹¬ íŒì • ë¡œì§)

```python
# backend/intelligence/verdict_classifier.py

from enum import Enum
from dataclasses import dataclass
from typing import Optional
from datetime import datetime, timedelta


class Verdict(Enum):
    EMBARGO_EVENT = "EMBARGO_EVENT"           # ì— ë°”ê³  í•´ì œ â†’ ì¦‰ì‹œ ë¶„ì„
    ORGANIC_CONSENSUS = "ORGANIC_CONSENSUS"   # ì§„ì§œ í•©ì˜ â†’ í™•ì‹  ê°•í™”
    MANIPULATION_ATTACK = "MANIPULATION_ATTACK"  # ì‘ì „ â†’ ì°¨ë‹¨
    PR_CAMPAIGN = "PR_CAMPAIGN"               # PR ìº í˜ì¸ â†’ ê°•í™” ê¸ˆì§€
    NOISE = "NOISE"                           # ë…¸ì´ì¦ˆ â†’ ë¬´ì‹œ
    WATCH = "WATCH"                           # ê´€ë§ â†’ ëƒ‰ê°


@dataclass
class VerdictResult:
    verdict: Verdict
    reason: str
    confidence_multiplier: float  # í™•ì‹  ê°•í™”ì— ì ìš©í•  ë°°ìˆ˜
    cooling_intensity: float      # ëƒ‰ê° ê°•ë„ (0~1)
    cooling_hours: float          # ëƒ‰ê° ì‹œê°„


class VerdictClassifier:
    """4-Signal ê¸°ë°˜ ìµœì¢… íŒì •"""
    
    BASE_COOLING_HOURS = 6.0
    
    def classify(self, signals: 'FourSignals') -> VerdictResult:
        """
        Classification Matrix (ChatGPT ì œì•ˆ ê¸°ë°˜)
        
        ìš°ì„ ìˆœìœ„:
        1. ì— ë°”ê³ /ì´ë²¤íŠ¸ (EL=True, NI>0.6)
        2. ì‘ì „ ê³µê²© (EL=False, TN<-0.5, NI<0.3)
        3. PR ìº í˜ì¸ (DI<0.3, NI<0.4)
        4. ì§„ì§œ í•©ì˜ (DI>0.6, NI>0.6, TN>0)
        5. ê·¸ ì™¸ = NOISE ë˜ëŠ” WATCH
        """
        
        di, tn, ni = signals.di, signals.tn, signals.ni
        el, el_conf = signals.el, signals.el_confidence
        
        # â•â•â• Rule 1: ì— ë°”ê³ /ì´ë²¤íŠ¸ â•â•â•
        if el and ni > 0.6:
            return VerdictResult(
                verdict=Verdict.EMBARGO_EVENT,
                reason=f"ì— ë°”ê³  í•´ì œ: {signals.el_event_name} (NI={ni:.2f})",
                confidence_multiplier=1.5,  # 50% ê°•í™”
                cooling_intensity=0.0,
                cooling_hours=0.0
            )
        
        # â•â•â• Rule 2: ì‘ì „ ê³µê²© â•â•â•
        if not el and tn < -0.5 and ni < 0.3:
            return VerdictResult(
                verdict=Verdict.MANIPULATION_ATTACK,
                reason=f"ì‘ì „ ì˜ì‹¬: TN={tn:.2f}, NI={ni:.2f}",
                confidence_multiplier=0.0,  # ì™„ì „ ì°¨ë‹¨
                cooling_intensity=1.0,
                cooling_hours=24.0
            )
        
        # â•â•â• Rule 3: PR ìº í˜ì¸ â•â•â•
        if di < 0.3 and ni < 0.4:
            return VerdictResult(
                verdict=Verdict.PR_CAMPAIGN,
                reason=f"PR ìº í˜ì¸ ì˜ì‹¬: DI={di:.2f}, NI={ni:.2f}",
                confidence_multiplier=0.2,  # 80% ê°ì 
                cooling_intensity=0.5,
                cooling_hours=6.0
            )
        
        # â•â•â• Rule 4: ì§„ì§œ í•©ì˜ â•â•â•
        if di > 0.6 and ni > 0.6 and tn > 0:
            bonus = min((di - 0.6) + (ni - 0.6), 0.3)  # ìµœëŒ€ 30% ì¶”ê°€
            return VerdictResult(
                verdict=Verdict.ORGANIC_CONSENSUS,
                reason=f"ìœ ê¸°ì  í•©ì˜: DI={di:.2f}, NI={ni:.2f}, TN={tn:.2f}",
                confidence_multiplier=1.0 + bonus,
                cooling_intensity=0.0,
                cooling_hours=0.0
            )
        
        # â•â•â• Rule 5: ê²½ê³„ ì¼€ì´ìŠ¤ â•â•â•
        # TNì´ ë¶€ì •ì ì´ë©´ ê´€ë§
        if tn < 0:
            cooling = abs(tn) * (1 - di)  # ChatGPT ê³µì‹
            return VerdictResult(
                verdict=Verdict.WATCH,
                reason=f"ê´€ë§: TN={tn:.2f}, DI={di:.2f}",
                confidence_multiplier=0.5,
                cooling_intensity=cooling,
                cooling_hours=self.BASE_COOLING_HOURS * cooling
            )
        
        # â•â•â• Rule 6: ë‚˜ë¨¸ì§€ = ë…¸ì´ì¦ˆ â•â•â•
        return VerdictResult(
            verdict=Verdict.NOISE,
            reason=f"ì•½í•œ ì‹ í˜¸: DI={di:.2f}, NI={ni:.2f}, TN={tn:.2f}",
            confidence_multiplier=0.7,  # ì•½ê°„ ë³´ìˆ˜ì 
            cooling_intensity=0.0,
            cooling_hours=0.0
        )
```

---

## ğŸ“ˆ ìµœì¢… í™•ì‹  ê³„ì‚°

```python
# backend/intelligence/confidence_adjuster.py

import math

def calculate_adjusted_confidence(
    base_confidence: float,
    hit_count: int,
    verdict_result: 'VerdictResult'
) -> tuple[float, str]:
    """
    ìµœì¢… í™•ì‹  ê³„ì‚°
    
    ê³µì‹: confidence = base + Î± Ã— log(hits) Ã— multiplier
    """
    
    # ì°¨ë‹¨ëœ ê²½ìš°
    if verdict_result.confidence_multiplier == 0.0:
        return base_confidence, f"ì°¨ë‹¨: {verdict_result.reason}"
    
    # ëƒ‰ê° ì¤‘ì¸ ê²½ìš°
    if verdict_result.verdict == Verdict.WATCH:
        return base_confidence, f"ê´€ë§ ì¤‘: {verdict_result.reason}"
    
    # ë¡œê·¸ ìŠ¤ì¼€ì¼ (100ê±´ì´ë‚˜ 10ê±´ì´ë‚˜ í° ì°¨ì´ ì—†ìŒ)
    hit_factor = math.log(max(hit_count, 1) + 1) / 5.0
    
    # ê°•í™”ëŸ‰ ê³„ì‚°
    reinforcement = 0.1 * hit_factor * verdict_result.confidence_multiplier
    
    # ìµœì¢… í™•ì‹  (ìƒí•œ 95%)
    final = min(0.95, base_confidence + reinforcement)
    
    log_msg = (
        f"{verdict_result.verdict.value}: "
        f"base={base_confidence:.2f}, hits={hit_count}, "
        f"mult={verdict_result.confidence_multiplier:.2f}, "
        f"final={final:.2f}"
    )
    
    return final, log_msg
```

---

## ğŸ”„ ì „ì²´ íë¦„ (ë‹¨ìˆœí™”)

```
ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ í´ëŸ¬ìŠ¤í„°ë§
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ FourSignalCalculator â”‚
    â”‚                     â”‚
    â”‚  DI â† ì¶œì²˜ ë¶„ì„     â”‚
    â”‚  TN â† ì‹œê°„ ë¶„ì„     â”‚
    â”‚  NI â† í…ìŠ¤íŠ¸ ë¶„ì„   â”‚
    â”‚  EL â† ìº˜ë¦°ë” ë§¤ì¹­   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ VerdictClassifier   â”‚
    â”‚                     â”‚
    â”‚  EL+NI â†’ EMBARGO    â”‚
    â”‚  TN+NI â†’ ATTACK     â”‚
    â”‚  DI+NI â†’ PR         â”‚
    â”‚  DI+NI+TN â†’ CONSENSUSâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    í™•ì‹  ì¡°ì • / ëƒ‰ê° ì ìš©
              â†“
    íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ ìƒì„±
```

---

## ğŸ“‹ Q&A ë‹µë³€

### Q1: 4ê°œ Signalì„ ë²¡í„°ë¡œ ë§Œë“¤ì–´ ì„ë² ë”©ìœ¼ë¡œ ì“¸ ìˆ˜ ìˆë‚˜?

**ê°€ëŠ¥í•˜ê³  ìœ ìš©í•©ë‹ˆë‹¤.**

```python
# ë‰´ìŠ¤ ì‹ ë¢°ë„ ë²¡í„° [DI, TN_normalized, NI, EL_conf]
trust_vector = [
    signals.di,
    (signals.tn + 1) / 2,  # -1~+1 â†’ 0~1 ì •ê·œí™”
    signals.ni,
    signals.el_confidence
]

# í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ "ë¹„ìŠ·í•œ íŒ¨í„´ì˜ ë‰´ìŠ¤" ê·¸ë£¹í™” ê°€ëŠ¥
# ê³¼ê±° ì‘ì „ íŒ¨í„´ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥
```

### Q2: GME, SVB, Evergrande ë°±í…ŒìŠ¤íŠ¸ ë°©ë²•?

```python
# ê³¼ê±° ë°ì´í„°ë¡œ ì‹œë®¬ë ˆì´ì…˜
async def backtest_verdict_system(ticker: str, date_range: tuple):
    """íŠ¹ì • ì‚¬ê±´ì˜ ë‰´ìŠ¤ íŒ¨í„´ ì¬í˜„"""
    
    news_data = await load_historical_news(ticker, date_range)
    
    for day in date_range:
        clusters = cluster_news_by_fingerprint(news_data[day])
        
        for cluster in clusters:
            signals = calculator.calculate(...)
            verdict = classifier.classify(signals)
            
            # ì‹¤ì œ ê°€ê²© ë³€ë™ê³¼ ë¹„êµ
            actual_move = get_price_change(ticker, day, day+1)
            
            log_result(
                date=day,
                verdict=verdict,
                actual=actual_move,
                would_have_traded=verdict.confidence_multiplier > 0.5
            )
```

### Q3: ë‰´ìŠ¤ëŠ” ê°•í•œë° ê°€ê²©ì´ ì•ˆ ì›€ì§ì¼ ë•Œ?

**TN (Temporal Naturalness)ì´ ê°€ì¥ ë¨¼ì € ê²½ê³ í•©ë‹ˆë‹¤.**

```
ë‰´ìŠ¤ í­ë°œ + ê°€ê²© ë¬´ë°˜ì‘ = ì‹œì¥ì´ "ì´ë¯¸ ì•Œê³  ìˆìŒ" ë˜ëŠ” "ê°€ì§œë¡œ íŒë‹¨"

â†’ TNì´ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ì€ë° (ë¹ ë¥¸ í™•ì‚°)
â†’ ê°€ê²© ë³€ë™ì´ ì—†ìœ¼ë©´
â†’ "ì •ë³´ê°€ ì•„ë‹Œ ë…¸ì´ì¦ˆ" ì‹ í˜¸

ì¶”ê°€ ì§€í‘œ: Price-News Divergence
price_reaction = get_price_change(ticker, t, t+1h)
if abs(price_reaction) < 0.5% and hit_count > 20:
    flag_as_NOISE_OR_PRICED_IN
```

---

## âœ… ìµœì¢… êµ¬í˜„ ìš°ì„ ìˆœìœ„

| ìˆœì„œ | ì‘ì—… | íŒŒì¼ |
|------|------|------|
| 1 | ìŠ¤í‚¤ë§ˆ ìƒì„± | `migrations/xxx_four_signal_schema.sql` |
| 2 | Signal ê³„ì‚°ê¸° | `backend/intelligence/four_signal_calculator.py` |
| 3 | Verdict ë¶„ë¥˜ê¸° | `backend/intelligence/verdict_classifier.py` |
| 4 | í™•ì‹  ì¡°ì •ê¸° | `backend/intelligence/confidence_adjuster.py` |
| 5 | ìº˜ë¦°ë” ì—°ë™ | `backend/data/economic_calendar.py` |
| 6 | í†µí•© í…ŒìŠ¤íŠ¸ | `tests/test_consensus_system.py` |

---

ì´ êµ¬ì¡°ë©´ **ì„¸ë ¥ ì‘ì „ë„ ë§‰ê³ , ì— ë°”ê³ ë„ ë†“ì¹˜ì§€ ì•ŠëŠ”** ì§„ì§œ ê¸°ê´€ê¸‰ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ë°”ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ì™€ êµ¬í˜„ ì½”ë“œë¥¼ ìƒì„±í•´ ë“œë¦´ê¹Œìš”?